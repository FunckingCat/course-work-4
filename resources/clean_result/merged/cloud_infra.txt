№ 11 (128)                                    
                         ноябрь, 2024 г. 
 
__________________________ 
Библиографическое описание:  Шумилов М.И. ОПТИМИЗАЦИЯ ВЫСОКОНАГРУЖЕННЫХ ВЕБ-ПРОЕКТОВ 
С ИСПОЛЬЗОВАНИЕМ МИКРОСЕРВИСНОЙ АРХИТЕКТУРЫ // Universum: технические науки : электрон. 
научн. журн. 2024. 11(128). URL: https://7universum.com/ru/tech/archive/item/18560  
СТАТЬИ НА РУССКОМ ЯЗЫКЕ 
 
ИНФОРМАТИКА, ВЫЧИСЛИТЕЛЬНАЯ ТЕХНИКА И УПРАВЛЕНИЕ 
DOI - 10.32743/UniTech.2024.128.11.18560 
ОПТИМИЗАЦИЯ ВЫСОКОНАГРУЖЕННЫХ ВЕБ-ПРОЕКТОВ  
С ИСПОЛЬЗОВАНИЕМ МИКРОСЕРВИСНОЙ АРХИТЕКТУРЫ 
Шумилов Михаил Игоревич 
технический директор компании Vadimages,  
США, штат Вашингтон, Ванкувер, 
Украина, г. Харьков 
E-mail: michael@vadimages.com 
 
OPTIMIZATION OF HIGH-LOAD WEB PROJECTS USING MICROSERVICE ARCHITECTURE 
Mykhailo Shumilov 
CTO at Vadimages, 
USA, WA, Vancouver, 
Ukraine, Kharkiv 
 
АННОТАЦИЯ 
В данной статье подробно исследуются методы оптимизации высоконагруженных веб-проектов с применением 
микросервисной архитектуры. Основная цель работы заключается в проведении всестороннего анализа эффективности использования микросервисного подхода для обеспечения гибкости, масштабируемости и высокой отказоустойчивости современных веб-приложений. В рамках исследования рассматриваются не только ключевые концепции микросервисной архитектуры, но и современные инструменты, такие как Docker и Kubernetes, которые играют 
важную роль в автоматизации и оркестрации процессов. Методология основана на анализе применения данных 
технологий для повышения общей производительности систем, улучшения управляемости и обеспечения легкой 
интеграции новых модулей в существующую инфраструктуру. Важным аспектом исследования является также 
изучение способов оптимизации времени отклика системы, повышения ее стабильности и устойчивости к сбоям. 
Результаты показывают, что внедрение микросервисной архитектуры не только сокращает время отклика 
и облегчает процесс интеграции новых модулей, но и существенно снижает эксплуатационные риски, одновременно повышая эффективность работы высоконагруженных систем. 
В выводах подчеркивается, что использование данного подхода позволяет веб-приложениям быстро адаптироваться к изменяющимся требованиям бизнеса, обеспечивая при этом их стабильную и безопасную работу, что 
особенно важно в условиях постоянно растущих нагрузок и изменяющихся бизнес-потребностей. 
ABSTRACT 
This article examines in detail the methods of optimizing high-load web projects using microservice architecture. The main 
purpose of the work is to conduct a comprehensive analysis of the effectiveness of using the microservice approach 
to ensure flexibility, scalability and high fault tolerance of modern web applications. The research examines not only the 
key concepts of microservice architecture, but also modern tools such as Docker and Kubernetes, which play an important 
role in automation and orchestration of processes. 
The methodology is based on the analysis of the application of these technologies to improve the overall performance 
of systems, improve manageability and ensure easy integration of new modules into existing infrastructure. An important 
aspect of the study is also the study of ways to optimize the response time of the system, increase its stability and resilience 
to failures. The results show that the introduction of a microservice architecture not only reduces response time and facilitates 
the integration of new modules, but also significantly reduces operational risks, while increasing the efficiency of highly 
loaded systems. The conclusions emphasize that using this approach allows web applications to quickly adapt to changing 
business requirements, while ensuring their stable and secure operation, which is especially important in conditions 
of constantly increasing loads and changing business needs. 
 
Ключевые слова: микросервисная архитектура, оптимизация, высоконагруженные системы, масштабируемость, 
отказоустойчивость, Docker, Kubernetes. 
Keywords: microservice architecture, optimization, high-load systems, scalability, fault tolerance, Docker, Kubernetes. 
________________________________________________________________________________________________ 
№ 11 (128)                                    
                         ноябрь, 2024 г. 
 
5 
 
Введение 
В условиях стремительного роста объёмов 
данных и увеличения количества пользователей 
веб-приложений разработчики сталкиваются с необходимостью поиска эффективных решений для оптимизации высоконагруженных систем. Традиционные 
монолитные архитектуры, применявшиеся ранее, 
часто не справляются с вызовами современных реалий, такими как высокая нагрузка, сложность интеграции новых модулей и обновлений, а также трудности 
в поддержании отказоустойчивости. 
В связи с этим, микросервисная архитектура 
становится всё более востребованной, так как она 
позволяет гибко адаптировать систему к изменяющимся требованиям и обеспечивать её стабильную 
работу даже при пиковых нагрузках. 
Микросервисы представляют собой модульный 
подход к разработке программного обеспечения, 
при котором приложение разбивается на небольшие, 
независимые друг от друга сервисы. Каждый микросервис выполняет определённую функцию и взаимодействует с другими через чётко определённые API. 
Такой подход позволяет разработчикам легко изменять или обновлять отдельные компоненты системы, 
не затрагивая её целостность. Использование микросервисной архитектуры также предоставляет возможности для автоматизации процессов разработки и 
развертывания, что особенно актуально в контексте 
применения DevOps и CI/CD практик. 
Актуальность темы обусловлена необходимостью создания высокопроизводительных и надёжных 
веб-приложений, способных справляться с большими 
объёмами данных и множеством параллельных запросов. Современные технологии, такие как Docker 
и Kubernetes, обеспечивают контейнеризацию и 
оркестрацию микросервисов, упрощая их развёртывание и управление. Благодаря этому разработчики 
могут быстро масштабировать сервисы в зависимости 
от текущей нагрузки, минимизируя риски возникновения ошибок и сбоев. 
Цель работы заключается в проведении анализа 
эффективности микросервисного подхода в обеспечении гибкости, масштабируемости и отказоустойчивости веб-приложений. 
Материалы и методы 
Исторически термин «архитектура» связывался в 
первую очередь с возведением зданий и сооружений. 
В сфере программного обеспечения архитектура 
представляет собой структурный подход к созданию 
решений, способных удовлетворять бизнес-задачи, 
обеспечивать удобство использования и адаптацию 
к изменениям и развитию [1]. 
Микросервисная архитектура или просто микросервисы – это особый метод разработки программных 
систем, который пытается сосредоточиться на создании однофункциональных модулей с четко 
определенными интерфейсами и операциями. Ниже 
на рисунке 1 будет отражен принцип работы микросервисной архитектуры.  
 
 
Рисунок 1. Принцип работы микросервисной архитектуры [2] 
 
Микросервисная архитектура предоставляет множество возможностей для Agile- и DevOps-команд, 
что подтверждается практикой таких крупных технологических компаний, как Netflix, eBay, Amazon, 
Twitter и PayPal, которые постепенно отказались 
от монолитных структур в пользу микросервисов. 
Микросервисная архитектура предлагает решение 
этих проблем за счёт максимального разделения на 
модули. Каждый микросервис представляет собой 
небольшую независимую службу, которая функционирует в рамках собственного процесса и может быть 
развёрнута автономно, без воздействия на остальные 
компоненты.  
№ 11 (128)                                    
                         ноябрь, 2024 г. 
 
6 
Такие службы допускают использование различных языков программирования и подходов к организации баз данных, что позволяет адаптировать их 
к специфическим требованиям. В то же время, это 
повышает гибкость и масштабируемость системы, 
но требует динамического управления и интеграции. 
Чаще всего микросервисы взаимодействуют посредством API и опираются на инфраструктуру, развившуюся в среде RESTful и веб-сервисов, что упрощает 
тестирование их взаимодействия и интеграции при 
развёртывании [2]. 
Далее рассмотрим принципы архитектуры микросервисов.  
1. Автономные и независимые сервисы. Каждый 
микросервис функционирует автономно, имея все 
необходимые ресурсы, что минимизирует взаимозависимости 
и поддерживает 
децентрализацию. 
Пример: отдельные сервисы для управления каталогом товаров и процессинга платежей.. 
2. API-агрегация. API обеспечивает совместимость микросервисов, независимо от технологий, 
и гибкость при интеграции новых сервисов. 
3. Гибкость Микросервисы легко адаптируются 
к изменениям, позволяя модифицировать или удалять 
модули без влияния на другие части системы. 
4. Масштабируемость. Архитектура поддерживает изменение ресурсов в зависимости от нагрузки, 
например, при увеличении трафика в пиковые периоды [3]. 
5. Непрерывный мониторинг. Постоянный мониторинг и системы журналов, метрик и трассировки 
помогают своевременно обнаруживать и устранять 
сбои. 
6. Отказоустойчивость. Изоляция компонентов и 
резервирование позволяют минимизировать влияние 
сбоев на систему. 
7. Балансировка нагрузки в реальном времени. 
Эти системы распределяют ресурсы между микросервисами в реальном времени, обеспечивая оптимальное использование процессорных мощностей 
и минимизируя время отклика на запросы. 
8. Интеграция DevOps. Принципы DevOps  
автоматизируют разработку, тестирование и развертывание микросервисов с использованием Docker 
и Kubernetes. 
9. Версионирование. Управление версиями позволяет обновлять сервисы, минимизируя влияние на 
пользователей и обеспечивая совместимость. 
10. Высокая доступность. Микросервисы должны обеспечивать круглосуточную работу с минимальными простоями [4]. 
Для разработки и создания микросервисов существует обширный выбор инструментов, которые 
способны упростить и ускорить данный процесс. 
В ниже приведенном обзоре рассматриваются наиболее распространенные из них: 
1. Spring Boot — это фреймворк, работающий 
на основе языка программирования Java. Он предоставляет широкий набор возможностей, значительно 
упрощающих создание и внедрение микросервисных 
архитектурных решений. 
2. Node.js — платформа для разработки серверных решений на JavaScript, которая обеспечивает 
создание 
масштабируемых 
и производительных 
микросервисов, используя возможности данного 
языка. 
Docker — это платформа с открытым исходным 
кодом, предназначенная для автоматизации процессов 
развёртывания и управления приложениями в контейнерах. Данный инструмент позволяет упаковывать 
микросервисы и развертывать их в изолированных 
контейнерных средах [5]. 
3. Kubernetes (или K8s) — система оркестрации 
контейнеров, обеспечивающая развертывание, масштабирование и управление микросервисными 
приложениями. Она предлагает обширный набор 
инструментов для управления сетевыми службами, 
хранения данных и автоматизации масштабирования 
компонентов микросервисов. 
4. Apache Kafka — распределённая система для 
обработки потоковых данных, позволяющая интегрировать микросервисы, опираясь на событийную 
архитектуру. Она функционирует как посредник для 
обмена сообщениями между различными сервисами, 
обеспечивая их взаимодействие и синхронизацию [6]. 
Для реализации масштабируемости и отказоустойчивости применяются методы, такие как контейнеризация и балансировка нагрузки. Контейнеризация 
с использованием инструментов, например, Docker 
в связке с Kubernetes или OpenShift, позволяет быстро 
развертывать и управлять экземплярами микросервисов. Балансировка нагрузки, в свою очередь, 
обеспечивает равномерное распределение запросов 
между компонентами, что способствует стабильной 
работе системы [7]. 
Отказоустойчивость в процессе проектирования 
современных систем направлена на обеспечение их 
функциональности и работоспособности, несмотря 
на возможные неисправности. По мере роста масштабов и сложности инфраструктур вероятность 
возникновения сбоев возрастает. Чтобы поддерживать надёжность и непрерывность взаимодействия 
с пользователями, важно предусматривать меры 
отказоустойчивости. Эти меры включают такие 
методики, как резервирование данных, выявление и 
устранение ошибок, а также автоматическое восстановление системы для минимизации влияния проблем 
с оборудованием, программным обеспечением или 
сетью. Использование таких подходов позволяет 
обеспечить бесперебойную работу сервисов [8]. 
Далее в таблице 1 будет проведено сравнение отказоустойчивости от высокая доступность балансировки нагрузки. 
 
№ 11 (128)                                    
                         ноябрь, 2024 г. 
 
7 
Таблица 1. 
Сравнение отказоустойчивости от высокая доступность балансировки нагрузки [9] 
Аспект 
Отказоустойчивость 
Высокая доступность  
балансировки нагрузки 
Определение 
Гарантирует, что система продолжает  
работать должным образом, даже если  
некоторые компоненты выходят из строя. 
Распределяет рабочие нагрузки между  
несколькими серверами, чтобы гарантировать, что ни один сервер не станет узким  
местом, обеспечивая доступность системы. 
Основная цель 
Поддерживайте функциональность  
системы, несмотря на сбои. 
Максимально увеличьте время безотказной работы и использование ресурсов 
за счет балансировки нагрузки. 
Ключевые методы 
Резервирование, репликация, механизмы 
отработки отказа, обнаружение и исправление ошибок. 
Алгоритмы распределения нагрузки  
(циклический перебор, наименьшее  
количество подключений и т.д.), проверки 
работоспособности, отказоустойчивость. 
Избыточность 
Высокий уровень резервирования  
(несколько компонентов выполняют одну 
и ту же задачу). 
Умеренное резервирование (достаточное 
для балансировки нагрузки и обеспечения 
доступности). 
Примеры 
RAID (резервный массив независимых 
дисков), распределенные базы данных 
с репликацией. 
Балансировка нагрузки DNS, балансировщики нагрузки приложений (например, 
NGINX, HAProxy). 
Влияние на производительность 
Может незначительно повлиять  
на производительность из-за проверок  
избыточности и обработки ошибок. 
В целом производительность повышается 
за счет равномерного распределения  
рабочей нагрузки. 
 
Таким образом, использование микросервисной 
архитектуры, подкрепленное современными технологиями и методами разработки, позволяет создавать 
масштабируемые, гибкие и устойчивые системы, 
удовлетворяющие требованиям бизнеса и пользователей. 
Результаты и обсуждение 
Планирование нагрузки создаваемых систем 
на длительный период времени зачастую является 
сложной задачей, так как прогнозирование конкретных показателей через полгода или год невозможно. 
Отсутствие заложенной возможности масштабирования может привести к снижению эффективности 
системы при увеличении нагрузки. В зависимости 
от особенностей проекта реализуется либо вертикальное, либо горизонтальное масштабирование. 
Вертикальное масштабирование подразумевает увеличение мощности отдельных элементов системы путем 
добавления ресурсов в рамках одного узла. Горизонтальное же масштабирование основывается на 
увеличении количества узлов, позволяя распределять 
нагрузку между ними. 
Проще всего масштабировать сервисы и компоненты, если они следуют принципу stateless — не сохраняют состояния между запросами. В этом случае 
можно легко развернуть несколько экземпляров компонента или сервиса, распределяя нагрузку между 
ними равномерно. При использовании statefulпарадигмы необходимо учитывать наличие и сохранение состояния компонентов при балансировке. 
Тестирование является неотъемлемой частью 
процесса запуска систем с высокой нагрузкой. 
Нагрузочные тесты используются для оценки соответствия системы определенным критериям эффективности (KPI). Из практики, увеличение времени 
обработки запросов часто связано с неэффективностью алгоритмов.  
Применение кэширования данных способствует 
увеличению производительности при многократных 
однотипных запросах. 
Проведение тестов помогает выявить узкие места, 
которые могли быть упущены на этапе проектирования.  
Например, веб-сервис одного из проектов при 
нагрузочном тестировании показал низкую производительность, несмотря на свою архитектурную простоту. Анализ показал, что значительное количество 
времени занимают процессы сериализации и десериализации данных, что приводило к конвертации 
структур в двоичный формат и обратно. Решение было 
найдено быстро — благодаря редким изменениям 
данных удалось применить предсериализацию, что 
значительно сократило время обработки запросов. 
Ниже на рисунке 2 будет отражена оптимизация 
сериализации для внешнего API. 
 
№ 11 (128)                                    
                         ноябрь, 2024 г. 
 
8 
 
Рисунок 2. Оптимизация сериализации для внешнего API (предсериализация) [10] 
 
Вместе с нагрузочными тестами, направленными 
на проверку соответствия системы установленным 
требованиям, проводятся также стресс-тесты. Их основная задача — определить максимальную нагрузку, 
которую может выдержать система без потери стабильности. 
Метрики выступают в роли сенсоров, которыми 
оборудована каждая сложная система, особенно та, 
которая работает под высокой нагрузкой. Отсутствие 
таких показателей превращает программу в «черный 
ящик» — она имеет входные и выходные данные, 
но внутренние процессы остаются неясными.  
Поэтому на этапе проектирования в архитектуру 
системы интегрируются специальные измерительные 
механизмы, которые обеспечивают подачу данных для 
мониторинговых систем. 
В некоторых проектах для мониторинга активности и визуализации данных системы применялся 
стек ELK (ElasticSearch, Logstash и Kibana).  
Этот инструмент обладает гибким интерфейсом, 
что позволяет быстро и удобно настроить нужные 
формы отчетности и визуализации (рис. 3.). 
 
 
Рисунок 3. Мониторинг Kibana [10] 
№ 11 (128)                                    
                         ноябрь, 2024 г. 
 
9 
Основные преимущества микросервисной архитектуры при разработке высоконагруженных систем 
включают: 
1. Возможность вносить изменения только в один 
модуль системы без необходимости полного развёртывания. 
2. Размещение микросервисов, требующих частого масштабирования, на мощных серверах для 
повышения их эффективности. 
3. Обеспечение отказоустойчивости системы: 
сбои в одном модуле не влияют на остальные. 
4. Использование наиболее подходящих фреймворков для каждой конкретной разработки. 
5. Замена устаревших микросервисов без негативного воздействия на другие части системы [11]. 
В свою очередь для решения проблем, связанных 
с микросервисной архитектурой, может быть применен событийно-ориентированный подход, включающий Event Sourcing и Command Query Responsibility 
Segregation (CQRS). Событийно-ориентированный 
подход представляет собой архитектурную парадигму, ориентированную на генерацию, обнаружение 
и обработку событий, а также на реакцию на них.  
Основу веб-приложения в такой системе составляют события, обеспечивающие обмен данными 
между различными сервисами. Каждый сервис реагирует на конкретные события, при этом инициатор 
события не обладает информацией о том, какие сервисы его обрабатывают. Такой механизм способствует низкой связанности между микросервисами, 
что повышает согласованность данных в приложении 
и упрощает его масштабируемость. 
Event Sourcing предполагает сохранение последовательности изменений, происходящих с состоянием 
веб-приложения.  
Такая последовательность может использоваться 
как указатель на текущее состояние приложения либо 
как список зарегистрированных событий.  
Применение этого подхода позволяет достичь 
высокой степени децентрализации операций чтения 
и модификации данных. 
В контексте Event Sourcing агрегат выступает 
индикатором актуального состояния веб-приложения. 
Агрегат представляет собой группу элементов предметной области, имеющих тесные взаимосвязи 
(см. рис. 6). Для получения актуального состояния 
агрегата необходимо загрузить и воспроизвести 
соответствующие события.  
Любое изменение агрегата требует создания 
нового события, которое описывает логику модификации его части, при этом сохраняется не сам агрегат, 
а только события, связанные с его изменением. 
Для хранения данных о событиях в системе 
используются 
столбцы 
Event_id, 
Event_type 
и Event_data, которые представляют собой идентификатор события, его тип и описание. В то же время 
столбцы Aggregate_id и Aggregate_type указывают на 
идентификаторы и типы соответствующих агрегатов. 
Применение архитектурного шаблона Event 
Sourcing предоставляет ряд преимуществ, основное 
из которых заключается в автоматической публикации событий при изменении агрегатов.  
Это значительно упрощает реализацию событийно-управляемой микросервисной архитектуры. 
Данный паттерн позволяет фиксировать полную 
историю изменений для каждого агрегата, что полезно 
в случае выполнения временных запросов, позволяющих восстановить предыдущее состояние агрегата. 
Тем не менее, даже с учетом всех положительных 
сторон Event Sourcing, проблема обработки запросов 
к базе данных остается актуальной. Решением может 
стать использование паттерна CQRS. 
CQRS представляет собой архитектурный подход, 
который разделяет процессы чтения и обновления 
данных в хранилище.  
Его реализация предполагает деление вебприложения на командную и запросную части. 
Командная часть обрабатывает команды, связанные 
с созданием, чтением, обновлением и удалением 
агрегатов, а запросная часть занимается обработкой 
запросов, таких как POST или GET. 
В качестве примера можно рассмотреть применение паттерна CQRS в интернет-магазине (рис. 4). 
Сервисы, отвечающие за управление клиентами 
и заказами («Customer» и «Order»), выносятся в командную часть приложения, где они предоставляют 
API-интерфейсы для создания и обновления информации о клиентах и их заказах. В то же время сервис 
«Viewing customers» находится в запросной части и 
предоставляет API-интерфейс для получения данных 
о клиентах.  
Этот сервис отслеживает события, исполняемые 
командной частью, и обновляет данные в хранилище 
интернет-магазина. 
Таким образом, разделение веб-приложения на 
командную и запросную составляющие повышает 
производительность, гибкость и безопасность всей 
системы. Кроме того, с помощью применения паттерна CQRS совместно с событийно-ориентированным подходом решается проблема согласованности 
данных в микросервисах, позволяя веб-приложению 
эффективно осуществлять различные виды запросов 
[12]. 
Заключение 
В статье рассмотрена оптимизация высоконагруженных веб-проектов посредством применения 
микросервисной архитектуры.  
Проведённый анализ показал, что микросервисный подход существенно улучшает гибкость и адаптивность систем за счёт разбиения на автономные 
модули, что позволяет разработчикам оперативно 
реагировать на изменяющиеся бизнес-требования 
и интегрировать новые функциональные элементы 
без влияния на общую работоспособность системы.  
Применение современных технологий, таких 
как Docker и Kubernetes, способствует автоматизации и оркестрации процессов, что улучшает масштабируемость и снижает время отклика сервисов. 
Внедрение данных инструментов и подходов позволяет минимизировать риски и обеспечивать отказоустойчивость веб-приложений, что особенно важно 
для высоконагруженных систем.  
№ 11 (128)                                    
                         ноябрь, 2024 г. 
 
10 
Таким образом, микросервисная архитектура 
представляет собой эффективное решение для создания и оптимизации современных веб-проектов, 
обеспечивая стабильность и производительность при 
высоких нагрузках и изменяющихся условиях эксплуатации. 
 

"Теория и практика современной науки" №6(24) 2017 
825 
 
 
 
УДК 004.4 
Терских М.Г. 
специалист по разработке распределённых вычислительных 
систем компании “EXON LV” 
 студент магистратуры 1 курса 
 факультет «Информатики и систем управления» 
МГТУ им. Н.Э.Баумана 
Россия, г. Москва 
ТЕХНОЛОГИИ ИЗОЛЯЦИИ ПРИЛОЖЕНИЙ И 
ИНСТРУМЕНТАЛЬНЫЕ СРЕДСТВА ДЛЯ УПРАВЛЕНИЯ 
КОНТЕЙНЕРАМИ 
Аннотация:  
В статье рассматриваются две технологии для обеспечения изоляции 
приложений. 
Показано 
сравнение 
этих 
технологий. 
 
Приведены 
инструментальные средства оркестрации контейнеров. Описаны ключевые 
возможности Docker Swarm, Mesosphere Marathon и Kubernetes. Приведены 
примеры использования и даны рекомендации. 
Ключевые слова: контейнеризация, виртуализация, докер, кубернетес, 
мезосфер марафон.  
 
Terskikh M.G. 
specialist in the development of distributed computing systems of the 
“EXON LV” company 
student of the Master’s program of 
 «Informatics and management systems» 
Bauman Moscow State Technical University  
Russia, Moscow 
TECHNOLOGIES OF ISOLATION OF APPLICATIONS AND 
INSTRUMENTAL MEANS FOR CONTAINERS ORCHESTRATION 
Abstract:  
The article is devoted to two technologies for securing application isolation. 
A comparison of these technologies is shown. The tools of container orchestration 
are given. The key features of Docker Swarm, Mesosphere Marathon and 
Kubernetes are described. Examples of use are shown and some recommendations 
are given. 
Keywords: containerization, Virtualization, docker, kubernetes, mesosphere 
marathon.  
 
Изоляция приложений 
Мир 
информационных 
технологий 
все 
больше 
использует 
контейнерные технологии. Docker контейнеры стали основной темой 
"Теория и практика современной науки" №6(24) 2017 
826 
 
разговоров при обсуждении серверной инфраструктуры и DevOps. Набор 
инструментов Docker включает в себя движок для управления контейнерами, 
многоуровневую файловую систему и публично размещенный реестр 
образов широкоиспольземых приложений. Как правило, движок Docker 
автоматизирует развертывание приложений внутри контейнеров Linux [1]. 
Механизмы ядра Linux, обеспечивающие изоляцию процессов друг от 
друга, такие как, cgroups , namespaces и chroot , позволяют группам 
процессов иметь частный вид системы и работать независимо друг от друга 
на 
одном 
хосте 
Linux. 
Эта 
архитектура 
позволяет 
избегать 
непроизводительных издержек при запуске и на уже работающих 
виртуальных машинах (ВМ). Так как виртуальные машины работают на ОС 
хоста, каждое новое приложение должно запускать гостевую ОС. Это не 
очень хорошо масштабируется для микро-сервисных архитектур, где 
сервисы занимают меньше пространства, а  их количество больше. Docker 
просто запускает приложения как группу процессов, которые остаются 
разделенными друг от друга, при этом базовые вычислительные ресурсы 
используются совместно. 
Сравнивая контейнеры Linux с виртуальными машинами (ВМ) можно 
заметить, что последние требуют больше ресурсов, но обеспечивают 
лучшую изоляцию, так как гипервизоры виртуальных машин эмулируют всю 
операционную систему, включая виртуальное аппаратное обеспечение. С 
другой стороны, контейнеры работают на общей операционной системе. 
Контейнеры, использующие миниатюрные пакеты, в которых содержатся 
приложение и зависимые от него объекты, оставляют позади большую часть 
громоздких ВМ (размеры ВМ часто могут достигать десятков гигабайт). В 
результате хост Docker может запускать гораздо больше приложений на 
идентичном аппаратном обеспечении. На рисунке 1 показана архитектура  
обеих технологий. 
 
 
Рисунок 1 – отличие виртуальной машины от контейнера 
"Теория и практика современной науки" №6(24) 2017 
827 
 
Кроме того, важным различием между контейнерами и виртуальными 
машинами является скорость исполнения. Запуск виртуальной машины 
может занимать несколько минут и часто влечет за собой интенсивное 
использование ресурсов. Таким образом, запуск нескольких виртуальных 
машин требует определенного планирования и очередности. В отличие от 
ВМ, несколько контейнеров, благодаря их легковесности, можно запустить в 
течение нескольких секунд. Такие характеристики и масштабируемость 
приводит к новому типу, может быть, даже новому поколению, 
распределенных приложений, в которых контейнеры автоматически 
запускаются и останавливаются в зависимости от различных факторов, таких 
как пользовательский трафик, события, запросы и запланированные задачи. 
 
Инструменты оркестрации 
Итак, контейнеры и легкий способ виртуализации приложений - 
важный элемент любого плана DevOps. Но для организации контейнеров 
нужны программы для их управления. К таким программам относятся 
Kubernetes, MesosphereMarathon и DockerSwarm.  
 
DockerSwarm 
Как правило, знакомство с контейнерами начинается с Docker, первой 
контейнерной 
программы, 
которая 
привлекла 
большую 
аудиторию 
пользователей. 
Естественный 
инстинкт 
подталкивает 
обратиться 
к 
программе по управлению контейнерами, созданной теми же людьми, 
которые 
проектировали 
инфраструктуру 
контейнера. 
Часто 
такой 
программой становится Docker Swarm. Swarm– техническим языком Docker 
Native Orchestration [2]. 
Для пользователей, хорошо знакомых с Docker, обучение работе в 
Swarm не вызывает проблем. Docker Native Orchestration использует 
одноузловую концепцию Docker и расширяет ее до Swarm. После запуска 
Docker узлов, установка Swarm не вызывает абсолютно никаких трудностей. 
Например, для добавления экземпляров Docker в кластер Swarm можно 
использовать одиночную командную оболочку. Обновленный Swarm также 
включает в себя поддержку для последовательного обновления, межузловое 
шифрование Transport Layer Security, простую балансировку нагрузки и 
легкую абстракцию сервиса. 
Но, в DockerSwarm еще есть, что улучшать. Например, не 
поддерживаются проверки состояния контейнеров и автоматические 
возвраты к предыдущему состоянию. Пользователи Swarm также жалуются 
на многочисленные проблемы внедрения. Например, многие пользователи 
жалуются, что Swarm должным образом не поддерживает мульти-хостовую 
конфигурацию сети контейнера. 
 
Mesosphere Marathon 
"Теория и практика современной науки" №6(24) 2017 
828 
 
Marathon является платформой контейнерной оркестровки для DC/OS 
Mesosphere и ApacheMesos. DC/OS – это распределенная операционная 
система на базе распределенных систем ядра Mesos. Mesos, в свою очередь, 
является открытой системой управления кластером. Marathon обеспечивает 
интеграцию 
управления 
между 
существующими 
приложениями, 
сохраняющими 
состояние, 
и 
контейнерными 
приложениями, 
не 
сохраняющими состояние [3]. 
Marathon имеет множество функций, включая высокую степень 
доступности, обнаружение сервисов и балансировку нагрузки. Если 
запустить его на DC/OS, приложения также получит виртуальную IPмаршрутизацию. 
 
Kubernetes 
Kubernetes имеет отличную сервисную поддержку со стороны 
поставщиков. Сегодня основным дистрибьютором Kubernetes выступает 
промышленный альянс Cloud Native Computing Foundation компании Linux 
Foundation.  
Помимо доказательства своей состоятельности на датацентрах Google, 
Kubernetes может похвастаться самовосстановлением, автоматизированными 
последовательными обновлениями и возвратами в исходное состояние, а 
также  оркестровкой хранения. И это далеко не полный список 
функциональных возможностей Kubernetes.  
Большим преимуществом Kubernetes является его способность к 
взаимодействию. Например, для создания необходимой топологии и  
инфраструктуры, достаточно написать одну JSON спецификацию и вставить 
ее в любой запущенный кластер Kubernetes. На рисунке 2 приведены 
примеры таких JSON файлов, с помощью которых можно создавать 
Deployment, Pod или Service [4]. 
"Теория и практика современной науки" №6(24) 2017 
829 
 
Рисунок 2 – примеры JSON файла для запуска компонентов 
Kubernetes 
 
Еще одно преимущество заключается в том, что "Kubernetes– это 
облако-агностик",- говорит инженер-программист ЭрезРаби (ErezRabih). 
"Кластер можно запустить на AWS, GoogleCloud, Microsoft’sAzure, 
Rackspace и т.д., и он будет работать более или менее одинаково". 
Действительно, одна из краткосрочных целей разработчиков Kubernetes 
заключается в том, чтобы позволить запускать контейнеры Docker, 
управляемые 
Kubernetes, 
с 
помощью 
мультиоблачной 
архитектуры, 
одновременно используя Cluster Federation. 
Конечно же, Kubernetes не совершенен. Сразу же на ум приходят два 
его недостатка. Во-первых, сложность балансировки нагрузки. В конце 
концов, имея доступ в Kubernetes, можно с легкостью запускать внешний 
балансировщик нагрузки изнутри Kubernetes, но это все еще на стадии 
разработки. Во-вторых, Kubernetes отлично справляется с автоматическим 
фиксированием проблем. Настолько хорошо, что можно даже не заметить 
повторный запуск контейнера после сбоя. Для решения этой проблемы 
необходимо добавить централизованную систему протоколирования. 
 
Заключение 
Как всегда, выбор зависит от конкретных потребностей пользователя. 
На данный момент сложно сказать, какое из решений было бы наилучшим. 
Абсолютно точно можно сказать то, что, если Вы собираетесь серьезно 
заниматься контейнерами, Вам потребуются эти программы. С ними Вам не 
нужно будет тратить время на создание собственных средств по 
администрированию контейнеров. 
Для создания уникального сочетания, учитывая все потребности 
компании, эти программы можно смешивать и объединять.  
Использованные источники: 
1. Using Docker: Developing and Deploying Software with Containers 1st 
Edition, 2016, 354 p 
2. Руководство Docker Swarm. URL: https://docs.docker.com/engine/swarm/ 
(дата обращения: 10.06.2017) 
3. Руководство Mesosphere Marathon. URL: https://docs.mesosphere.com (дата 
обращения: 10.06.2017) 
4. Руководство Kubernetes. URL: https://kubernetes.io/docs/home/ (дата 
обращения: 12.06.2017) 
 
 
 
 
 

V Международная научно-практическая конференция 
74 
13. Bourdeau C., Billauxand D., Chraibi A.F. Thermo-hydro-mechanical 
analysis of a DAM using FLAC3D software. FKAC/DEM, Minneapolis, 2008. 
 
 
 
 
 
УДК 004.4 
Чернов Андрей Владимирович 
Chernov Andrey Vladimirovich 
Д.т.н., профессор 
Dr.Tech.Sc., Professor 
Коновалов Никита Сергеевич 
Konovalov Nikita Sergeevich 
Ростовский государственный университет путей сообщения 
Rostov State Transport University 
Побойкина Алина Олеговна 
Poboykina Alina Olegovna 
Студент 
Student 
Донской государственный технический университет 
Don State Technical University 
 
СТРАТЕГИЯ РАЗВЕРТЫВАНИЯ МИКРОСЕРВИСОВ В ОБЛАКЕ 
 
STRATEGY FOR DEPLOYING MICROSERVICES IN THE CLOUD 
 
Аннотация: В данной статье рассматриваются способы развертывания микросервисов 
в облаке. Целью данной статьи является выбор наиболее подходящего способа развертывания 
микросервиса в облаке. В результате исследования рассмотрены различные стратегии 
развертывания микросервисов и приведены детальные примеры возможного развертывания с 
выделением положительных и отрицательных сторон. 
Abstract: This article discusses how to deploy microservices in the cloud. The purpose of this 
article is to choose the most appropriate way to deploy a microservice in the cloud. As a result of the 
study, various strategies for deploying microservices are considered and detailed examples of possible 
deployments are given, highlighting the positive and negative aspects. 
Ключевые 
слова: 
микросервисная 
архитектура, 
развертывание, 
облачное 
развертывание, стратегия развертывания. 
«Вопросы развития современной науки и техники» 
75 
Keywords: microservice architecture, deployment, cloud deployment, deployment strategy. 
 
Введение. Приложение, которое содержит в себе микросервисы, состоит 
из множества сервисов, написанных на разных языках и фреймворках. Каждый 
из микросервисов представляет собой отдельное приложение со своим 
особенным окружением и условиями к развертыванию, мониторингу, ресурсам 
[1]. К примеру, появляется надобность в нескольких копиях сервиса в 
зависимости от его нагруженности и востребованности, также каждому сервису 
необходимо предоставить соответствующее количество ресурсов. Сложнее всего 
то, что сервисы должны быстро устанавливаться, а также быть надежными.  
Существует несколько шаблонов развертывания, которые представлены на 
рис. 1 [2]. 
 
 
Рис. 1. Виды шаблонов развертывания микросервисов 
 
Множество образов микросервиса на хосте. В данном подходе 
используется шаблон развертывания нескольких экземпляров на хосте. При 
использовании данного подхода, на предоставляемых облачных хостах, 
запускается несколько экземпляров сервисов на каждом [3, 5]. Плюсом является 
то, что при хорошей балансировке сервисы будут доступны для большего 
количества клиентов и в случае отказа одного из сервисов, работа не 
Шаблоны развертывания
Множество образов микросервиса на хосте
Образ сервиса на каждый хост
Образ сервиса на виртуальную машину
Образ сервиса на каждый контейнер
Бессерверное развертывание
V Международная научно-практическая конференция 
76 
остановится. Минусом является то, что не каждый сервис возможно таким 
образом разбить (к примеру, сервис работает с глобальной базой и обновляет 
какой-либо компонент по определенным условиям, если два сервиса 
одновременно будут делать противоположную операцию, это может привести к 
неправильному результату). 
Образ сервиса на каждый хост. При использовании данного шаблона 
каждый образ сервиса запускается на отдельном хосте, что позволяет 
изолировать экземпляры [4]. Плюсом данного подхода является то, что в случае 
если один из сервисов расходует все ресурсы системы, то остальные образы на 
других хостах от этого не страдают и могут продолжать свою работу. 
Существенным минусом является затратное содержание нескольких хостов и 
контроль их состояния, необходимость в отдельном специалисте по серверу для 
конфигурации. 
Образ сервиса на виртуальную машину. Используя данный шаблон, 
каждый сервис упаковывается в отдельную виртуальную машину (например, как 
yandex cloud). Каждый образ сервиса – это виртуальная машина, которая 
запускается из конкретной сборки виртуальной машины [3]. Данный подход в 
основном используют крупные компании для хранения изолированных сборок. 
Положительным моментом данного подхода является то, что нет необходимости 
тратить время на настройку окружения, так как это доступно из панели 
конфигураций, а также в данном шаблоне, как и в шаблоне образа микросервиса 
на каждый хост, ресурсы выделяются под конкретную виртуальную машину и 
не могут влиять на другие виртуальные машины. Отрицательной же стороной 
является то, что ресурсы используются не совсем правильно, расходы идут на 
всю виртуальную машину, а именно операционную систему. Также как правило 
имеются фиксированные шаблоны системы, которые могут быть избыточны. 
Образ сервиса на каждый контейнер. Данный шаблон развертывания 
микросервиса является простым, но при этом имеет положительные стороны 
развертывания такие же, как на виртуальных машинах. При использование 
данного шаблона, во время каждой сборки образ сервиса запускается в 
«Вопросы развития современной науки и техники» 
77 
собственном 
контейнере. 
Контейнеры 
– 
это 
механизм, 
реализующий 
виртуализацию на уровне операционной системы, состоит из процессов, 
исполняющихся в своем виртуальном окружение. С точки зрения процессов и 
окружения, это отдельная виртуальная система, которая имеет свою корневую 
файловую структуру, порты и пространство имен. В данном шаблоне имеется 
возможность ограничивать ресурсы контейнера, а именно оперативную память, 
дисковое пространство, а также загруженность cpu. Наиболее известной 
контейнерной технологией является docker [4]. При применении данного шаблон 
создается образ docker контейнера с необходимым окружением для нужного 
приложения. После создания конфигурационного файл образа, появляется 
возможность запустить несколько образов на физических или виртуальных 
хостах. Плюсами данного шаблона является то, что имеется возможность 
простого наблюдения за ресурсами, которые расходует каждый образ, вдобавок 
возможность изменять допустимые объемы выделяемых ресурсов. Из минусов 
можно обозначить, что контейнеры используют совместно ядро системы хоста, 
а 
также 
сложность 
администрирования, 
если 
не 
применяются 
специализированные решения. 
Бессерверное развертывание. Основной особенностью данного шаблона 
является то, что основу составляют функции, исполняющие конкретную задачу 
с возможностью подписаться на конкретные события в облаке. Этот подход 
позволяет 
запускать 
приложения 
в 
безопасном, 
отказоустойчивом 
и 
масштабируемом окружении, без создания и обслуживания виртуальных машин 
[5, 6]. Из отрицательных сторон неизвестность места запуска функции, 
отсутствие структурированности приложения, так как каждая функция, это 
отдельный объект. 
Заключение. Развертывание микросервисов в облаке не такая тривиальная 
задача, как может показаться. При выборе определённого подхода следует четко 
оценить все положительные и отрицательные стороны, а также сложность 
администрирования, ведь каждый микросервис, это отдельное приложение со 
своими особенностями, на которые необходимо обращать внимание. 
V Международная научно-практическая конференция 
78 
Библиографический список: 
1. Штефуряк, А. С. Грамотный выбор стратегии развёртывания 
микросервисного программного продукта / А. С. Штефуряк, Т. С. Яницкая // 
Молодой ученый. 2020. № 43 (333). С. 19-21. 
2. Маркелов А. А. Введение в технологии контейнеров и Kubernetes – М.: 
ДМК Пресс, 2019. 
3. 
Михайлов 
К.И. 
Анализ 
программных 
средств 
мониторинга 
высоконагруженных облачных информационных систем // Системный анализ в 
проектировании и управлении. 2019. С. 359-362. 
4. Грузин Н.А. Сравнение Docker контейнеров и виртуальных машин, а 
также облачных платформ AWS и Azure // MODERN SCIENCE. 2020. №2-1. С. 
351-355. 
5. Надейкина Л.А., Черкасова Н.И. Распределенные системы, построенные 
на базе микросервисной архитектуры // Инновационные, информационные и 
коммуникационные технологии. 2019. №1. С. 300-304. 
6. 
Артамонов 
Ю.С., 
Востокин 
С.В. 
Разработка 
распределенных 
приложений сбора и анализа данных на базе микросервисной архитектуры // 
Известия Самарского научного центра Российской академии наук. 2016. №4(4). 
С. 688-693. 
 
 
 
 
 
 
 
 
 
 
 

«Colloquium-journal»#14(38),2019 / BIOLOGICAL SCIENCES 
33 
УДК 004.35  
Стрелец Андрей Иванович 
Магистр кафедры «Компьютерные системы и технологии», 
Национальный исследовательский ядерный университет «МИФИ»  
Россия, г. Москва 
Орлов Александр Петрович 
Магистр кафедры «Компьютерные системы и технологии», 
Национальный исследовательский ядерный университет «МИФИ»  
Россия, г. Москва  
Храпов Александр Сергеевич 
Магистр кафедры «Компьютерные системы и технологии», 
Национальный исследовательский ядерный университет «МИФИ» 
Атавина Анастасия Владиславовна 
Магистр кафедры «Финансовый мониторинг», 
Национальный исследовательский ядерный университет «МИФИ» 
DOI: 10.24411/2520-6990-2019-10422 
ИСПОЛЬЗОВАНИЕ МЕТОДОВ МАШИННОГО ОБУЧЕНИЯ ДЛЯ ПРЕДСКАЗАНИЯ УРОВНЯ 
ЗАГРУЗКИ РЕСУРСОВ СИСТЕМЫ 
 
Strelets Andrey Ivanovich 
Master degree Department of Computer Systems and Technologies, 
National Research Nuclear University MEPhI  
Moscow, Russia 
Orlov Aleksandr Petrovich 
Master degree Department of Computer Systems and Technologies, 
National Research Nuclear University MEPhI  
Moscow, Russia 
Hrapov Aleksandr Sergeevich 
Master degree Department of Computer Systems and Technologies, 
National Research Nuclear University MEPhI  
Atavina Anastasia Vladislavovna 
Master degree Department of Financial Monitoring, 
National Research Nuclear University MEPhI 
 
USING THE ML METHODS FOR PREDICATING SYSTEM RESOURCES LOADING 
 
Аннотация 
В данной статье рассмотрены методы машинного обучения, предназначенные для предсказания 
уровня загрузки ресурсов системы. Рассмотренные находят широкое применение в области распределённых вычислений.  
Abstract 
This article is about methods for predicating system resources loading. These methods widely used in distributed computing 
 
Ключевые слова: машинное обучение, микросервисы, виртуализация, высокопроизводительные вычисления. 
Key words: machine learning, microservices, virtualization, high performance computing.  
 
Введение 
Все больше компаний обращаются к облачным 
технологиям. В облаках разворачиваются многомодульные системы, настройка которых, в основном, 
требует большого количества времени. Также разработчики могут столкнуться с проблемами отслеживания зависимостей, масштабирования приложения и обновлений. Большинство систем на основе контейнеризации чаще всего работают в 
кластере. Стандартные алгоритмы оркестрации 
Docker Swarm, Google Kubernetes не учитывают 
возможные изменения, которые могут происходить 
внутри контейнера, что может приводить к неравномерному распределению нагрузки по узлам. Для 
решения поставленных задач разрабатываются алгоритмы, работающие поверх стандартной реализации. Контейнеризация позволяет масштабировать 
систему с помощью добавления новых копий сервиса. Балансировщик нагрузки позволяет разделить 
запросы к разным копиям контейнера. 
Архитектура сети 
Подход к построению архитектуры сети сконцентрирован на распределение сервисов на узлах 
сети на основе алгоритмов ML после их запуска для 
уменьшения задержки, повышение пропускной 
способности системы в целом. Это является необходимостью из-за динамического поведения контейнера во время работы. 
34 
BIOLOGICAL SCIENCES / «Colloquium-journal»#14(38),2019 
Архитектура разработанного решения состоит 
из следующих компонентов: 
1) Менеджера запуска контейнеров на основе 
Docker Swarm; 
2)  Менеджера распределения контейнеров 
на основе ML; 
3) Менеджера мониторинга узлов на основе 
сбора метрик и kpi узлов и контейнеров. 
 
Сравнение виртуализации на гипервизорах и 
контейнерах 
Рассмотрим два наиболее используемых подхода к виртуализации сетевых функций, которые, к 
тому же имеют реализации в виде свободного ПО.  
Подход 1 — это виртуализация на основе развёртывания виртуальных машин с использованием 
гипервизора как управляющего элемента. 
Подход 2 — это виртуализация на основе контейнеров. Гипервизор в данном случае отсутствует, 
и управление ресурсами осуществляет сама ОС. 
Практическая часть данной статьи разрабатывалась с использованием второго подхода, но концептуально все решения применимы и для первого. 
Предлагаемая схема инфраструктуры абстрагирагирована от конкретного программного способа 
виртуализации сетевых функций. 
Виртуализация с использованием гипервизоров. 
Наиболее наглядное представление можно получить на примере схемы виртуализации, используемой в KVM (рисунок 1). 
 
 
Рисунок 1. Схема виртуализации на гипервизорах 
 
Оценки потерь производительности (overhead) 
контейнеризации приводятся в различных работах, 
и они находятся в районе 0.1-1% в зависимости от 
типа нагрузки и методики тестирования.  
Контейнеризация: Docker, OpenVZ, LXC и т. д. 
Есть API, поэтому можно писать скрипты для развёртывания. Аппаратная поддержка: аналогично 
Linux. На гипервизорах: KVM, Xen и т. д.. Есть API, 
поэтому можно писать скрипты для развёртывания. 
Аппаратная поддержка: аналогично Linux, обязательна поддержка виртуализации на уровне аппаратного обеспечения. Предполагается выделение 
сервисов, занимающихся наблюдением и обслуживанием работы вычислительных узлов на отдельный узел. Предполагается выполнение двух основных задач данным узлом: мониторинг текущего состояния узлов, а также перераспределение инстансов по различным узлам в зависимости от текущей 
нагрузки.  
Дополнительной решаемой задачей является 
визуализация различных текущих характеристик 
системы. Для более удобного наблюдения за текущим состоянием вычислительной инфраструктуры. 
Для получения характеристик с узлов используется 
Telegraf - универсальный агент для для сбора, обработки и агрегирования всех необходимых метрик, 
таких как нагрузка сети, памяти, процессора. Для 
более полного понимания состояния системы 
Telegraf агент расположен не только на каждом 
узле, но и в каждом instance.  
Собранные метрики на каждом узле собираются системой мониторинга Prometheus, сохраняющей все собранные метрики в собственную локальную NoSQL базу данных, в которой каждая метрика 
имеет привязку к моменту времени, в который она 
была измерена, а также указание агента, с которого 
она была получена. 
Сервис поставки метрик по заданному интервалу времени считывает метрики в агрегированном 
состоянии из Prometheus и персистит в Cassandra. 
Данное решение позволит упростить процесс анализа метрик в дальнейшем, за счет их предварительной обработки. 
Из-за этого разница в производительности при 
использовании гипервизоров относительно обычной ОС достигает 5-15% в зависимости от типа используемого ПО и конфигурации системы. Есть несколько реализаций данного подхода: KVM (в т.ч. 
OVirt), Xen. 
Заключение 
Таким образом, в настоящей статье исследован 
новый способ оркестрации процессов управления 
контейнерами, основанный на анализе состояния 
узлов планировщиком, реализованным с использованием алгоритмов машинного обучения. Использование следующего планировщика позволяет повысить утилизацию и уменьшить время доступа к 
контейнерам. В качестве дальнейших исследований 
предлагается увеличить количество параметров, по 
которым будет определяться действия deploy 
manager. 

ISSN 2410-6070 
Международный научный журнал «Инновационная наука» 
№ 10-2 / 2023 
 
 
 
37 
в зимних условиях. Томск: Издательство ТГАСУ, 2014. -412с. 
6. В.Д. Копылов. Устройство монолитных бетонных конструкций при отрицательных температурах среды 
М.: Изд-во АСВ, 2014. - 184с. 
© Сергеев О.И., 2023 
 
 
 
 
УДК 62 
Филисов Д.А. 
Руководитель команды разработки Grid Dynamics 
Белград, Сербия 
 
ВЫСВОБОЖДЕНИЕ ПРОИЗВОДИТЕЛЬНОСТИ: ГЛУБОКОЕ ПОГРУЖЕНИЕ  
В ПРИЛОЖЕНИЯ С ВЫСОКОЙ НАГРУЗКОЙ 
 
Аннотация 
В этой статье рассматривается область параллелизма в серверных архитектурах и его оптимизация 
для сценариев с высоким трафиком. Цель состоит в том, чтобы предоставить разработчикам всестороннее 
представление о стратегиях и методах, которые можно использовать для эффективной обработки 
больших объемов трафика, обеспечивая при этом оптимальную производительность, оперативность 
реагирования и масштабируемость. 
Статья начинается с содержательного введения, в котором подчеркивается важность серверной 
архитектуры, способной управлять интенсивным трафиком от одновременных пользователей. Это 
подчеркивает необходимость того, чтобы разработчики оптимизировали свою серверную инфраструктуру 
для удовлетворения требований высокой активности пользователей. 
Далее в статье исследуется концепция параллельной обработки, управляемой событиями, 
демонстрируются практические примеры кода на JavaScript и Node.js. Это демонстрирует, как 
разработчики могут использовать функциональные блоки и возможности цикла обработки событий для 
эффективного управления выполнением кода. 
В статье далее рассматривается дедупликация запросов как метод оптимизации для уменьшения 
избыточных запросов и улучшения воспринимаемой задержки. В нем освещаются сценарии, в которых 
выполняется несколько идентичных запросов, и преимущества дедупликации запросов при обработке 
"горячих" данных. 
Кроме того, в статье обсуждается использование пакетной записи в качестве еще одной 
эффективной стратегии параллелизма на стороне сервера. В нем рассматриваются преимущества 
объединения нескольких операций записи в пакеты, снижающие нагрузку как на приложение, так и на 
базу данных. 
В заключение статьи подчеркиваются преимущества разработки кода, эффективно использующего 
параллелизм. Это подчеркивает важность выбора бессерверных вычислительных решений для 
использования параллелизма на стороне приложения, позволяющего внедрять эффективные шаблоны, 
которые полностью используют параллелизм. 
Понимая и внедряя концепции и стратегии, представленные в этой статье, разработчики могут 
раскрыть неиспользованный потенциал своих приложений, добиваясь повышения производительности, 
масштабируемости и отзывчивости в сценариях с высоким трафиком. 
ISSN 2410-6070 
Международный научный журнал «Инновационная наука» 
№ 10-2 / 2023 
 
 
 
38 
Ключевые слова:  
параллелизм, архитектура сервера, высокий трафик, оптимизация, обработка на основе событий, 
дедупликация запросов, пакетная запись, производительность приложений, 
 масштабируемость, отзывчивость. 
 
PERFORMANCE UNLEASHED: A DEEP DIVE INTO HIGH LOAD APPLICATIONS 
 
Аbstract 
This article delves into the realm of parallelism in server architectures and its optimization for high-traffic 
scenarios. The goal is to provide developers with a comprehensive understanding of the strategies and 
techniques that can be employed to efficiently handle large volumes of traffic while ensuring optimal 
performance, responsiveness, and scalability. 
The article begins with an insightful introduction that emphasizes the importance of server-side 
architecture capable of managing intensive traffic from concurrent users. It highlights the need for developers 
to optimize their server infrastructure to meet the demands of high user activity. 
Subsequently, the article explores the concept of parallel event-driven processing, showcasing practical 
code examples in JavaScript and Node.js. It demonstrates how developers can leverage functional blocks and 
exploit the event loop's capabilities to efficiently manage code execution. 
The text further investigates query deduplication as an optimization technique to reduce redundant 
queries and improve perceived latency. It highlights scenarios where multiple identical requests occur and the 
benefits of deduplicating queries in handling hot data. 
Additionally, the article discusses the utilization of batched writes as another effective strategy for serverside parallelism. It explores the advantages of aggregating multiple write operations into batches, reducing the 
load on both the application and the database. 
Finally, the article concludes by emphasizing the rewards of developing code that effectively utilizes 
parallelism. It underscores the importance of choosing serverless compute solutions to leverage application-side 
parallelism, allowing the adoption of efficient patterns that fully harness parallelism. 
By understanding and implementing the concepts and strategies presented in this article, developers can 
unlock the untapped potential of their applications, achieving enhanced performance, scalability, and 
responsiveness in high-traffic scenarios. 
Кeywords: 
parallelism, server architecture, high-traffic, optimization, event-driven processing, query deduplication, 
batched writes, application performance, scalability, responsiveness. 
 
Введение 
В области разработки программного обеспечения построение клиент-серверного приложения 
требует создания серверной архитектуры, способной беспрепятственно управлять интенсивным 
трафиком, исходящим от значительного числа одновременных пользователей. Разработчики активно 
стремятся 
оптимизировать 
свою 
серверную 
инфраструктуру, 
чтобы 
обеспечить 
надежную 
производительность даже в условиях интенсивной активности пользователей.  
В настоящей статье предпринята попытка глубже проникнуть в многогранную область 
параллелизма, предлагая наглядные примеры для объяснения сложных стратегий, которые можно 
использовать для эффективного обслуживания больших объемов трафика. Эти тщательно разработанные 
стратегии позволяют разработчикам максимально эффективно использовать параллелизм при 
развертывании приложения на таких платформах, как AWS App Runner, AWS Fargate или любая другая 
вычислительная платформа, поддерживающая одновременную обработку множества запросов единым 
прикладным процессом. 
ISSN 2410-6070 
Международный научный журнал «Инновационная наука» 
№ 10-2 / 2023 
 
 
 
39 
Параллелизм приобретает первостепенное значение при проектировании и оптимизации 
приложений, подвергающихся значительным рабочим нагрузкам. Применяя разумные стратегии 
параллелизма, разработчики могут гарантировать, что их приложения демонстрируют оптимальную 
производительность, отзывчивость и масштабируемость. Эта статья посвящена всестороннему 
исследованию различных подходов, которые раскрывают истинный потенциал приложений, работающих 
в сценариях с высоким трафиком, тем самым проясняя оптимальное использование параллелизма. 
Обзор литературы 
Концепция параллелизма и ее применение в серверных архитектурах является широко изучаемой 
темой в области компьютерных наук. Способность эффективно управлять обширным трафиком от 
одновременных пользователей, особенно в сценариях с высоким трафиком, является критически важным 
аспектом оптимизации серверной инфраструктуры. 
Как обсуждалось Херлихи и Шавитом (2008) [1], параллелизм играет фундаментальную роль в 
вычислительной технике, где задачи разбиваются на части и выполняются одновременно для повышения 
производительности системы. Эта концепция приобретает все большее значение в серверной среде в 
связи с распространением веб-приложений и сервисов, требующих высокой скорости реагирования и 
масштабируемости. 
Параллельная обработка, управляемая событиями. 
Технология параллельной обработки, управляемой событиями, стала ключевой стратегией при 
работе со сценариями с высоким трафиком. Как пояснили Сурьянараянан и др. (2012) [2], архитектура, 
управляемая событиями, позволяет одновременно обрабатывать события в приложении, что приводит к 
повышению быстродействия и производительности. 
В области JavaScript и Node.js, был проведен ряд работ. Крокфорд (2008) [3] иллюстрирует 
использование функциональных блоков и цикла обработки событий для эффективного управления 
выполнением кода. Уникальная управляемая событиями неблокирующая модель ввода-вывода в Node.js 
был популярным выбором для создания масштабируемых сетевых приложений, как отметили ХьюзКраучер и Уилсон (2012) [4]. 
Дедупликация запросов 
Дедупликация запросов - еще один важный подход к улучшению параллелизма на стороне сервера. 
В работах Колдера и др. (1997) [5] и Вейкума и Воссена (2001) [6] подробно рассказывается о том, как 
устранение избыточных запросов может значительно снизить нагрузку на базу данных и повысить 
производительность системы. Необходимость дедупликации запросов, особенно в сценариях с высоким 
трафиком, также подчеркивается Бернсом и др. (2010) [7], где они обсуждают стратегии эффективной 
обработки "горячих" данных. 
Пакетные записи 
Пакетная запись — это еще одна важная стратегия, выявленная в области параллелизма на стороне 
сервера. Она включает в себя группировку нескольких операций записи в один пакет, тем самым снижая 
нагрузку на приложение и базу данных (Gray et al., 1996) [8]. Более того, Чанг и др. (2008) [9] объясняют 
основополагающие принципы распределенной системы хранения структурированных данных, где они 
подчеркивают преимущества пакетной обработки. 
Бессерверные вычислительные решения и параллелизм на стороне приложения 
В последние годы большое внимание также уделяется бессерверным вычислениям и параллелизму 
на стороне приложений. Бессерверная архитектура, описанная Робертсом (Roberts, 2016) [10], позволяет 
разработчикам создавать и запускать приложения, не думая о серверах, что обеспечивает возможность 
автоматического масштабирования и взимания платы только за затраченное вычислительное время. 
Джонас и др. (2019) [11] далее исследуют, как можно эффективно использовать параллелизм на стороне 
приложения в бессерверных средах, подчеркивая преимущества внедрения эффективных шаблонов для 
полного использования параллелизма. 
ISSN 2410-6070 
Международный научный журнал «Инновационная наука» 
№ 10-2 / 2023 
 
 
 
40 
В заключение, этот обзор литературы иллюстрирует различные подходы и методы, доступные 
разработчикам для обработки сценариев с высоким трафиком за счет эффективного параллелизма на 
стороне сервера. В последующих разделах этой статьи мы более подробно рассмотрим практическую 
реализацию этих стратегий с целью дать разработчикам возможность лучше оптимизировать свою 
серверную инфраструктуру и раскрыть неиспользованный потенциал своих приложений. 
Основы параллелизма: Изучение циклов обработки событий и параллельного кода 
В основе оптимизации при высоком трафике лежит фундаментальная концепция, согласно которой 
единый серверный прикладной процесс может эффективно обслуживать множество клиентов 
одновременно. Чтобы понять это явление, крайне важно вникнуть в работу серверного прикладного 
процесса. Как правило, серверные приложения получают HTTP-запросы, выполняют определенные 
операции в ответ на эти запросы и впоследствии предоставляют соответствующий ответ клиенту (рис. 1). 
 
 
Рисунок 1 - Жизненный цикл одного HTTP-запроса  
Figure 1 - The lifecycle of a single HTTP request 
 
Жизненный цикл одного HTTP-запроса можно представить следующим образом: при получении 
HTTP POST-запроса серверный процесс проверяет входящую полезную нагрузку JSON. Впоследствии 
серверный процесс инициирует SQL-запрос для вставки строки данных в базу данных, тем самым сохраняя 
полученную полезную нагрузку. На следующем этапе серверный процесс ожидает ответа от базы данных, 
чтобы убедиться в успешном выполнении операции вставки. После этого серверный процесс выполняет 
вызов другой нисходящей службы, потенциально информируя ее о вновь сохраненном объекте в базе 
данных. Затем серверный процесс ожидает результата этого вызова, гарантируя его успех. Наконец, 
серверный процесс отвечает на исходный HTTP-запрос, выдавая код состояния 200 OK. 
В целом весь этот процесс обычно занимает около 120 миллисекунд. Однако при ближайшем 
рассмотрении становится очевидным, что подавляющее большинство этого времени не отводится на 
выполнение инструкций процессора. Вместо этого он в основном тратится на ожидание сетевых операций 
ввода-вывода (рис. 2).  
 
 
Рисунок 2 - Время ожидание сетевых операций ввода-вывода 
Figure 2 - Waiting time for network I/O operations 
 
Из общих 120 миллисекунд всего 10 миллисекунд фактически отводятся на выполнение кода 
приложения, в то время как оставшееся время тратится на холостой ход. Такая неэффективность вызывает 
озабоченность. 
Для этого предлагается альтернативный подход, продемонстрированный на рисунке 3. 
ISSN 2410-6070 
Международный научный журнал «Инновационная наука» 
№ 10-2 / 2023 
 
 
 
41 
 
Рисунок 3 - Альтернативный подход операций 
Figure 3 - Alternative approach of operations 
 
В альтернативном подходе, когда приложение сталкивается с моментом ожидания операции 
сетевого ввода-вывода, оно может воспользоваться возможностью обработать другой входящий HTTPзапрос. За счет поэтапного выполнения инструкций кода процесс подачи заявки получил бы возможность 
обрабатывать несколько входящих запросов одновременно. 
Большинство современных языков прикладного программирования включают встроенный цикл 
обработки событий, способный облегчить этот точный рабочий процесс. Цикл обработки событий 
работает как вечный цикл, непрерывно отслеживая события и реагируя на них. Эти события могут 
проявляться в виде HTTP-запроса, отправленного на сервер, получения ответа из базы данных или 
завершения операции сохранения файла. Если событие происходит во время выполнения другого кода, 
оно ставится в очередь. 
Всякий раз, когда блок кода завершает свое выполнение, цикл обработки событий извлекает 
следующее событие из очереди и начинает выполнение соответствующего блока кода. Это позволяет 
циклу обработки событий поддерживать производительность процесса за счет постоянного приема 
событий от нескольких клиентов и выполнения связанного с ними кода.  
Цикл обработки событий можно представить как планировщик задач в процессе выполнения, 
управляющий многочисленными небольшими фрагментами кода, для выполнения каждого из которых 
требуется всего лишь доля миллисекунды. Имея в распоряжении тысячу миллисекунд в секунду, 
планировщик цикла обработки событий эффективно использует это время для обработки входящих 
событий (рис. 4). 
 
Рисунок 4 - Цикл обработки событий 
Figure 4 - Event processing cycle 
 
Эффективно используя возможности циклов обработки событий и параллельного выполнения кода, 
разработчики могут раскрыть потенциал для существенного повышения производительности и 
повышения быстродействия в сценариях с высоким трафиком. В последующих разделах мы рассмотрим 
передовые стратегии и рекомендации по оптимизации параллелизма в серверных архитектурах, 
позволяющие разработчикам раскрыть истинные возможности своих приложений, когда они 
сталкиваются со сложными рабочими нагрузками. 
ISSN 2410-6070 
Международный научный журнал «Инновационная наука» 
№ 10-2 / 2023 
 
 
 
42 
Практические примеры кода: JavaScript & Node.js 
Чтобы проиллюстрировать реализацию кода параллельного цикла обработки событий в JavaScript и 
Node.js, рассмотрим следующие практические примеры (рис. 5, 6). Эти примеры демонстрируют 
использование функциональных блоков и демонстрируют способность цикла обработки событий 
эффективно управлять выполнением кода. 
 
Рисунок 5 - Использование времени простоя цикла обработки событий 
Figure 5 - Using the idle time of the event processing cycle 
 
 
Рисунок 6 - Использование возможностей асинхронных функций 
Figure 6 - Using the capabilities of asynchronous functions 
 
Эти примеры подчеркивают способность цикла обработки событий эффективно управлять 
выполнением кода за счет разумного использования периодов простоя и предоставления контроля во 
время асинхронных операций. Используя такие методы, разработчики могут повысить общую 
отзывчивость и пропускную способность своих JavaScript и Node.js приложения, эффективно 
использующие возможности параллелизма. 
Высвобождение возможностей дедупликации выборки 
После успешного внедрения программного кода, способного одновременно выполнять несколько 
асинхронных задач, открывается доступ к множеству замечательных оптимизаций, которые используют 
этот новообретенный параллелизм. 
Одним из особенно увлекательных методов является дедуплицирование выборки. Исследуем, как 
работает этот метод, рассмотрев следующий жизненный цикл запроса, который отражает поведение 
многих REST API (рис. 7). 
 
Рисунок 7 - Поведение многих REST API 
Figure 7 - Behavior of many REST APIs 
ISSN 2410-6070 
Международный научный журнал «Инновационная наука» 
№ 10-2 / 2023 
 
 
 
43 
1. Клиент инициирует запрос GET для получения сведений об учетной записи пользователя с 
идентификатором 1. 
2. Серверный процесс отправляет SQL-запрос для получения соответствующих сведений об учетной 
записи из базы данных. 
3. Примерно через 10 миллисекунд поступает ответ из базы данных. 
4. Сервер приложений сериализует ответ в формате JSON и отправляет его обратно в виде HTTPответа. 
Этот поток представляет собой обычное явление в клиентских/серверных приложениях. Однако 
также представим другой сценарий, в котором поступает второй запрос для того же пользовательского 
объекта, в то время как первый запрос все еще ожидает ответа на SQL-запрос (рис. 8). 
 
Рисунок 8 - Альтернативный сценарий событий 
Figure 8 - Alternative scenario of events 
 
1. Клиент отправляет запрос GET на получение сведений об учетной записи пользователя с 
идентификатором 1. 
2. Серверный процесс отправляет SQL-запрос для получения сведений о пользователе из базы 
данных. 
3. Примерно через 5 миллисекунд ответ на SQL-запрос еще не поступил, но поступает второй 
клиентский запрос для того же пользовательского объекта. 
4. Вместо отправки избыточного SQL-запроса сервер идентифицирует текущий SQL-запрос и 
связывает второй запрос с ответом на первый запрос. 
5. Оба HTTP-запроса выполняются с использованием одних и тех же данных, полученных из 
исходного SQL-запроса. 
С точки зрения клиентов, инициирующих эти запросы, задержка первого запроса на стороне сервера 
составляет 10 миллисекунд, в то время как задержка второго запроса составляет всего 5 миллисекунд, 
поскольку он использует уже выполняющийся SQL-запрос в течение 5 миллисекунд. 
Эта стратегия оказывается исключительно эффективной для приложений с "горячими" данными, где 
множество клиентов одновременно запрашивают одни и те же данные. Реализация дедупликации 
выборки не только снижает нагрузку на базу данных, но и значительно улучшает воспринимаемую 
задержку. Во многих случаях один запрос может обслуживать несколько идентичных запросов. Однако 
важно отметить, что эту стратегию не следует применять в сценариях, требующих строгой 
согласованности, таких как получение баланса банковского счета в режиме реального времени. 
Для дальнейшей оптимизации дедупликации выборки вы можете использовать сеть доставки 
контента (CDN) перед вашим API, которая помогает дедуплицировать выборки на уровне HTTP-запроса. 
Однако все еще могут быть случаи, когда проскальзывает несколько идентичных запросов. Например, 
ISSN 2410-6070 
Международный научный журнал «Инновационная наука» 
№ 10-2 / 2023 
 
 
 
44 
если ваш CDN имеет несколько пограничных местоположений, каждое местоположение должно 
извлекать данные с исходного сервера приложений. Аналогичным образом, многочисленные 
параллельные запросы могут одновременно извлекать одни и те же базовые данные. В таких случаях 
дедупликация выборки служит дополнительным уровнем оптимизации, повышая скорость вашего 
сервиса и одновременно снижая нагрузку на базовый уровень данных. 
Использование возможностей пакетной записи 
Еще одна область, где можно эффективно использовать параллелизм на стороне сервера, - это 
операции на стороне записи. Давайте рассмотрим типичный жизненный цикл запроса на стороне сервера 
(рис. 9). 
 
Рисунок 9 - Цикл запроса на стороне сервера 
Figure 9 - Server-side request cycle 
 
1. Клиент инициирует запрос POST или PUT с полезной нагрузкой JSON для сохранения данных. 
2. Серверное приложение анализирует полезную нагрузку JSON и отправляет запрос SQL update для 
сохранения данных в базе данных. 
3. База данных подтверждает завершение SQL-запроса. 
4. Серверное приложение отвечает клиенту кодом состояния 200 OK, указывающим на успешный 
запрос. 
Теперь рассмотрим альтернативный поток, который использует параллелизм на стороне сервера 
(рис. 10). 
 
Рисунок 10 - Альтернативный поток, используемый параллелизм на стороне сервера 
Figure 10 - Alternative flow used by server-side concurrency 
 
1. Клиент отправляет запрос POST или PUT с полезной нагрузкой JSON для сохранения данных. 
2. Вместо того чтобы немедленно отправить SQL-запрос, сервер вводит период ожидания, 
возможно, в течение 10 миллисекунд. 
3. В течение этого периода в 10 мс на сервер поступают дополнительные запросы POST и PUT. 
4. Сервер агрегирует эти запросы и отправляет один массовый запрос в базу данных либо по 
истечении периода времени в 10 мс, либо когда "пакет" заполняется достаточным количеством объектов. 
5. После успешного завершения массового запроса все запросы получают 200 одобрительных 
ответов. 
ISSN 2410-6070 
Международный научный журнал «Инновационная наука» 
№ 10-2 / 2023 
 
 
 
45 
Эта стратегия может привести к небольшой задержке операций записи, но она значительно снижает 
нагрузку как на серверное приложение, так и на базу данных. Его эффективности способствуют несколько 
факторов, в том числе: 
1. Оптимизация пакетов TCP: в большинстве конфигураций пакет TCP может содержать до 64 
КБАЙТ. Отправка нескольких обновлений в рамках более крупного TCP-пакета более эффективна, чем 
передача нескольких пакетов меньшего размера. 
2. Эффективность сериализации и десериализации: пакетная обработка данных более эффективна 
с точки зрения сериализации и десериализации, что приносит пользу как серверу, так и базе данных. 
3. Эффективное сохранение: когда база данных сохраняет данные на носителе информации 
(например, на вращающемся жестком диске или твердотельном накопителе), выполнение однократной 
более масштабной очистки для долговременного хранения оказывается более эффективным. 
Также преимущества этой стратегии еще больше проявляются по мере увеличения параллелизма 
серверного приложения. Для оптимизации его производительности можно настроить два основных 
параметра: 
1. Размер пакета: определяет максимальное количество объектов, которое может накопиться 
перед запуском отправки пакетного запроса. 
2. Задержка партии: указывает период ожидания поступления дополнительных товаров перед 
отправкой партии, даже если она еще не заполнена. Меньшая задержка отправки партии приводит к 
более быстрой отправке партии, в то время как более высокая задержка позволяет накапливать больше 
товаров.  
По мере увеличения объема входящих запросов пакеты заполняются и отправляются быстрее, что 
приводит к снижению воспринимаемой задержки. Скорость, с которой выполняются вызовы к 
нижестоящей базе данных или службе, увеличивается ступенчато по мере увеличения параллелизма. 
Например, если размер пакета установлен равным 10, количество обращений к нижестоящей службе 
увеличивается со скоростью 1/10 по сравнению с общим объемом входящих запросов (рис. 11). 
 
Рисунок 11 - Воспринимаемая задержка 
Figure 11 - Perceived delay 
 
Многие базы данных и последующие службы обеспечивают поддержку пакетных операций. 
Примечательно, что на платформе AWS основные сервисы, такие как Amazon DynamoDB (BatchWriteItem 
и BatchGetItem), Amazon Simple Queue Service (SendMessageBatch) и Amazon Simple Notification Service 
(PublishBatch), предлагают пакетные конечные точки, которые можно использовать для максимального 
использования преимуществ пакетной записи. 
Заключение 
Разработка кода, который может эффективно использовать параллелизм, может представлять 
ISSN 2410-6070 
Международный научный журнал «Инновационная наука» 
№ 10-2 / 2023 
 
 
 
46 
трудности, но награда существенна — высокоэффективное приложение, способное обрабатывать 
увеличенный трафик. Хотя писать код в вычислительной среде, подобной AWS Lambda, относительно 
просто, где каждый запрос может быть изолирован от других, недостаток заключается в невозможности 
эффективно использовать несколько параллельных запросов в рамках одной системы параллелизма. 
Однако выбор в пользу бессерверных вычислительных решений, таких как AWS App Runner или AWS 
Fargate, предоставляет возможность параллелизма на стороне приложения, позволяя внедрять 
эффективные шаблоны, которые полностью используют параллелизм. 
 
Список использованной литературы: 
1. Херлихи М., Шавит Н. Искусство многопроцессорного программирования. Морган Кауфманн. 2008. 
2. Сурьянараянан Р., Джусак Дж., Верхоф М., Майклс П. Архитектура, управляемая событиями, для сервисориентированных архитектур: обзор литературы. Транзакции IEEE по разработке программного 
обеспечения, 2012. 38(2), 173-186. 
3. Крокфорд Д. JavaScript: Хорошие стороны. O'Reilly Media, Inc., 2008. 
4. Хьюз-Краучер Т., Уилсон М. Работают с Node.js . O'Reilly Media, Inc., 2012. 
5. Колдер Б., Кринц С., Джон С., Остин Т. Размещение данных с учетом кэширования. В материалах 
восьмой международной конференции по архитектурной поддержке языков программирования и 
операционных систем. 1997. стр. 139-149. 
6. Вейкум Г., Воссен Г. Транзакционные информационные системы: теория, алгоритмы и практика 
управления параллелизмом и восстановления. Морган Кауфманн. 2001. 
7. Бернс Э., Адальберт У., Коннатсер С. Высокопроизводительные веб-сайты: необходимые знания для 
разработчиков интерфейсов. "О'Рейли Медиа, Инк.". 2010. 
8. Грей Дж., Хелланд П., О'Нил П., Шаша Д. Опасности тиражирования и решение. В материалах 
международной конференции ACM SIGMOD по управлению данными 1996 года. 1996. стр. 173-182. 
9. Чанг Ф., Дин Дж., Гемават С., Се В. С., Уоллах Д. А., Берроуз М., ... Грубер Р. Э. Bigtable: Распределенная 
система хранения структурированных данных. Транзакции ACM в компьютерных системах (TOCS), 2018.  
26(2), стр. 1-26. 
10. Робертс М. Бессерверные архитектуры. MartinFowler.com. [Электронный ресурс] - Режим доступа: 
https://martinfowler.com/articles/serverless.html . 2016 год 
11. Джонас Э., Шлейер-Смит Дж., Сриканти В., Цай К. С., Ханделвал А., Пу К., ... Раган-Келли Дж. 
Упрощенное облачное программирование: взгляд Беркли на бессерверные вычисления. Препринт arXiv 
arXiv: 1902.03383. 2019. 

Intellectual Technologies on Transport. 2022. No 1 
DOI: 10.24412/2413-2527-2022-129-16-23 
 
Интеллектуальные технологии на транспорте. 2022. № 1 
16
 
Особенности технологий 
бессерверных вычислений 
А. Т. Гордина, к.и.н. А. В. Забродин 
Петербургский государственный университет путей сообщения Императора Александра I 
Санкт-Петербург, Россия 
anngordtm@gmail.com, teach-case@yandex.ru 
 
Аннотация. Статья посвящена одному из перспективных 
направлений в области облачных вычислений, известному 
под названием Serverless Computing (бессерверные вычисления). 
Рассматриваются 
технологии, 
применяемые 
для 
Serverless, дается сравнение облачных моделей и поставщиков услуг, описание основных компонентов архитектурного 
шаблона FaaS (функций, ресурсов и событий). Показаны преимущества и недостатки технологии Serverless-вычислений, а 
также раскрываются проблемы миграции приложений с локальных серверов в облако, и как при этом из процесса разработки программного продукта исключаются задачи по 
управлению ресурсами и их оптимизации. 
Ключевые слова: облачные вычисления, бессерверные вычисления, модель FaaS, функции, автомасштабирование, архитектура приложения. 
ВВЕДЕНИЕ 
Для функционирования и организации работы любой 
информационной системы необходима инфраструктура, 
которая позволит объединить, организовать и трансформировать информацию в соответствии с поставленными 
задачами. Инфраструктура является комплексом взаимосвязанных информационных сервисов и систем, таких 
как специализированное программное обеспечение, серверное, сетевое и компьютерное оборудование, СУБД, 
каналы связи, персонал и многое другое, необходимых 
для функционирования и развития инструментов информационного взаимодействия в компании. Создание эффективной инфраструктуры — это довольно сложный 
процесс, требующий значительных ресурсов (как финансовых, так и временных), а также высокого уровня компетенций. Одним из направлений снижения капитальных затрат на инфраструктуру, а также повышения гибкости информационных задач и процессов является использование облачных технологий (хостинга). Период 
пандемии и массовый переход на удаленную работу способствовали росту востребованности облачных платформ. 
Несомненно, миграция в облако также приносит проблемы в планировании производительности и управлении 
операциями серверной части [1]. Так провайдеры облачных услуг стали развивать решения, в которых облако автоматически и динамически занимается распределением 
вычислительных ресурсов. Одним из таких решений является 
стратегия 
бессерверных 
вычислений.  
В парадигме Serverless весь процесс разработки программного обеспечения превращается в совокупность рабочих процессов с администрированием извне, что позволяет не акцентировать внимание на инфраструктуре вокруг, а сфокусироваться на самом процессе разработки. 
Так, Serverless решения становятся следующим этапом 
развития архитектуры вычислительных систем в эпоху 
облаков. 
МОДЕЛЬ ОБСЛУЖИВАНИЯ «КАК УСЛУГА» 
Сущность облачного хостинга заключается в том, что 
физические серверы провайдера соединяются в единый 
пул, мощности которого могут гибко распределяться 
между клиентами в зависимости от их необходимостей. 
Облачный рынок в настоящее время продолжает развиваться, расширяются модели облачных вычислений [2]. 
При планировании использования облаков нужно понимать, какие облачные сервисы существуют, чтобы остановить свой выбор на правильном и наиболее подходящем решении в пользу той или иной модели работы, что позволит 
найти необходимый баланс между гибкостью и простотой. 
Неотъемлемыми характеристиками при выборе инфраструктурной платформы являются: 
• увеличение скорости и качества разработки; 
• автоматизация обслуживания и эксплуатации; 
• безопасность и стабильность. 
Концепция «как услуга» (англ. as-a-Service, aaS) подразумевает использование услуги с применением облачных 
технологий по необходимости. Каждая модель IT-инфраструктуры, которая поставляется в этом формате, уникальна по-своему. Рассмотрим самые востребованные aaSсервисы, представленные на рынке облачных вычислений. 
Модель обслуживания «инфраструктура как услуга» 
(англ. Infrastructure-as-a-Service, IaaS) стала первым шагом 
в модернизации IT-инфраструктуры при переходе от физических к виртуальным серверам. В 2012 году поставщики 
«инфраструктуры как услуги» занимали ведущие позиции 
в развитии облачных вычислений [3]. 
IaaS-провайдеры располагают услугами разного масштаба. Есть возможность арендовать единственный виртуальный сервер или пул виртуальных серверов с возможностью их объединения в виртуальную сеть. 
Парадигма IaaS предоставляет полные административные права внутри арендованных виртуальных серверов. 
Все, что касается настройки операционных систем арендованных серверов, необходимо выполнять самостоятельно. 
Изначально IaaS-провайдер лишь гарантирует, что ваш сервер будет доступен по сети в соответствии с соглашением об 
уровне услуг (англ. Service Level Agreement, SLA) [4]. 
Преимуществом в выборе IaaS является тот факт, что 
такая модель подходит для случаев, когда значение имеет 
масштабируемость. IaaS также может быть полезна для 
приложений с постоянной нагрузкой или необходимостью 
наращивать вычислительные ресурсы. 
Intellectual Technologies on Transport. 2022. No 1 
 
 
Интеллектуальные технологии на транспорте. 2022. № 1 
17
 
Основной недостаток IaaS: по-прежнему остается необходимость и ответственность за управление безопасностью. Кроме того, IaaS-провайдеры предоставляют только 
серверы и API, а настройкой всего остального приходится 
заниматься самостоятельно. 
Создание качественного программного обеспечения зависит от информационно-технологических платформ: 
среды разработки и тестирования, операционных систем, 
средств для развертывания, инструментов управления базами данных. Модель «платформа как услуга» (англ. Platform-as-a-Service, PaaS) позволяет размещать готовые программы на хостинге с поддержкой всего необходимого. 
Комплект сервисов PaaS передает ответственность по 
управлению всей информационно-технологической инфраструктурой провайдеру. Облачный провайдер берет на себя 
установку всех зависимостей операционной системы, необходимых для поставляемой версии среды выполнения. 
Остается только сосредоточиться на создании приложения, 
которое работает в выбранной среде выполнения. 
Zimki (2006) и Heroku (2007) были первыми компаниями, которые использовали модель разработки «платформа 
как услуга» [5]. В то время было ясно, что такой подход 
значительно ускоряет время разработки и сокращает расходы на обслуживание. Это позволяет разработчикам 
настраивать и управлять приложениями, значительно сокращая время от начала разработки до выпуска продукта и 
последующей его поддержки. 
Недостаток PaaS-модели заключается в том, что управление сфокусировано лишь на том, что построено на самой 
платформе. Провайдер берет ответственность за всю физическую инфраструктуру, а также администрирование на 
уровне операционных систем. Соответственно, PaaS предоставляет меньше управления над вычислительной инфраструктурой в сравнении с IaaS. 
На данный момент практика показывает, что PaaS как 
отдельная модель в мире облачных услуг утратила свою актуальность. На смену независимым PaaS-платформам пришли PaaS-подобные сервисы [6]. 
Разработка программного продукта требует оптимального распределения ресурсов, при котором обеспечивается 
их наиболее эффективное использование, что позволяет 
внедрять инновации, снижать риски и сокращать время выхода продукта на рынок. 
Приложение может использовать сотни серверов, и выбор Serverless для проектирования архитектуры приложения позволит избавится от проблем, с которыми сталкиваются при управления серверами. Serverless вовсе не означает отказ от сервера, данная парадигма позволяет разработчикам не беспокоиться о его настройке и администрировании. 
В 2014 году Amazon анонсировал AWS Lambda, что 
позволило Serverless выйти на новый уровень [7]. Данный 
бессерверный, управляемый событиями вычислительный 
сервис представил иную тенденцию к минимизации кода и 
фокусировании на нем. Так появился архитектурный шаблон «функция как услуга» (англ. Function-as-a-Service, 
FaaS), который позволяет сократить необходимую инфраструктуру и инвестиции. 
Модель FaaS вводит новый уровень абстракции, в котором исчезает физическая инфраструктура и архитектура 
программного обеспечения, остаются только «функции», 
работающие по необходимости. FaaS предоставляет 
наибольшую гибкость при регулировании производительностью: в период простоя функция не потребляет ресурсы, 
а при необходимости платформа может быстро выделить 
необходимую мощность, которая справится с почти любой 
нагрузкой. 
РАЗВИТИЕ ОБЛАЧНЫХ ВЫЧИСЛЕНИЙ 
Модель облачных вычислений основывается на традиционном IT-стеке ПО, который состоит из пяти уровней: 
аппаратное обеспечение, виртуализация, операционная система, среда выполнения, приложение. 
Уровень аппаратного обеспечения включает все вычислительные, сетевые и ресурсы хранения. Уровень выше — 
уровень виртуализации, где происходит виртуализация 
всех необходимых аппаратных ресурсов с помощью гипервизора для создания виртуальных машин для горизонтального масштабирования. Следующий уровень — операционная система. Здесь располагается программное обеспечение, которое находится на самом деле на удаленном компьютере и позволяет запускать программы. Вдобавок к этому 
запускается среда выполнения. Она является неотъемлемым связующим звеном между операционной системой и 
приложением [8]. И также будет средой, где работает приложение. Следующий уровень — уровень приложения — 
включает в себя всю бизнес-логику. Команды разработчиков проводят на данном уровне большую часть своего времени, потому что именно здесь происходит проектирование, создание и развертывание приложения. 
IT-стек в стандартном виде сложен в обслуживании для 
компании, отказ на одном из уровней может повлиять на 
любой другой. Но с приходом облачных моделей подход к 
управлению изменился (рис. 1). 
Рис. 1. Развитие облачных моделей 
IaaS на самом деле абстрагирует нижние уровни инфраструктуры. Они запускаются от провайдера облачных 
услуг, что позволяет быстрее подготовить и запустить приложение. 
PaaS поднимает уровень абстракции еще дальше. Используя данную модель, разработчики беспокоятся только 
о своем приложении: это единственное, что нуждается в 
поддержке. Пропадает необходимость думать о среде выполнения и операционной системе. 
Intellectual Technologies on Transport. 2022. No 1 
 
 
Интеллектуальные технологии на транспорте. 2022. № 1 
18
 
FaaS позволяет абстрагировать приложение и позволяет 
сосредоточиться на функциях. Они представляют собой отдельные компоненты, составляющие это приложение. Такой подход помогает сфокусироваться на разработке без 
необходимости эксплуатации или управления серверами. 
ВЫБОР SERVERLESS FAAS 
Одна из особенностей Serverless-архитектуры — отсутствие необходимости общения с серверами напрямую. Поэтому помимо термина «бессерверный» прибегают к его синониму — «беcхостовый» (hostless). Положение, которое исключает централизованную архитектуру, исключает расходы 
на обслуживание парка серверов. Hostless к тому же означает, 
что архитектура становится эластичной [9]. Это освобождает 
от ручного управления ресурсами, поэтому многие проблемы, 
связанные с распределением ресурсов, исчезают. 
Идея, которую представляет модель FaaS, на самом 
деле проста. Традиционное программирование основано на 
функциях. Они обеспечивают предсказуемый результат и 
не меняют ничего в самой программе, что выходит за пределы функции. Сами функции являются «stateless», то есть 
не сохраняют состояние. Stateless также предполагает, что 
количество ошибок существенно уменьшается. 
Модель FaaS предоставляет клиентам выполнение кода 
в облаке, а все операции по серверной части берут на себя 
провайдеры. Клиенты отвечают лишь за свои данные и 
функции, которые выполняются. 
Одной из причин, по которой предпочтение отдают использованию Serverless FaaS, является возможность миновать управление процессами на уровне операционной системы и администрирования серверов. Некоторые сервисы 
PaaS также предоставляют эту возможность. В действительности концепции PaaS и FaaS похожи, но большинство 
приложений PaaS не ориентированы на то, чтобы запускать 
приложения в ответ на события, а платформы FaaS делают 
именно это. 
Ключевое отличие FaaS от PaaS — масштабируемость 
приложения, основным назначением которого является задача смягчить проблемы, возникающие при сбое или перегрузке приложения. При использовании PaaS, как правило, 
необходимость думать о том, как масштабировать, все еще 
присутствует. Так, например, в одной из наиболее используемых облачных PaaS-платформ — Heroku — представлены «dynos» — изолированные виртуальные Linux-контейнеры, которые предназначены для выполнения кода по 
запросу заданной от пользователя команды. То есть один 
dyno — один запущенный экземпляр приложения. И 
сколько dynos необходимо запустить? С FaaS же этот процесс становится прозрачным. Даже если использовать автомасштабирование в PaaS, приложение FaaS все еще эффективнее, когда дело доходит до затрат. 
Еще одной абстракцией, которая активно используется 
в наше время, являются контейнеры приложений. Эта технология обеспечивает приложению среду, отдельную от 
операционной системы, позволяющую работать независимо от платформы. Системы управления контейнеризованными рабочими нагрузками и сервисами, такие как 
Mesos и Kubernetes, позволяют также абстрагировать приложения от развертывания на уровне операционной системы [10, 11]. А облачный сервис управления контейнерами Amazon ECS к тому же позволяет еще и избегать 
управления собственными серверами. Тогда стоит ли останавливать свой выбор на Serverless FaaS? Аргумент, который использовался при сравнении с PaaS, также актуален и 
для контейнеров: масштабирование все еще прозрачно и 
автоматически управляемо. А контейнерные платформы 
все еще требуют управления размером кластеров. Кроме 
того, FaaS рассматривается как наиболее подходящий выбор для событийно-ориентированной архитектуры, а при 
выборе контейнеров отдают предпочтение для компонентов, управляемых синхронным запросом с несколькими 
точками входа. 
Сочетание контейнеров и Serverless может быть и полезным. Например, если в случае сложной контейнерной 
системы есть необходимость выполнить некоторые дополнительные задачи, которые вызваны событиями. Эти задачи будет выгоднее и удобнее перенести от контейнерных 
настроек к бессерверным функциям [12]. Таким образом, 
из этого следует интегрирование и передача информации о 
состоянии между контейнерной и серверной архитектурой. 
При разработке программного обеспечения с помощью 
Serverless сначала определяется логика, необходимая для 
запуска провайдером, то есть проектируется необходимое 
количество функций. Шлюз-система API Gateway, которая 
является общей точкой входа в приложение, принимает и 
обрабатывает запросы от клиентов к функциям. Затем по 
требованию эти функции выполняются. Любые данные, которые сгенерированы функциями, возвращаются обратно 
устройству, которое их вызвало, также возможен вариант 
выгрузки во внешнее хранилище. Монолитный API заменяется функциями (рис. 2). 
Рис. 2. Схема Serverless-приложения 
Serverless-решения хорошо подходят для встраивания в 
микросервисную архитектуру. В случае небольшой кодовой базы разворачивание отдельного контейнера с операционной системой смысла никакого не имеет, так как это 
неэкономично с точки зрения использования ресурсов. В 
таком случае легче использовать функцию. Кроме того, 
возможно использовать функции как вспомогательные сервисы для тестовой среды. Каждое выполнение функции 
изолировано и эфемерно, поэтому проектирование приложения с архитектурой Serverless с использованием FaaS 
требует кроме функции еще и управления данными в хранилищах, механизмов запуска, масштабирования, логирования. В связи с этим построение Serverless не ограничивается 
Intellectual Technologies on Transport. 2022. No 1 
 
 
Интеллектуальные технологии на транспорте. 2022. № 1 
19
 
только FaaS. Провайдеры бессерверных вычислений предлагают целые экосистемы состоящие из Serverless-сервисов. 
ПРОВАЙДЕРЫ SERVERLESS 
Как правило, Serverless-вычисления представляют провайдеры облачных услуг. Бессерверные вычисления способствуют росту бизнеса по той причине, что облегчение 
программирования приложения на Serverless-архитектуре 
помогает привлекать новых клиентов, а уже существующим клиентам использовать больше возможностей облачных приложений. 
Выбирая подходящий сервис, отвечающий необходимым условиям, нужно учитывать следующие ключевые 
факторы: 
• поддержка языков программирования; 
• типы триггера функций; 
• безопасность; 
• ценообразование и факторы биллинга; 
• время выполнения и параллелизм; 
• метод развертывания. 
Serverless развивается, и вместе с ним на рынке появляется все больше возможностей и сервисов, которые предлагают провайдеры. 
AWS Lambda 
Компания 
Amazon 
была 
пионером 
в 
развитии 
Serverless-технологий. На текущий момент в рамках 
Amazon Web Services корпорация предлагает сервис для 
бессерверных вычислений AWS Lambda. 
Суть Lambda заключается в выполнении кода при реагировании на различные события. Иными словами, единственным, что требуется в использовании данного сервиса, 
является загрузка кода, который должен исполняться в 
функции, и настройка триггера или события, которое отвечает за выполнение кода. Всю остальную ответственность 
за администрирование платформы AWS Lambda полностью берет на себя. Таким образом, пропадает необходимость в обслуживании серверов, управлении операционной 
системой, выделении нужных ресурсов, масштабировании 
и мониторинга кода. В AWS Lambda от пользователя 
скрыта внутренняя реализация модели и почти все внутренние процессы, поэтому теряется актуальность безопасности данной платформы. 
Список сервисов AWS для бессерверных вычислений 
широк. Вместе с Lambda активно используется Amazon API 
Gateway, с помощью которого можно значительно сократить расходы. Изменение данных происходит в корзинах 
Amazon S3 и базах данных Amazon DynamoDB. С сервисом 
для оркестрации бессерверных вычислений AWS Step 
Functions обычно проектируется и выполняется процесс, 
который может координировать несколько функций AWS 
Lambda. Также, используя комплект средств разработки 
AWS SDK, есть возможность запускать код через вызовы 
API. Кроме того, Lambda охватывает широкий спектр языков программирования. 
Google Cloud Functions 
Следующим по распространенности поставщиком 
услуг FaaS в мире является Google Cloud Functions. Данный 
сервис также позволяет абстрагироваться от всей базовой 
инфраструктуры, что позволяет сосредоточиться на коде. 
Google Cloud Functions отлично интегрирована с сервисами Google. Сервис обеспечивает быстрый переход от 
кода к развертыванию, который включает интегрированный мониторинг. Полный обзор и диагностику приложения 
позволяет получить сервис Google Cloud Trace, предназначенный для оптимизации работы в облаке, и сервис Google 
Cloud Debugger, который проверяет состояние запущенного приложения в режиме реального времени. Кроме того, 
Cloud Functions позволяет использовать сторонние облачные сервисы, которые являются строительными блоками 
для кода. 
Ценообразование основывается на количестве вызовов 
функций. Первые два миллиона вызовов бесплатны, каждый последующий миллион вызовов — 0,40 долларов 
США. Однако Google взимает дополнительную плату за 
использование памяти и ЦП. 
Присутствует автомасштабирование: Cloud Functions 
автоматически управляют и масштабируют необходимую 
инфраструктуру в зависимости от рабочей нагрузки. 
Microsoft Azure Cloud Functions 
На текущий момент компания Microsoft является лидером среди поставщиков облачных услуг [13] и представляет платформу для бессерверных вычислений Azure Cloud 
Functions.  
Время на написание кода значительно сокращается при 
использовании триггеров и привязок, что позволяет приложениям реагировать на события и без особых проблем подключаться к другим службам. Функции Azure предоставляют создание более масштабируемых и стабильных событийно-ориентированных приложений. 
Платформа Azure для бессерверных вычислений позволяет создавать надежные приложения со встроенными 
средствами 
защиты 
и 
мониторинга. 
Сервис 
Azure 
Application Insights предоставляет средства для отслеживания и анализа производительности кода. Контроль политики доступа происходит в безопасном хранилище секретов Azure Key Vault, где можно хранить исходные параметры приложения, при этом оставляя код без изменений. 
Основное преимущество Azure Functions — применение любого стека технологий. Использование сервиса не 
предполагает ограничения на выбор какого-то конкретного 
языка или сервера. Что дает возможность выбрать оптимальный язык программирования для каждого сценария. В 
случае использования языка, не имеющего встроенной поддержки, Azure Cloud Functions предлагают особые стратегии для создания функций [14]. 
Метод ценообразования в Azure такой же, как и в 
Lambda, и также предоставляются бесплатные пакеты на 
первые вызовы функций, далее взимается 0,000016 долларов США за гигабайтосекунду. Стоимость при высокой 
нагрузке у Azure немного ниже, если сравнивать с Lambda. 
Кроме того, Microsoft также взимает плату за использование памяти. 
Yandex Cloud Functions 
Отечественная IT-компания также не отстает от зарубежных и активно развивает свою Serverless-экосистему на 
платформе Yandex.Cloud. Она берет на себя обслуживание 
и все необходимые условия для непрерывной работы. 
Yandex.Cloud предоставляет отказоустойчивое и автомасштабируемое окружение для всей Serverless-экосистемы.  
Intellectual Technologies on Transport. 2022. No 1 
 
 
Интеллектуальные технологии на транспорте. 2022. № 1 
20
 
В нее входит сервис Yandex Cloud Functions, который позволяет запускать код приложения в виде функции. 
В сервисе присутствуют триггеры для запуска функции; 
с помощью данной возможности интеграция между Cloud 
Functions и другими сервисами происходит легко и без использования интеграционного кода. К поддерживаемым 
языкам программирования на текущий момент относятся: 
Node.js, Python, Go, PHP, Bash, Java, C# и R [15]. 
Автомасштабирование 
позволяет 
при 
увеличении 
нагрузки на количество вызовов создавать дополнительные экземпляры функции. Функции, которые запущены, 
имеют возможность выполняться параллельно. 
В сервисе Cloud Functions действуют удобные тарифы: 
каждый месяц использования в тариф не входят один миллион вызовов функций и 10 ГБ/ч выполнения функций. Далее за 1 ГБ/ч взимается плата 3,42 руб., стоимость вызова 1 
млн функций — 10 руб. 
В Serverless-экосистему Yandex.Cloud помимо Yandex 
Cloud Functions также входят следующие сервисы: 
1. Yandex API Gateway. Сервис управления API-шлюзами, который передает запросы от приложений к сервисам 
Yandex.Cloud и сокращает задержку при обработке запросов к API. 
2. Yandex Database (YDB). База данных имеет поддержку Serverless-режима и предоставляет автоматизированное масштабирование выполнения запросов, хранения 
и резервного копирования. Кроме того, YDB совместима с 
Amazon DynamoDB. 
3. Сервис очереди сообщений Message Queue. При масштабировании системы использование сервиса очередей 
является неотъемлемой частью обмена сообщениями 
между компонентами. Очередь сообщений позволяет освобождать ресурсы для новых запросов при выполнении длительных задач.  
Таким образом, использование Serverless-сервисов 
Yandex.Cloud позволяет создавать надежные проекты с 
простым управлением. В бессерверной инфраструктуре обработка запросов происходит посредством API Gateway, 
далее они перенаправляются в Cloud Functions. Хранение 
данных в Yandex Database автоматически масштабируется 
при росте интенсивности запросов. 
Сравнительная характеристика вышеупомянутых провайдеров услуг представлена на рисунке 3. 
 
Рис. 3. Сравнение провайдеров услуг 
При выборе провайдера важно помнить и о рисках 
Serverless, которые связаны с Vendor Lock, привязкой к поставщику. Функции, созданные с помощью AWS, сложно 
импортируются в другие платформы, например в Google 
Cloud. Помимо функций обычно используются базы данных, сервисы очередей, логирования и прочие Serverlessсервисы, которые у каждого провайдера могут быть свои. 
Например, Yandex Cloud Functions предусматривает совместимость с AWS [16]. 
ПРЕИМУЩЕСТВА SERVERLESS 
Основные потенциальные преимущества бессерверных 
вычислений заключаются в следующем: 
1. Абстракция от внешней инфраструктуры. Несомненным и главным преимуществом Serverless является абстракция от рабочих процессов, связанных с администрированием серверов и управлением операционной системой. 
2. Низкие эксплуатационные расходы. Поддержка 
меньшего количества компонентов означает меньшее коIntellectual Technologies on Transport. 2022. No 1 
 
 
Интеллектуальные технологии на транспорте. 2022. № 1 
21
 
личество затраченной работы. Serverless-приложения выпускаются быстрее, а значит, время для получения обратной связи от конечного пользователя также уменьшается, 
такая возможность влечет улучшения в будущем в течение 
всей разработки программного обеспечения. 
3. Максимальная эластичность. Одна из сильных сторон 
Serverless — масштабирование. Оно происходит без внесений изменений в уже написанный код, и при росте трафика 
количество запущенных функций также возрастает. 
4. Эффективная стоимость. Оплата происходит только 
за используемые вычислительные ресурсы, то есть не взимается, когда приложение простаивает. 
5. Балансировка нагрузки. В случае высокой посещаемости сервиса возрастает нагрузка на железо. В традиционной архитектуре балансировщик распределяет нагрузку на 
виртуальные 
машины. 
Несомненно, 
что 
внедрение 
Serverless окажет положительный эффект, то есть функции, 
запросы к которым приходят от балансировщика через API 
Gateway, будут брать на себя ответственность в обработке 
части запросов. Таким образом, система оптимизируется, 
распределяя нагрузку (рис. 4). В таком случае запуск функций обходится выгоднее, чем все время работающие виртуальные машины. 
Рис. 4. Архитектура с Serverless-подходом  
и балансировщиком нагрузки 
6. Встроенные интеграции. Большинство представленных на рынке провайдеров Serverless предлагают интеграцию с другими сервисами. Это преимущество благоприятно сказывается на создании программного обеспечения, 
так как основное внимание уделяется разработке, а не 
настройке и управлению. 
НЕДОСТАТКИ SERVERLESS 
Как и любая технология, Serverless обладает рядом недостатков. Некоторые из них неподвластны прогрессу и 
всегда будут актуальны, другие же связаны с текущей реализацией и в скором времени найдут решение, которое позволит технологии быть лучше. 
1. Контроль за поставщиками. Прибегая к услугам провайдера, он берет на себя контроль над частями вашей системы. Отсутствие контроля может выражаться в простое, 
неожиданном изменении стоимости услуг, потере функциональности, принудительном обновлении API и прочим 
проблемам. Кроме того, определенные способы развертывания, мониторинга, создания Serverless-функций у разных 
провайдеров разные. И в данном случае не стоит забывать 
об обратной совместимости: от любого интерфейса может 
зависеть другой сервис или функция. 
2. Время холодного запуска. Время холодного старта 
при запуске бессерверных вычислений появляется вследствие того, что при срабатывании триггера сначала происходит загрузка среды выполнения, а лишь потом запуск 
функции (рис. 5). 
Рис. 5. Схема запуска функции при холодном старте 
Управлять средой выполнения у разработчика нет возможности, в итоге жизненный цикл среды может быть непредсказуем. Помимо этого, на холодный запуск влияет 
много зависящих друг от друга факторов: язык программирования, объем написанного кода, встроенные дополнительные ресурсы, такие как базы данных. Разработчик может этим управлять, сократив время старта, но доступ к 
управлению контейнером есть только у провайдера. 
Холодный старт может стать теплым в случае, если количество обращений на функцию растет, или если разработчик запускает функции, следуя времени по таймеру. 
3. Serverless не подходит для долгосрочных процессов. 
При проектировании приложения нужно учитывать, что 
если в архитектуре присутствует функция, которая используется приложением постоянно, то и потребление ресурсов 
не будет отличаться от традиционного подхода. Поэтому 
накладывать ответственность на функцию нужно разумно, 
тщательно обдумав ее дальнейшее использование. 
ЗАКЛЮЧЕНИЕ 
На основании вышесказанного логично предположить, 
что облачные вычисления продолжают набирать свою популярность, как и бессерверные технологии, которые 
предоставляют упрощенную среду для программирования, 
что намного облегчает использование облака. Несомненно, 
это способствует привлечению людей, заинтересованных в 
его использовании. 
Неоспоримое преимущество Serverless заключается в 
фокусировании на разработке кода и избавлении от необходимости управления ресурсами и их оптимизации вручную. Такой подход схож с переходом от языка ассемблера 
на высокоуровневые языки. При этом следует отметить и 
повышенную сложность разработки и тестирования приложений по причине того, что бессерверная архитектура требует структурного разделения одного приложения на сервисы и функции, а это в свою очередь значительно увеличивает время проектирования архитектуры [17]. 
Очевидной ценностью Serverless является сокращение 
объема инвестиций, необходимых для поддержки работы 
приложений, что позволяет получить больше пространства 
для экспериментов и инноваций. 
Несмотря на уже достигнутый успех в Serverless вычислениях, они продолжают свою эволюцию, поставщики 
услуг упорно работают над развитием новых услуг и улучшением уже существующих, устраняя выявленные недостатки. Разработчики уже приступили к изучению вопроса 
Intellectual Technologies on Transport. 2022. No 1 
 
 
Интеллектуальные технологии на транспорте. 2022. № 1 
22
 
о дальнейшем процессе развития бессерверных платформ 
и повышения эффективности рабочей нагрузки [18]. Решаются проблемы в областях абстракции, систем, сетей, безопасности и архитектуры. Нетрудно спрогнозировать, что 
дальнейший рост и изменение бессерверности будет и идти 
дальше. Так через десятки лет Serverless станет стандартом 
среди облачных технологий, заменив серверные вычисления. 
ЛИТЕРАТУРА 
1. Occupy the Cloud: Distributed Computing for the 99% /  
E. Jonas, Q. Pu, S. Venkataraman, [et al.] // Proceedings of the 
Eighth ACM Symposium on Cloud Computing (SoCC`17) (Santa 
Clara, CA, USA, 24–27 September 2017). — New York (NY): 
Association for Computing Machinery, 2017. — Pp. 445–451. 
DOI: 10.1145/3127479.3128601. 
2. Переход в облако стал повседневной задачей для бизнеса // C-News. — 2018. — 31 октября. 
URL: http://www.cnews.ru/articles/2018-10-31_perehod_v_oblako_stal_povsednevnoj_zadachej_dlya_srednego_biznesa (дата 
обращения 07.03.2022). 
3. Тренды развития облачных вычислений // TAdviser — 
Государство. Бизнес. Технологии. — 2022. — 24 января. 
URL: http://www.tadviser.ru/a/147459 (дата обращения 
07.03.2022). 
4. Что такое IaaS-облако, сравнение с PaaS и SaaS на 
примерах // 1cloud. URL: http://1cloud.ru/services/privatecloud/iaas-paas-saas (дата обращения 08.03.2022). 
5. Desde IaaS, CaaS, PaaS y FaaS, ¿Сómo Elegir la Plataforma Adecuada? / Traducido por Fu Hui // Programador Clic. 
URL: http://programmerclick.com/article/80131207853 (дата 
обращения 09.03.2022). 
6. Стельмах, С. Облачные вычисления: пик популярности пройден? // itWeek. — 2020. — 10 февраля.  
URL: http://www.itweek.ru/its/article/detail.php?ID=211336 (дата обращения 09.03.2022). 
7. Avram, A. FaaS, PaaS, and the Benefits of the Serverless 
Architecture // InfoQ. — 2016. — 25 June.  
URL: http://www.infoq.com/news/2016/06/faas-serverless-architecture (дата обращения 09.03.2022). 
8. Delimitrou, C. Quasar: Resource-Efficient and QoSAware Cluster Management / C. Delimitrou, C. Kozyrakis // Proceedings of the 19th International Conference on Architectural 
Support for Programming Languages and Operating Systems 
(ASPLOS XIX) (Salt Lake City, UT, USA, 01–05 March 2014). 
ACM SIGPLAN Notices. 2014. Vol. 49, Is. 4. Pp. 127–144. 
DOI: 10.1145/2644865.2541941. 
9. Peeking Behind the Curtains of Serverless Platforms / 
 L. Wang, M. Li, Y. Zhang, [et al.] // Proceedings of the 2018 
USENIX Annual Technical Conference (USENIX ATC`18) 
(Boston, MA, USA, 11–13 July, 2018). — Washington (D.C.): 
USENIX Association, 2018. — Pp. 133–145. 
10. Chandra, B. Mesos — Creating Your Own Serverless // 
Medium. — 2019. — 17 July. URL: http://bhanuandchandra.medium.com/mesos-creating-your-own-serverless2aaf65cfb0b7 (дата обращения: 10.03.2022). 
11. Brewer, E. A. Kubernetes and the Path to Cloud Native 
// Proceedings of the Sixth ACM Symposium on Cloud Computing (SoCC`15) (Kohala Coast, HI, USA, 27–29 August 
2015). — New York (NY): Association for Computing Machinery, 2015. — P. 167. DOI: 10.1145/2806777.2809955. 
12. Serverless vs Containers: Choose One or Use Both? // 
Cloud Native Wiki. URL: http://www.aquasec.com/cloudnative-academy/serverless-architecture/serverless-vscontainers-choose-one-or-use-both 
(дата 
обращения 
10.03.2022). 
13. Cloud Wars Top 10 — The World’s Top Cloud Vendors // 
Acceleration 
Economy. 
URL: 
http://accelerationeconomy.com/cloud-wars-top-10 
(дата 
обращения 
15.03.2022). 
14. Supported Languages in Azure Functions // Microsoft 
Docs. — 2021. — 21 October. 
URL: http://docs.microsoft.com/en-gb/azure/azure-functions/ 
supported-languages (дата обращения 15.03.2022). 
15. Среда выполнения. Обзор // Yandex.Cloud. 
URL: http://cloud.yandex.ru/docs/functions/concepts/runtime (дата обращения 12.03.2022). 
16. Serverless 
— 
бессерверные 
вычисления 
// 
Yandex.Cloud. URL: http://cloud.yandex.ru/solutions/serverless (дата обращения 12.03.2022). 
17. Hennesy, J. L. A New Golden Age for Computer Architecture / J. L. Hennessy, D. A. Patterson // Communications of 
the ACM. 2019. Vol. 62, Is. 2. Pp. 48–60.  
DOI: 10.1145/3282307. 
18. Serverless Computing: One Step Forward, Two Steps 
Back / J. M. Hellerstein, J. Faleiro, J. E. Gonzalez, [et al.] // 
Proceedings of the 9th Biennial Conference on Innovative Data 
Systems Research (CIDR 2019) (Asilomar, CA, USA,  
13–16 January 2019). 9 p. URL: http://cidrdb.org/cidr2019/ 
papers/p119-hellerstein-cidr19.pdf 
(дата 
обращения 
12.03.2022). 
 
 
Intellectual Technologies on Transport. 2022. No 1 
DOI: 10.24412/2413-2527-2022-129-16-23 
 
Интеллектуальные технологии на транспорте. 2022. № 1 
23
 
Characteristics of Serverless 
Computing Technologies 
A. T. Gordina, PhD A. V. Zabrodin 
Emperor Alexander I St. Petersburg State Transport University 
Saint Petersburg, Russia 
anngordtm@gmail.com, teach-case@yandex.ru 
 
Abstract. This article is devoted to one of the prospective types 
of cloud computing known as Serverless computing. The paper 
considers the technologies used for Serverless, gives comparison of 
cloud models and service providers, also it describes main components of FaaS architectural pattern (functions, resources and 
events). The authors consider the advantages and disadvantages of 
Serverless computing. Besides that, the article discloses problems 
of application migration from local servers to the cloud and how 
resource management and optimization are excluded from the 
software development process. 
Keywords: cloud computing, Serverless, FaaS model, functions, 
autoscaling, application architecture. 

13 
УДК 82-5+80(042.5) 
СОВРЕМЕННАЯ ОБЛАЧНАЯ ИНФРАСТРУКТУРА:  
БЕССЕРВЕРНЫЕ ВЫЧИСЛЕНИЯ 
Макосий Алексей Иванович, 
кандидат физико-математических наук, доцент кафедры программного обеспечения  
вычислительной техники и автоматизированных систем  
Инженерно-технологический институт Хакасского государственного университета им. Н. Ф. Катанова (г. Абакан). 
Макосий Роман,  
DevOps-инженер  
Фирма Base2Services (г. Мельбурн, Австралия) 
Статья посвящена одному из новых и весьма популярных направлений в области облачных вычислений, известных под названием «бессерверные вычисления». Представлены такие области использования бессерверных 
вычислений, как автоматическое масштабирование веб-сайтов и API, обработка потока событий, манипуляция изображениями и видео. Авторами описываются основные компоненты FaaS: функции, события и ресурсы. Отмечены наиболее важные преимущества и недостатки технологии бессерверных вычислений. 
Ключевые слова: облачные вычисления, беcсерверные вычисления, области использования, преимущества, 
недостатки. 
MODERN CLOUD INFRASTRUCTURE: SERVERLESS COMPUTING 
Makosiy Aleksey Ivanovich, 
Sc. D., Associate Professor, the Computer Facilities Software and Automated Systems Department,  
Engineering and Technology Institute, Katanov Khakass State University (Abakan). 
Makosiy Roman, 
DevOps-engineer,  
Base2Services Pty, Ltd (Melbourne, Australia) 
The article is devoted to one of the new and very popular directions in the field of cloud computing, known as 
serverless computing. Such fields of serverless computing are presented as automatic scaling of websites and APIs, 
processing of the events flow, manipulation of images and videos. The authors describe such basic components of FaaS 
as functions, events, and resources and mark the most important advantages and disadvantages of the serverless 
computing technology. 
Key words: cloud computing, serverless computing, fields of use, advantages, disadvantages. 
 
В мире IT существуют различные уровни 
абстрагирования инфраструктуры, такие как 
прямой доступ к аппаратным ресурсам, работа 
с виртуальными машинами или с контейнерами. Для решения некоторых задач удобнее 
всего полностью абстрагировать функции администрирования с той целью, чтобы заказчик 
мог выполнять свой код, не беспокоясь об инфраструктуре и управлении серверами. Такова 
суть «бессерверных вычислений» [1].  
Бессерверные вычисления позволяют сосредоточиться на бизнес-логике, не беспокоясь 
о проблемах, связанных с инфраструктурой 
работы приложений, что существенно упрощает управление серверами в облаке. Бессерверные вычисления можно считать последним 
шагом в отделении сервисов от виртуальной 
или физической инфраструктуры. 
Можно выделить две определяющие характеристики бессерверных технологий: невидимая инфраструктура вместо настраиваемых 
образов виртуальных машин и схема оплаты, 
основанная на фактически потребляемых ресурсах вместо фиксированной почасовой ставки. Вторая характеристика не нова, так как 
большинство облачных сервисов уже работают 
по такому принципу.  
Пионером в области бессерверных вычислений является Amazon Web Services. В 2014 
году AWS выпустила новаторскую услугу под 
названием AWS Lambda, в которой предложила новый способ развѐртывания в облаке веб14 
приложений, мобильных приложений или 
приложений IoT. Вместо того, чтобы сразу 
развѐртывать всю кодовую базу, Lambda позволяет развернуть функции приложения по 
отдельности в своих собственных контейнерах. Microsoft ответила, выпустив «Azure 
Function», а Google в феврале 2016 года создал 
«Cloud Function» как свою собственную службу вычислений без сервера. 
Основные области использования бессерверных вычислений. Среди наиболее популярных сфер применения данной технологии 
можно назвать: 
– автоматическое масштабирование вебсайтов и API. Бессерверные веб-сайты и приложения могут быть написаны и развѐрнуты 
без предварительной настройки инфраструктуры. Наилучшая часть в этом заключается в 
том, что backends автоматически масштабируются в зависимости от спроса. Всплески трафика обслуживаются без проблем со скоростью и доступностью; 
– обработка потока событий. Бессерверные вычисления могут запускаться как следствие какого-либо события, предоставляя эластичные, масштабируемые потоки событий без 
обслуживания сложных кластеров. Эти потоки 
событий могут обслуживать аналитические 
системы, обновлять вторичные хранилища 
данных и кэши, системы мониторинга; 
– манипуляция изображениями и видео. 
Бессерверные вычисления позволяют создавать улучшенные изображения и видеоуслуги 
для приложений, а также динамическое изменение размера изображений или изменение 
транскодирования видео для разных устройств. Приложения также всѐ чаще полагаются на такие вещи, как распознавание изображений, чтобы улучшить пользовательский интерфейс; 
– IoT. Большинство IoT-систем обычно полагаются на общие шаблоны в том, как они 
обрабатывают и хранят данные. Во-первых, 
этим системам необходимо принимать данные 
от различных датчиков, расположенных в разных местах. Затем эти системы обрабатывают 
и анализируют потоковые данные для получения информации в режиме реального времени. 
Бессерверные вычисления предлагают необходимую гибкость и отказоустойчивость при минимальных затратах на инфраструктуру. 
Архитектурные принципы бессерверных 
вычислений. Хребтом бессерверных вычислений является функция как услуга (FaaS) – один 
из видов облачных вычислений, обеспечивающий платформу, которая позволяет клиентам разрабатывать, запускать и управлять 
функциональными возможностями приложений, практически избегая сложностей построения и поддержки инфраструктуры. Разработчик пишет функции, фрагменты приложения и задаѐт правила, основанные на 
событиях, которые вызывают код в нужные 
моменты [2]. 
Основными компонентами FaaS являются 
функции, события и ресурсы. Функции – это 
независимые единицы развѐртывания, которые 
могут выполнять разные действия, например, 
сохранение пользователя в базе данных, обработку файлов, выполнение запланированных 
задач и т. п. Всѐ, что запускает выполнение 
функции, рассматривается как событие, в частности, загрузка файла на файловый сервер, 
публикация сообщений и т. п. Ресурс относится к инфраструктуре или компонентам, используемым функциями. Например, службы 
базы данных (для сохранения данных пользователей/сообщений/комментариев), 
службы 
файловой системы (для сохранения изображений или файлов), службы очереди сообщений 
(для публикации сообщений). 
Очень важным моментом во всѐм этом 
является то, что программное обеспечение, 
работающее в такой среде,  должно этой среде соответствовать. Одного только FaaS недостаточно для создания полностью функционального приложения. Приложения часто 
требуют работы с базами данных, службами 
15 
аутентификации, 
очередями 
и 
другими. 
Большинство этих сервисов доступны в данной среде облачных провайдеров и могут 
быть использованы FaaS напрямую через 
SDK.  
Эти услуги практически не требуют обслуживания. Например, DynamoDB – база данных 
класса NoSQL от Amazon в качестве службы 
хранения данных позволяет разработчикам 
приобретать услугу, основанную на пропускной способности, а не на объѐме хранения. Если включено автоматическое масштабирование, то база данных будет автоматически масштабироваться. Кроме того, администраторы 
могут запрашивать изменения пропускной 
способности, а DynamoDB будет распространять данные и трафик по нескольким серверам, 
что позволяет прогнозировать производительность. Другие услуги также легко доступны 
для FaaS. 
Функции аутентификации, работы с БД и 
ряд других могут перекладываться с серверов 
приложений и баз данных через специальное 
API стороннему провайдеру – так называемой 
Backend-as-a-Service (BaaS) платформе (во 
всех этих XaaS аббревиатурах скоро уже можно будет запутаться).  
BaaS-платформами пользуется множество 
разработчиков игр, социальных и корпоративных приложений. Но рынок здесь ещѐ очень 
незрелый. Ранее флагманом движения BaaS 
считалась компания Parse. Она была основана 
в 2011 году и предоставляла решение для удалѐнной работы с данными (обработка и хранение), которое упрощало написание серверной 
бизнес-логики на JS. В 2016 году Facebook купил стартап, но прекратил обслуживание клиентов через год. Это вызвало лѐгкую панику 
среди разработчиков и позволило войти в эту 
среду новым игрокам, например, российской 
компании Prof-ItVentures с еѐ продуктом 
Scorocode. Указывая на молодость рынка, 
можно также отметить, что сегодня большинство применений FaaS пока что представляют 
собой обычные приложения, завѐрнутые в новую среду.  
Для облегчения перехода на новую технологию на рынке появились беcсерверные 
фреймворки, которые предоставляют CLI для 
упрощения процесса создания и развѐртывания беcсерверного приложения и шаблоны, 
построенные над FaaS, с тем чтобы дать простой 
способ 
создания 
и 
настройки 
беcсерверных приложений на многих языках, 
включая Python, Node.js, Java и некоторые 
другие. 
Использовать работающие приложения с 
бессерверными фреймворками нецелесообразно. Чаще всего эти фреймворки используются 
для создания новых приложений либо для добавления функций или дополнительных услуг 
в работающие приложения. Наиболее известными фреймворками являются Serverless.com, 
APEX и AWS SAM.  
Serverless.com – проект с открытым исходным кодом, поддерживающий пока только 
AWS Lambda с планами поддержки других 
крупных поставщиков облачных вычислений в 
будущем. Serverless.com использует YAML 
для описания функций, событий и связанных 
ресурсов. Во время развѐртывания платформа 
Serverless преобразует весь синтаксис, описанный на YAML, в один шаблон AWS Cloud 
Formation.  
APEX использует JSON для задания правил 
поведения и различных аспектов работы приложения без сервера. В APEX можно отметить 
поддержку языков, которые не поддерживаются Lambda, например, Golang. 
AWS SAM является стандартной моделью 
приложений для приложений без сервера, размещѐнных в AWS Lambda. AWS SAM использует шаблон Cloud Formation для развѐртывания приложения в виде стека Cloud Formation 
и определяет набор объектов, которые могут 
быть включены в шаблон Cloud Formation для 
простого описания общих компонентов бессерверных приложений. 
16 
Шаблоны описывают бессерверное приложение в соответствии с текстовыми файлами 
AWS SAM JSON или YAML и вводят несколько новых ресурсов и типов свойств, которые 
могут быть встроены в раздел ресурсов в шаблоне. Шаблоны могут включать все другие 
разделы шаблонов и использовать встроенные 
функции Cloud Formation для доступа к свойствам, доступным только во время выполнения. 
Преимущества и недостатки бессерверных вычислений. Отметим некоторые важные 
преимущества и недостатки новой технологии. 
Главное преимущество бессерверной архитектуры состоит в том, что она позволяет сосредоточиться только на функционале приложения, а не на обслуживании инфраструктуры и 
еѐ параметрах, таких как масштабируемость, 
высокая доступность и т. п. 
Но размышления о вопросах поддержки 
инфраструктуры в актуальном состоянии и 
взаимодействии этого процесса с установленными приложениями идут вразрез с природой 
бессерверных вычислений. Обслуживание зависимостей среды – это не то, чем должен заниматься бессерверный разработчик. 
Очевидным преимуществом является сокращение объѐма инвестиций, необходимых 
для поддержки работы приложений. Обратной 
стороной можно назвать повышенную сложность их разработки и тестирования. Бессерверная архитектура требует структурного разделения одного приложения на сервисы и 
функции, что значительно увеличивает время 
разработки. Точно так же сложно выполнение 
интеграционных тестов и тестирование нагрузки для платформы Serverless, поскольку 
существует зависимость от внешних систем 
при запуске сценариев тестирования. 
В качестве положительной характеристики 
можно также отметить снижение сложности 
установки и развѐртывания приложений, разработанных для платформы Serverless. Процедуры компиляции и загрузки намного проще 
по сравнению с традиционной упаковкой и 
развѐртыванием. 
В настоящее время многие разработчики и 
администраторы впечатлены идеями и подходами бессерверных вычислений. Но маловероятно, что бессерверный подход станет панацеей и вряд ли заменит все существующие подходы и программные архитектуры. Однако 
можно с уверенностью сказать, что эта область 
будет развиваться и двигать вперѐд сферу разработки программного обеспечения и облачных вычислений. 
Библиографический список 
1. Gyanendra Rai, Prashant Pasricha, Santosh Pandey, etc. Serverless Architecture: Evolution of a New Paradigm. – URL: 
https://www.globallogic.com/gl_news/serverless-architecture-evolution-of-a-new-paradigm (дата обращения: 17.09.18). 
2. Mike Roberts. Serverless Architectures. – URL: https://martinfowler.com/articles/serverless.html (дата обращения: 17.09.18). 
© Макосий А. И., Макосий Р., 2019 
 
 
 
 
 
 
 
 
 
 

 
Труды учебных заведений связи                                                                                             2024. Т. 10. № 3 
 
 
 Волков А.Н., 2024 
24                                              tuzs.sut.ru 
 
Научная статья 
 
УДК 621.391 
https://doi.org/10.31854/1813-324X-2024-10-3-24-34 
 
Динамические туманные вычисления  
и бессерверная архитектура:  
на пути к зеленым ИКТ 
 
 Артем Николаевич Волков, artem.nv@sut.ru 
 
Санкт-Петербургский государственный университет телекоммуникаций им. проф. М. А. Бонч-Бруевича, 
Санкт-Петербург, 193232, Российская Федерация 
 
Аннотация 
Актуальность. В условиях роста парка оборудования центров обработки данных, развития сетей IMT2020 и появлением услуг Телеприсутствия сетей IMT-2030 особо актуальным направлением современных 
исследований является поиск нетривиальных, нестандартных подходов и решений в области обеспечения 
вычислительными и сетевыми ресурсами. Данная статья освещает актуальные вопросы инфраструктурного направления сетей IMT-2030 ‒ динамических туманных вычислений. Рассматривается вклад 
данной технологии для повышения эффективности используемых ресурсов, приводятся актуальные сценарии сетей IMT-2030. В частности, исследуется задача поиска группы устройств в туманных вычислениях для последующей миграции типовых контейнеров платформы FaaS.  
Постановка задачи: исследование вопросов совместного использования бессерверной архитектуры и 
динамических туманных вычислений для эффективного распределения нагрузки услуг Телеприсутствия. 
Цель работы: исследование и разработка эффективного метода распределения группы микросервисов в 
динамических туманных вычислениях.  
Используемые методы: исследуемые алгоритмы относятся к типу метаэвристических алгоритмов 
для решения задач многокритериальной оптимизации. Для апробации метода был разработан сегмент 
лабораторной сети, который послужил генератором реальных данных работы тестируемых платформ 
в условиях роста нагрузки. На базе серии экспериментов были собраны данные для последующего моделирования предложенного метода, который, в свою очередь, был реализован на языке программирования 
Python.  
Анализ результатов показал эффективность предложенного метода в рамках поставленной задачи, 
что, в конечном итоге, позволяет значительно быстрее принимать решение о миграции.  
Новизна: разработаны модель и метод для бессерверной архитектуры с миграцией групп микросервисов 
на группы устройств туманных вычислений в условиях их подвижности, и использован метаэвристический алгоритм стаи серых волков с целью определения группы устройств для последующей миграции типовых микросервисов.  
Практическая значимость: разработанная модель и метод могут быть использованы при реализации 
туманных вычислений в условиях подвижности устройств, в том числе с целью достижения требований 
перспективных услуг Телеприсутствия. 
 
Ключевые слова: IMT-2030, туманные вычисления, услуги Телеприсутствия, бессерверная архитектура, 
метаэвристические алгоритмы 
 
Источник финансирования: статья подготовлена в рамках мегагранта Минобрнауки по соглашению 
№ 075-15-2022-1137. 
 
Ссылка для цитирования: Волков А.Н. Динамические туманные вычисления и бессерверная архитектура: 
на пути к зеленым ИКТ. 2024. Т. 10. № 3. С. 24‒34. DOI:10.31854/1813-324X-2024-10-3-24-34. EDN:QOELMJ 
 
 
 
 
Proceedings of Telecommunication Universities                                                           2024. Vol. 10. Iss. 3 
 
 
Electronics, Photonics, Instrumentation and Communications 
25                                              tuzs.sut.ru 
 
Original research 
https://doi.org/10.31854/1813-324X-2024-10-3-24-34 
 
Dynamic Fog Computing Towards Green ICT 
 
 Artem N. Volkov, artem.nv@sut.ru 
 
The Bonch-Bruevich Saint Petersburg State University of Telecommunications, 
St. Petersburg, 193232, Russian Federation 
 
Annotation  
Relevance: In the context of the growing fleet of data center equipment, the development of IMT-2020 networks 
and the imminent emergence of Telepresence services of IMT-2030 networks, a particularly relevant area of modern 
research is the search for non-trivial, non-standard approaches and solutions in the field of provision of computing 
and network resources. This article covers current issues in the infrastructure direction of IMT-2030 networks - dynamic fog computing. The contribution of this technology to improve the efficiency of used resources is considered, 
and current scenarios for IMT-2030 networks are presented. In particular, we study the problem of searching for a 
group of devices in the computing fog for subsequent migration of typical FaaS platform containers.  
Problem statement: Research on the joint use of serverless architecture and dynamic fog computing for efficient 
load distribution of telepresence services.  
Goal of the work: Research and development of an effective method for distributing a group of microservices in 
dynamic fog computing.  
Methods: the algorithms under study belong to the type of metaheuristic algorithms for solving multicriteria optimization problems. To test the method, a laboratory network segment was developed, which served as a generator 
of real data on the operation of the tested platforms under conditions of increasing load. Based on a series of experiments, data was collected that formed the basis for subsequent modeling of the proposed method, which in turn was 
implemented in the Python programming language.  
Result: Analysis of the results showed the effectiveness of the proposed method within the framework of the task, 
which ultimately makes it possible to make a decision on migration many times faster.  
Novelty: A model and method for serverless architecture have been developed for migrating groups of microservices 
to groups of fog computing devices, under conditions of their mobility, and a meta-heuristic algorithm of a pack of 
gray wolves has been used to determine a group of devices for subsequent migration of typical microservices.  
Practical significance: The developed model and method can be used in the implementation of fog Computing, in 
conditions of device mobility, including in order to achieve the requirements of promising Telepresence services. 
 
Keywords: IMT-2030, fog computing, telepresence services, serverless architecture, metaheuristic algorithms  
 
Funding: The article was prepared within the framework of a megagrant from the Ministry of Education and 
Science under agreement No. 075-15-2022-1137. 
 
For citation: Volkov A.N. Dynamic Fog Computing Towards Green ICT. Proceedings of Telecommunication 
Universities. 2024;10(3):24‒34. (in Russ.) DOI:10.31854/1813-324X-2024-10-3-24-34. EDN:QOELMJ 
 
Введение 
Общее стремление к цифровизации процессов в 
государстве, бизнесе и обществе одновременно 
приносит как положительные аспекты ‒ повышение эффективности и прозрачности систем, так и 
новые вызовы и трудноразрешимые задачи. Принятие и внедрение концепции Интернета Вещей 
(ИВ), масштабы которой 10 лет назад были недооценены, и последующие базирующиеся на ней 
технологические направления привели к фундаментальным изменениям не только в области 
конкретных технологий и решений, но и во взглядах на услуги и саму сеть. Так, развитие платформ 
ИВ, позволяющих обрабатывать гигабайты данных и получать новые знания с использованием 
инструментов искусственного интеллекта (ИИ), 
привело к бурному развитию центров обработки 
данных (ЦОДы). Последующее развитие ИВ породило различные концепции услуг, например таких, 
как: Тактильный Интернет, Интернет Навыков и 
другие. Приведенные примеры относятся к сверхнадежным сетям с ультрамалыми задержками 
 
Труды учебных заведений связи                                                                                             2024. Т. 10. № 3 
 
 
Электроника, фотоника, приборостроение и связь 
26                                              tuzs.sut.ru 
 
(URLLC, аббр. от англ. Ultra-Reliable Low Latency 
Communications), которые являются одним из 
фундаментальных направлений сетей IMT-2020. 
Стоит отметить, что принятая в конце 2023 г. концепция сетей IMT-2030 естественным образом 
стала преемницей IMT-2020, в тоже время увеличив требования и представив такие сценарии, как 
TIRO (аббр. от англ. Tactile Internet for Remote Operations) и HTC (аббр. от англ. Holographic Type 
Communications). Сети IMT-2030 определяют следующие «технологические измерения»: 
‒ Massive Communication (пер. с англ. ‒ массовые 
коммуникации), что, в свою очередь, является 
расширением massive Machine Type Communication 
(mMTC); 
‒ Immersive Communication (пер. с англ. ‒ иммерсивные коммуникации), является расширенным 
enhanced Mobile Broadband (eMBB); при этом важно отметить, что новый сценарий включает в себя 
и поднаправления mMTC и URLLC; 
‒ Hyper Reliable & Low-Latency Communication 
(HRLLC, пер. с англ. ‒ гипернадежные сети связи с 
ультрамалой задержкой), представляют развитие 
направления URLLC-сетей IMT-2020; 
‒ Artificial Intelligence in Communication (AI in 
Communication, пер. с англ. ‒ искусственный интеллект в связи) или автономные сети связи; на 
сегодня можно обнаружить достаточно большой 
пласт разработанных Рекомендаций МСЭ-Т (Международный союз электросвязи, сектор стандартизации телекоммуникаций) в 13 ИК (Исследовательской комиссии), где определены фундаментальные решения для последующей имплементации технологий ИИ в сети связи;  
‒ Ubiquitous Connectivity (пер. с англ. ‒ повсеместная связь): данное направление, в первую 
очередь, раскрывает концепцию МСЭ сетей 2030 
STIN (от англ. Space-Terrestrial Integrated Network ‒ 
космически-наземная интегрированная сеть) и 
является предложением по решению ряда целей 
устойчивого развития ООН, в том числе для сокращения цифрового разрыва с удаленными районами и поселения;  
‒ Integrated Sensing and Communication (пер. с 
англ. ‒ интегрированное зондирование и связь): 
формирует целый пласт задач в области позиционирования в сети IMT-2030 и тесно связано с другими сценариями ее применения.  
В результате бурного развития ИВ и других 
сценариев сетей IMT-2020 несколько лет назад 
был сформирован тренд на декомпозицию систем 
вычислений. Граничные вычисления позволили, 
благодаря своей архитектуре, приблизить реализацию ряда услуг, а также снизить нагрузку на ядро сети. В то же время автономные сети призваны 
разрешить вопросы интеллектуального распределения ресурсов как сетевых, так и вычислительных, которые мягко интегрированы в системы 
управления сетью. В итоге глобальный тренд на 
абстрагирование программного обеспечения сетевых и вычислительных сущностей от аппаратной 
части сформировал технические возможности реализации 
программного 
слайсинга 
ресурсов. 
Пройдя вышеприведенные этапы развития сети и 
вычислительных систем, стала более ясна цель ‒ 
максимальная автономизация инфраструктуры с 
рациональным использованием вычислительных 
и сетевых ресурсов на базе алгоритмов машинного 
интеллекта.  
Если обратиться к отчетам статистического 
агентства Straits Research [1], можно заметить 
продолжающийся рост объема рынка оборудования для ЦОДов. Ожидается, что к 2031 г. он достигнет 164,36 млрд долларов США, а среднегодовой темп роста (рисунок 1) составит 13,2 % (CAGR, 
аббр. от англ. Compound Annual Growth Rate ‒ совокупный среднегодовой темп роста) в течение 
прогнозируемого периода (2023–2031 гг.). 
 
Рис. 1. Рост рынка оборудования ЦОД [1] 
Fig. 1. Data Center Equipment Market Growing [1] 
С учетом планируемых услуг Телеприсутствия 
сетей IMT-2030 цифры роста могут быть изменены в бо́льшую сторону, и факт объема строящихся 
ЦОДов заставляет задуматься об эффективности 
используемого сетевого и вычислительного оборудования, а также увеличении потребления энергоресурсов для обеспечения работы данного парка 
оборудования.  
В работе [2] приводится проблема создания зеленых 
инфокоммуникационных 
технологий 
(Green ICT). Исследуются как организационные, 
так и информационно-технологические способы 
перехода к Green ICT, то есть на рациональное 
энергопотребление. К примеру, приводится сравнительный анализ потребности энергии для судна-контейнеровоза 
«Emma-Maersk», 
одного 
из 
крупных ЦОДов европейской части России, где 
сравнительно 
наглядно 
становится 
понятно, 
насколько ЦОДы потребляют электроресурсов. 
Существует немало работ в области Green ICT [3‒
6], а также ‒ немало решений в данной области, 
 
Proceedings of Telecommunication Universities                                                           2024. Vol. 10. Iss. 3 
 
 
Electronics, Photonics, Instrumentation and Communications 
27                                              tuzs.sut.ru 
 
которые внедряются различными компаниями по 
всему миру. Стоит также отметить то, что данная 
повестка активно исследуется и прорабатывается 
на уровне международных рекомендаций МСЭ-Т 
Исследовательской комиссией № 5. В частности, 
можно найти серию Рекомендаций МСЭ-Т L.1300L.1399: Энергоэффективность, умная энергетика и 
экологически чистые центры обработки данных, а 
также немало технических отчетов и спецификаций. МСЭ-Т дает следующее определение: «Зеленый» или устойчивый ЦОД ‒ это хранилище для 
управления и распространения данных, в котором 
механические, осветительные, электрические и 
компьютерные системы спроектированы с учетом 
максимальной 
энергоэффективности 
и 
минимального воздействия на окружающую среду. В 
работе [7] дано следующее определение Green ICT 
как совокупности способов, призванных уменьшать вредное воздействие на человека и окружающую среду, эффективно использовать ресурсы, 
которые предоставляет природа и одновременно 
повышать производительность систем в расчете 
на единицу потребляемых физических ресурсов. 
Существует также принцип Р. Ландауэра, который 
позволяет установить связь между объемом данных и энергозатратами, при этом независимо от 
физики и технологии вычислительного процесса в 
случае потери одного бита данных в ходе вычисления как минимум выделяется энергия E (Дж). 
Как бы это «парадоксально» не звучало, принцип 
Ландауэра означает, что компьютер потребляет 
тем меньше энергии, чем меньше вычислительных 
операций он выполняет. Если же рассматривать 
принципы обработки данных на базе ЦОД, то 
необходимо учесть добавочные затраты энергоресурсов, которые необходимы для постоянной работы систем охлаждения и других вспомогательных систем, а также систем питания сетей связи, 
которые обеспечивают доступ к ЦОД.  
Таким образом, с точки зрения принципа 
Ландауэра, а также учитывая вышеприведенные 
исследования в области трендов рынка и развития 
технологий услуг Телеприсутствия и сетей IMT2030, в настоящее время существует потребность 
в поиске более эффективных методов построения 
инфраструктуры. В частности, ожидается, что декомпозиция ЦОДов в меньшие центры, а также 
внедрение алгоритмов ИИ и более инновационных 
материалов, а также использование возобновляемых источников энергии позволит снизить общий 
эффект на окружающую среду. При этом декомпозиция вычислительной архитектуры и систем 
«эпохи» до туманных вычислений (Fog), в том числе динамических, позволит исключить некоторые 
составляющие в общей формуле потребления 
энергии на удельный размер вычислительной задачи. Здесь использование устройств пользователя и устройств ИВ не требует дополнительных 
систем охлаждений, резервирования и построения 
технологичных зданий с надстроенными инженерными системами для его функционирования. В 
таком случае концепция Fog не только позволяет 
обеспечить требования к качеству обслуживания 
для ряда услуг при условии микросервисных архитектур, но и снизить общую суммарную потребность в энергоресурсах. Де-факто Fog являются 
инструментом бережливого производства в области инфокоммуникационных технологий и систем 
связи. 
 
Динамические туманные вычисления  
и бессерверная архитектура услуг 
Бессерверная архитектура (serverless) и контейнеры могут быть интегрированы для создания 
высокомасштабируемых и эффективных платформ высоконагруженных услуг. Контейнеры могут использоваться для упаковки и развертывания 
функций serverless, что позволяет разработчикам 
услуг использовать преимущества обеих технологий. К преимуществам совместного использования 
технологий можно отнести следующие:  
‒ упрощенное 
развертывание 
(контейнеры 
упрощают развертывание функций serverless, поскольку они уже содержат все зависимости программного обеспечения в виде библиотек и 
фреймворков); 
‒ портативность (контейнер позволяет быстро 
переносить функции serverless между различными 
облачными платформами и локальными средами); 
‒ безопасность (контейнеры обеспечивают изоляцию и безопасность между serverless-функциями, 
что в итоге снижает риск уязвимостей).  
Для анализа работы сети Fog необходимо использовать модель, которая позволит описать 
функционирование сети в условиях изменяющейся архитектуры ввиду мобильности Fog-устройств 
как рамках кластера, так и в рамках туманностей 
(Nebula) [8]. При этом данные кластера и/или туманности обладают характеристикой гетерогенности как самих Fog-устройств, так и собственно 
структур. Таким образом, с учетом вышеприведенных допущений, в качестве математической 
модели может быть использована модель точечного процесса, учитывая гетерогенность структур 
и Fog-устройств. При этом, в данных условиях 
процессы Неймана ‒ Скотта, относящиеся к точечным процессам с кластеризацией (то есть с объединениями устройств), будут предпочтительнее 
для построения модели. Так как процессы без кластеризации применяются для моделирования однородных систем, где объекты образуют одно 
единственное поле, размещенное на плоскости 
или в пространстве (такая модель может быть 
применима в частных случаях при использовании 
 
Труды учебных заведений связи                                                                                             2024. Т. 10. № 3 
 
 
Электроника, фотоника, приборостроение и связь 
28                                              tuzs.sut.ru 
 
однородных вычислительных ферм). Более подробные исследования в области применения точечных процессов для распределенных Fog представлены в следующей статье [9]. Стоит отметить, 
что Fog обладают отчасти противоречивыми характеристиками. Например, устройства динамических Fog могут образовывать mesh-сеть, при этом 
обладающую характеристикой самоорганизации с 
возможностью горизонтального масштабирования, что больше напоминает самоорганизующиеся 
сенсорные сети. В то же время, Fog являются вычислительным кластером, на базе которого могут 
быть развернуты платформы, услуги и др. Соответственно, для разрешения задач может быть 
перенят опыт исследований и разработок в вышеуказанных областях знаний. Так, в работах [10, 11] 
приводятся исследования применения аппарата 
точечных процессов, в частности процессов Томаса 
для формирования математических моделей в 
сверхплотных сенсорных сетях ИВ, что говорит об 
актуальности использования данной математической базы для перспективных сетей и услуг. 
В данной работе необходимо решить задачу поиска группы устройств в вычислительной туманности, которая может быть представлена в виде 
группы или одного кластера Fog. При этом делается допущение о том, что все устройства, находясь в 
условиях 
мобильности 
относительно 
базовой 
станции или сетевого координатора, стремятся 
сохранять единый вектор перемещения при минимально возможных отклонениях между собой. 
На практике такой сценарий может быть обнаружен в быстрых поездах типа Сапсан, самолетах и 
других похожих условиях (общественный транспорт, проспекты и т. п).  
Таким образом, опираясь на сценарий, модель 
сети Fog при поиске группы Fog-устройств для 
дальнейшей живой миграции 4-х типовых контейнеров платформы FaaS может быть представлена в 
виде рисунка 2. Каждое из Fog-устройств ‒ набор 
параметров, характеризующих устройство с точки 
зрения элемента вычислительного кластера: CPU 
(количество и частота ядер процессора), GPU (производительность 
графического 
процессора), 
Network (тип интерфейса и тип технологии связи, 
в том числе возможная скорость подключения), и 
многие другие. В текущей модели, для развертывания FaaS-unit, с 4-мя типовыми контейнерами 
(𝑀𝑆𝑖, ∀ 𝑖 ∈𝑁) и оценки предлагается использовать 
временны́ е характеристики реализации задачи. 
Вводя типизацию контейнеров 𝑀𝑆𝑖, возможно перейти 
к 
оценке 
характеристик 
самого 
Fogустройства, а также собственно контейнера в пространстве времени, а именно: времени реализации 
задачи типовым контейнером 𝑇𝑐 (мс) и задержки 
передачи соответствующего объема информации 
𝑇𝑟 (мс) , который является конечным. Типовой 
контейнер решает одну задачу: например, сортировка данных методом пузырька, где определен 
соответствующий формат и объем принимаемых 
данных, а также формат и объем передаваемых 
результатов. 
Fog-устройства (FD)
Сеть
Сервера услуг
...
MS1
MS2
MS3
Живая миграция типовых 
микросервисов MSi
Пользователи (UD)
3
4
1
...
3
4
1
...
2
2
m
n
После AGW
MS4
Рис. 2. Модель сети туманных вычислений при поиске 
группы Fog-устройств 
Fig. 2. Fog Computing Network Model for Group of Fog-Devices 
Searching 
Таким образом, необходимо определить параметры, описывающие каждое из исследуемых Fogустройств. При этом некоторые параметры могут 
оцениваться на уровне сравнения с предельными 
значениями: например, количество выделенной 
логической ОЗУ, необходимой для работы готовящегося к миграции микросервиса или группы 
микросервисов. В рамках данной задачи были 
определены параметры, описывающие Fog-узел с 
точки зрения затраченного времени реализации 
типовой задачи.  
В общем виде целевая функция оценки устройства может быть представлена следующим образом: 
𝐹 = ∑𝑘𝑖𝑃𝑖
𝑚
𝑖=1
, 
(1) 
где 0 ≤ 𝑘𝑖≤1 ‒ коэффициенты, при этом ∑𝑘𝑖= 1; 
𝑃𝑖 ‒ параметры оценки, при m ‒ количество параметров для ∀ 𝑖 >  0, 𝑖∈𝑁, а основная задача 
сформулирована в виде выражения: 
 {𝐹1,, 𝐹2, 𝐹3, 𝐹4} = arg min
𝑇𝑟 ,𝑇𝑐,𝑆
{𝐹}, 
(2) 
где  {𝐹1,, 𝐹2, 𝐹3, 𝐹4} ‒ ряд решений 𝐹(𝑇𝑟 , 𝑇𝑐, 𝑆) в порядке неубывания их значений, то есть: 𝐹1 ≤ 𝐹2 ≤
 ≤𝐹3 ≤ 𝐹4. 𝑇𝑐 ‒ время выполнения типовой задачи 
в типовом контейнере 𝑀𝑆𝑖. Параметр 𝑇𝑟  характеризует связь узла с основным шлюзом-брокером в 
соответствующей зоне Fog, через который проходят все транзакции между устройствами и, соот 
Proceedings of Telecommunication Universities                                                           2024. Vol. 10. Iss. 3 
 
 
Electronics, Photonics, Instrumentation and Communications 
29                                              tuzs.sut.ru 
 
ветственно, контейнерами; может быть представлен как задержка при передаче типового запроса с 
соответствующим объемом данных до шлюзаброкера. Параметр 𝑆 описывает степень стабильности кластера Fog, в частности рассматривается 
стабильность соответствующего устройства в кластере Fog. При этом вероятность того, что расстояние между элементом кластера Fog и шлюзомброкером превысит величину R, может быть оценено как [9]: 
𝑝(𝑑> 𝑅) =  1 −Dis(𝑅, δ), 
(3) 
где Dis(𝑅, δ) – функция распределения вероятности расстояния между между устройством и координатором. В зависимости от условий, она может 
быть описана Гамма-распределением или его 
частными формами, например, распределением 
Пирсона или распределением Райса. 𝑅 ‒ расстояние между устройством и координатором. При 
этом вероятность того, что расстояние между координатором и хотя бы одним из элементов кластера превышает величину R определяется как [9]:  
𝑝(> 𝑅) =  1 − ∏Dis(𝑅, δ𝑖)
𝑘
𝑖=1
, 
(4) 
где k – число устройств кластера Fog; δ𝑖 – параметр 
распределения вероятности расстояния. 
Таким образом, в работе [9] было сформулировано предложение о максимальной стабильности 
Fog-кластера, где под стабильностью понимается 
стремление сохранить структуру, то есть связи 
между координатором и элементами кластера: 
min
𝑅{1 − ∏Dis(𝑅, δ𝑖)
𝑘
𝑖=1
}. 
(5) 
Стабильность кластера Fog может быть представлена как оценка, описанная в виде функции 
минимизации вероятности изменения географического состояния устройства относительно так 
называемого центра масс кластера или шлюзаброкера в кластере Fog, через который проходят 
все транзакции. Данная задача может быть решена 
с использованием точечных процессов Неймана ‒ Скотта [9].  
Соответственно, стабильность самого устройства относительно его отношения к шлюзуброкеру будет представляться в следующим виде: 
min
𝑅{1 −Dis(𝑅, δ)}, 
(6) 
а используемый параметр оценки стабильности 𝑆 
в основной целевой-функции может быть представлен следующим образом:  
𝑆= max {min
𝑅{1 −Dis(𝑅, δ)}}. 
(7) 
В данной работе предлагается рассмотреть 
частный случай, как ранее было упомянуто, в качестве исследуемых параметров взять оценку Fogузла с точки зрения 𝑇𝑐 и 𝑇𝑟, а также параметра 
оценки стабильности устройства в кластере, где 
0 < 𝑆≤1.  
Таким образом, целевая функция, исследуемая в 
данной работе, может быть представлена в следующем виде: 
𝐹= ∑𝑘𝑖𝑃𝑖 
𝑚
𝑖=1
= 𝑘1𝑇𝑟 + 𝑘2 𝑇𝑐 + 𝑘3𝑆. 
(8) 
Данный метод может быть использован при 
выполнении условия типизации контейнеров для 
развертывания serverless. В общем случае, метод 
может быть сложно реализуем с точки зрения 
необходимости предварительного тестирования 
среднего времени выполнения функции соответствующим нетиповым контейнером общей услуги. 
Для решения поставленной задачи был исследован класс метаэвристических алгоритмов, которые позволяют определить глобальный экстремум фитнес-функции. Существует немало метаэвристических алгоритмов. Рассмотрим основные: 
алгоритм стаи серых волков (GWO, аббр. от англ. 
Grew Wolf Optimizer), алгоритм оптимизации роя 
частиц (PSO, аббр. от англ. Particle Swarm Optimization), генетический алгоритм (GA, аббр. от англ. 
Genetic Algorithm) и алгоритм роя сальп (SSA, аббр. 
от англ. Salp Swarm Algorithm). Каждый из этих 
алгоритмов имеет свои уникальные особенности и 
преимущества. К их сходству можно отнести следующие критерии: все они являются популяционными (то есть, они работающими с популяцией 
кандидатных решений) и используют итеративный процесс поиска. 
В целом GWO, PSO, GA и SSA являются мощными 
и эффективными алгоритмами, которые полезно 
использовать для решения широкого спектра оптимизационных задач. Выбор алгоритма зависит 
от конкретной задачи и требований к производительности. Так, например, генетический алгоритм 
является более ресурсозатратным, что также влечет за собой скорость/время сходимости.  
В результате анализа особенностей данных алгоритмов, в частности при рассмотрении требований к скорости сходимости, простоте (в том числе 
при настройке параметров), предлагается использовать алгоритм GWO (метаэвристический алгоритм, который был вдохновлен социальным поведением серых волков). Серые волки ‒ это социальные животные, которые живут в стае, возглавляемой альфа-самцом. Метаэвристический алгоритм 
GWO имитирует социальное поведение серых волков, чтобы решить оптимизационные задачи ‒ 
поиск экстремума фитнес-функции. Алгоритм 
 
Труды учебных заведений связи                                                                                             2024. Т. 10. № 3 
 
 
Электроника, фотоника, приборостроение и связь 
30                                              tuzs.sut.ru 
 
инициализирует популяцию волков (возможных 
решений) и оценивает каждого волка в популяции 
𝑋α, 𝑋β, 𝑋δ. Затем алгоритм итеративно обновляет 
положения волков на основе их текущих позиций 
и позиций других волков в стае.  
ТАБЛИЦА 1. Сравнительный анализ 
TABLE 1. Comparing Analysis 
 
Метаэвристические алгоритмы 
GWO 
PSO 
GA 
SSA 
Биологическая  
модель-источник 
социальное поведение 
волков 
поведении стаи птиц 
процесс эволюции 
поведение роя сальп 
(морские животные) 
Структура популяции 
волки 
частицы 
хромосомы 
сальпы 
Принцип обновления 
волки обновляют свои 
положения на основе 
позиций альфа-, бета-  
и дельта-волков 
частицы обновляют свои 
положения на основе 
своих собственных  
лучших положений  
и лучшего положения  
в популяции 
хромосомы обновляются 
с помощью операций 
кроссовера и мутации 
сальпы обновляют свои 
положения на основе 
положения передней и 
задней сальпы 
Топологии 
волки расположены  
в иерархической  
структуре 
частицы обычно  
расположены  
в топологии кольца 
хромосомы не имеют 
определенной топологии 
сальпы расположены  
в одномерной топологии 
 
Таким образом, на каждой итерации, в том числе финальной, алгоритм будет выдавать 4 результата, которые по возрастающей будут отражать 
соответствующие устройства, пригодные для последующего размещения типовых микросервисов 
услуг. 
В GWO позиция каждого волка рассчитывается 
на основе следующих выражений (9‒11):  
𝐷α = |𝐶1𝑋𝑎 −𝑋(𝑖) |,  𝐷β = |𝐶2𝑋β −𝑋(𝑖) |, 
(9) 
 𝐷δ = |𝐶3𝑋δ −𝑋(𝑖) |, 
𝑋1  = 𝑋𝑎 −𝐴1(𝐷α ),  𝑋2  = 𝑋β −𝐴2 ∗(𝐷β ), 
(10) 
𝑋3  = 𝑋δ −𝐴3 ∗(𝐷δ ), 
𝑋(𝑥+ 1) = 𝑋1  +  𝑋2 + 𝑋3 
3
, 
(11) 
где [ 𝑋α, 𝑋β, 𝑋δ, 𝑋ω]: альфа-волк (𝑋α) ‒ лучшее решение в популяции; бета-волк (𝑋β) ‒ второе лучшее решение в популяции; дельта-волк (Xδ) ‒ третье лучшее решение в популяции; омега-волки 
( 𝑋ω ) ‒ остальные волки в популяции; 𝑋𝑖  ‒ 
позиция решения на соответствующей итерации 𝑖; 
𝐷α , 𝐷β, 𝐷δ – вспомогательные векторы для расчета 
соответственно значений 𝑋1 , 𝑋2 , 𝑋3 .  
На каждой итерации алгоритма обновляются 
коэффициенты A и C, согласно следующим выражениям (12‒14): 
𝐴 =  2𝑎∙𝑟1 −𝑎, 
(12) 
𝐶= 2 𝑟2, 
(13) 
𝑎= 2 −(𝑖∙2
𝐼). 
(14) 
где 𝑎 – параметр, линейно уменьшающейся от 2 до 
0 на каждой итерации согласно выражению (14); 
𝑟1 , 𝑟2  – равномерно распределенные случайные 
числа от 0 до 1; I – количество итераций. 
Следственно параметрами, которые могут повлиять на эффективность алгоритма GWO, являются количество волков, количество итераций, а 
также параметры 𝑎 и C. При этом, величина C на 
каждой итерации обеспечивает диверсификацию 
алгоритма GWO. Схематично работа алгоритма 
представлена на рисунке 3. 
Xa
a1
c1
X(t+1)
a2
c2
a3
c3
X(t)
Рис. 3. Схема работы алгоритма 
Fig. 3. Algorithm Scheme 
 
 
Proceedings of Telecommunication Universities                                                           2024. Vol. 10. Iss. 3 
 
 
Electronics, Photonics, Instrumentation and Communications 
31                                              tuzs.sut.ru 
 
Алгоритм GWO представлен в виде псевдокода:  
0:BEGIN 
1:   Инициализация популяции GWO ‒ 𝑋𝑖(𝑖: = 1, 2, . . . , 𝑛); 
2:   Инициализация  GWO-параметров: a, A и C; 
3:   Расчет фитнес-функции 𝑭𝑿 для каждого агента; 
4:    /* комментарий: 
5:       𝑋α ‒ лучший агент 
6:       𝑋β‒- второй лучший агент 
7:       𝑋δ ‒ третий лучший агент 
8:       𝑋ω ‒ остальные агенты 
9:    */ 
10:  Нахождение 3-х лучших 𝑋α, 𝑋β, 𝑋δ; 
11:  While достижение критерия do 
12:    for каждого агента do 
13:    Обновление позиции согласно с формулой (11); 
14:    end for 
15:    обновить a, A и C; 
16:    расчет 𝑭𝑿 фитнес-функции для каждого агента; 
17:    обновление 𝑋α, 𝑋β, 𝑋δ; 
18:  end while 
19: вернуть значение 𝑋α, лучшее значение F(𝑋α), а также значение на последней итерации алгоритма [𝑋β, 𝑋δ, 𝑋ω]; 
20:END. 
GWO ‒ это мощный и эффективный алгоритм, 
который можно использовать для решения широкого спектра оптимизационных задач, в том числе 
для поиска группы значений экстремумов функции, в том числе фитнес-функции, описывающей 
каждое из Fog-устройств в туманности. 
 
Результаты моделирования 
Для моделирования было проведено предварительное натурное исследование существующих 
платформ: Kata, Firecracker, Wasm, Docker. Тестирование проводилось на базе серверного оборудования лаборатории Meganetlab 6G кафедры сетей 
связи и передачи данных СПбГУТ. Для исследования были выбраны два метода распределения поступающей нагрузки: 
Метод 1. Каждый новый запрос, приходящий на 
агент (брокер/шлюз), направлялся на типовой 
микросервис/контейнер, 
который 
обслуживал 
запросы в порядке очереди FIFO. 
Метод 2. Каждый новый запрос, приходящий на 
агент, обслуживался вновь созданным клоном типового микросервиса, после чего данный клон 
уничтожался. Таким образом, система гибко масштабировалась под рост нагрузки. 
Для этого рассматривался пример работы типового микросервиса, который был представлен в 
виде контейнера и решал типовую задачу ‒ расчет 
числа π. На рисунке 4a в виде пузырьковой диаграммы отражены результаты тестирования первого метода на реальном стенде, где были развернуты платформы и реализованы типовые контейнеры. На рисунке 4b в виде пузырьковой диаграммы отражены результаты тестирования второго 
метода на реальном стенде, где при поступлении 
новой задачи происходил принудительный старт 
нового контейнера в кластере, и задача выполнялась параллельно.  
На рисунке 4 размер пузырька является средним 
временем решения типовой задачи, вычисленным 
на базе пяти повторных экспериментов. При этом 
ось абсцисс представлена в логарифмическом масштабе на обоих рисунках. Представленные графики позволяют визуально оценить возможности 
существующих решений контейнеризации для 
сегментов Fog, учитывая различные подходы к 
распределению вычислений в микросервисной 
архитектуре. Согласно рисунку 4b, Docker при росте нагрузки и сравнительно равным (в меньшей 
степени изменяющимся) временем старта нового 
контейнера для каждый задачи уменьшает время 
реализации типовой задачи. При этом Docker находится на втором месте после Wasm, относительно 
времени старта типового контейнера.  
 
a) 
 
b) 
Рис. 4. Диаграммы: по методу 1 (a); по методу 2 (b) 
Fig. 4. Diagram for method 1 (a) and method 2 (b) 
Для дальнейшего моделирования были взяты 
за основу данные, которые измерялись на стенде 
при работе Docker-контейнеров ввиду эффективности их работы при возрастающих нагрузках и 
большом количестве типовых микросервисов, что 
может быть свойственно Fog. Полученные в ходе 
эксперимента результаты легли в основу набора 
данных 𝑇𝑟 и 𝑇𝑐,  которые были составлены для 100  
Fog-устройств, при этом структура этих данных 
была согласована с целевой функцией, представленной в выражении (8). Для визуализации ре 
Труды учебных заведений связи                                                                                             2024. Т. 10. № 3 
 
 
Электроника, фотоника, приборостроение и связь 
32                                              tuzs.sut.ru 
 
зультата параметр устойчивости узла был установлен в виде константы 𝑆= const = 1, а коэффициенты 𝑘1, 𝑘2 и 𝑘3 ‒ равными 0,33. Итог моделирования Fog-устройств представлен на рисунке 6, где 
ось абсцисс ‒ это значение 𝑘1𝑇𝑟 ,  ось ординат ‒ 
представляет значение 𝑘2 𝑇𝑐, а ось аппликат, соответственно, ‒ значение целевой функции F. Далее 
на поле данных смоделированных Fog-устройств 
был применен алгоритм GWO, разработанный 
также на языке программирования Python. Результат поиска группы устройств с помощью данного алгоритма представлен на рисунке 7.  
 
0,002
0,004
0,006
0,008
0,0010 0,0012
0,7
0,8
0,9
1,0
1,1
1,2
0,00
0,05
0,10
0,15
0,20
0,25
0,30
0,35
Tr
Tp
 
Рис. 6. Результаты моделирования Fog-устройств 
Fig. 6. Results of Fog-Devices Modeling 
0,002 0,004
0,006 0,008
0,0010 0,0012
0,7
0,8
0,9
1,0
1,1
1,2
0,00
0,05
0,10
0,15
0,20
0,25
0,30
0,35
Tr
Tp
 
Рис. 7. Результат работы алгоритма GWO 
Fig. 7. Result of GWO Algorithm Working 
В ходе поиска были найдены основные три 
устройства, выделенные фиолетовым цветом (см. 
рисунок 7), а четвертое является первым устройство во множестве данных ω.  
Оценка эффективности метода 
Для оценки эффективности предложенного метода предлагается сравнить алгоритм GWO с алгоритмом PSO, который также является одним из 
эффективных с точки зрения скорости схождения 
и затрат вычислительных ресурсов. Ниже представлено описание данного алгоритма.  
Каждая отдельная частица i состоит из трех 
векторов: ее положение в D-мерном пространстве 
поиска 𝑥𝑖̅ = (𝑥𝑖1, 𝑥𝑖2, … , 𝑥𝑖𝐷), лучшая найденная позиция 𝑝𝑖̅ = (𝑝𝑖1, 𝑝𝑖2, … , 𝑝𝑖𝐷), направленная скорость 
движения 𝑣𝑖̅ = (𝑣𝑖1, 𝑣𝑖2, … , 𝑣𝑖𝐷). При запуске алгоритма частицы равномерно, случайным образом 
инициализируются по всему пространству поиска, 
при этом скорость частиц также инициализируется случайным образом. Сформированные частицы 
перемещаются по пространству поиска с помощью 
довольно простого набора уравнений обновления 
векторов частицы. Алгоритм обновляет весь рой 
на каждом временно́м шаге, обновляя скорость и 
положение всякой частицы в каждом измерении 
по следующим правилам: 
𝑣𝑖𝑑= 𝑣𝑖𝑑+ 𝑐ε1(𝑝𝑖𝑑−𝑥𝑖𝑑) + 𝑐ε2(𝑝𝑔𝑑−𝑥𝑖𝑑), 
(15) 
𝑣𝑖𝑑= 𝑥𝑖𝑑+ 𝑣𝑖𝑑, 
(16) 
где c – константа со значением 2; 0, 1 и 2 – независимые случайные числа, уникально генерируемые 
при каждом обновлении для всякого отдельного 
измерения от d = 1, до D; 𝑝𝑔𝑑 – положение, найденное любой соседней частицей.  
Процесс обновления кратко описан в алгоритме 
PSO, представленном в виде псевдокода:  
1.for каждого шага t do; 
2.   for каждой частицы i в рое do 
3.        обновить позицию xt, используя выражения (15) и (16) 
4.        рассчитать фитнес-функцию для xt f (xt ) 
5.        обновить pi, pg  
6.   end for; 
7.end for. 
Стоит отметить, что в алгоритме скорость частиц фиксируется на максимальном значении 
vmax. Без фиксации алгоритм склонен не сойтись, 
когда расчет значений (15) и (16) приводил бы к 
быстрому увеличению скорости и, следовательно, 
положения частиц, приближались бы к бесконечности. Параметр vmax не позволяет системе войти 
в данное состояние, ограничивая скорость всех 
частиц. 
Для оценки эффективности алгоритмов была 
проведена серия экспериментов, где отслеживалось время схождения алгоритмов, то есть время 
поиска устройств для последующей миграции 
группы типовых контейнеров. Функция счета времени была реализована в разработанном программном коде самой модели на языке Python. Результаты сравнения представлены на рисунке 8. 
 
Proceedings of Telecommunication Universities                                                           2024. Vol. 10. Iss. 3 
 
 
Electronics, Photonics, Instrumentation and Communications 
33                                              tuzs.sut.ru 
 
0,00501
0,03
0,00262
0,00302
0,003
0,00336
0,00644
0,00325
0,00399
0,00407
0,19197
0,16731
0,18659
0,18846
0,17606
0,18466
0,18219
0,17376
0,19056
0,18344
Рис. 8. Cравнение алгоритмов PSO и GWO 
Fig. 8. PSO and GWO Algorithms Comparing 
Среднее время в течение 10-ти экспериментов 
по алгоритму PSO составило 0,18249 с, в то время, 
как по алгоритму GWO ‒ 0,00377 с, что меньше 
примерно в 48 раз. Также стоит учесть, что PSO в 
результате работы выдавал значение глобального 
экстремума целевой функции, то есть находил одно устройство, наиболее подходящее для миграции контейнера, в отличие от GWO, который позволял найти группу устройств (α, β, δ, ω), соответственно. Стоит отметить, что данное сравнение 
алгоритмов актуально в рамках исследуемой задачи. Как ранее было приведено в теоретической 
части, метаэвристические алгоритмы обладают 
своими особенностями ввиду собственной «проекции» с биологического мира, примерами которого они были вдохновлены. Соответственно, PSO 
может быть также достаточно эффективным решением в рамках других условий задачи.  
 
Выводы 
Исследования и разработки в области распределенных вычислений занимают немалую часть 
работ в современных и перспективных сетях и 
услугах. Одной из ожидаемых инфраструктурных 
технологий являются Fog, в частности, динамические. 
Данная 
технология 
позволит 
снизить 
нагрузку на ядро сети, замыкая пользовательский 
трафик, а также ‒ приблизиться к достижению 
цели снижения энергозатрат вычислительной инфраструктуры в условиях бурного роста ЦОДов. В 
качестве практических исследований, в статье 
приводится разработанная модель и метод поиска 
группы Fog-устройств для последующей миграции 
типовых контейнеров одной из платформ FaaS. В 
качестве математической базы предлагается для 
решения сложной оптимизационной задачи использовать метаэвристические алгоритмы. В работе приводится практическое сравнение работы 
двух распространенных алгоритмов: GWO и PSO. В 
результате моделирования на основе экспериментальных данных эффективное решение поставленной задачи было достигнуто на основе алгоритма GWO. 
Список источников  
1. Market Overview // Straits research. URL: https://straitsresearch.com/report/data-center-equipment-market (дата 
обращения 31.05.2024) 
2. Колбанёв М.О., Палкин И.И., Пойманова Е.Д., Татарникова Т.М. Пути создания зеленых информационных технологий // Гидрометеорология и экология. 2021. № 62. С. 127‒138. DOI:10.33933/2074-2762-2021-62-127-138. 
EDN:OEJEMQ 
3. Manner J. Black software ‒ the energy unsustainability of software systems in the 21st century // Oxford Open Energy. 
2023. Vol. 2. DOI:10.1093/ooenergy/oiac011 
4. Alloghani M.A. Architecting Green Artificial Intelligence Products: Recommendations for Sustainable AI Software Development and Evaluation // Artificial Intelligence and Sustainability. Signals and Communication. Cham: Springer, 2024. 
PP. 65–86. DOI:10.1007/978-3-031-45214-7_4  
5. Schwartz R., Dodge J., Smith N.A., Etzioni O. Green AI // Communications of the ACM. 2020. Vol. 63. Iss. 12. PP. 54–63. 
DOI:10.1145/3381831 
6. Li Y., Zhu Z., Guan Y., Kang Y. Research on the structural features and influence mechanism of the green ICT transnational cooperation network // Economic Analysis and Policy. 2022. Vol. 75. PP. 734–749. DOI:10.1016/j.eap.2022.07.003 
7. Кричевский Г.Е. Экология и «Зеленые технологии». Как сдержать превращение биосферы в техносферу? // 
НБИКС ‒ Наука. Технологии. 2019. Т. 3. № 8. C. 22‒26.  
8. Волков А.Н. Туманность в перспективных сетях связи для услуг телеприсутствия // Электросвязь. 2024. № 4. 
С. 50‒56.  
9. Волков А.Н. Стабильность кластера в динамических туманных вычислениях // Электросвязь. 2024. № 6. С. 8‒16.  
10. Марочкина А.В. Моделирование и кластеризация трехмерной сети интернета вещей с применением метода 
оценки фрактальной размерности // Электросвязь. 2023. № 6. С. 60‒66. DOI:10.34832/ELSV.2023.43.6.008. EDN:ZBNQKI 
11. Марочкина А.В. Выбор головных узлов кластеров в трехмерных сетях Интернета вещей высокой плотности // 
Электросвязь. 2023. № 7. С. 26‒32. DOI:10.34832/ELSV.2023.44.7.004. EDN:MKMNQZ 

Труды ИСП РАН, том 34, вып. 4, 2022 г. // Trudy ISP RAN/Proc. ISP RAS, vol. 34, issue 4, 2022 
211 
DOI: 10.15514/ISPRAS-2022-34(4)-15 
Построение требований и архитектуры 
облачного оркестратора платформенных 
сервисов 
Н.А. Лазарев, ORCID: 0000-0002-1008-1022 <lazarevn@ispras.ru> 
О.Д. Борисенко, ORCID: 0000-0001-8297-5861 <al@somestuff.ru> 
Институт системного программирования им. В.П. Иванникова РАН, 
109004, Россия, г. Москва, ул. А. Солженицына, д. 25 
Аннотация. 
Облачные 
технологии 
предоставляют 
пользователям 
простое 
и 
надежное 
масштабирование ресурсов, за счет чего они получили широкое распространение. На сегодняшний день 
особенно актуальна задача управления распределенными службами в облачной среде. Для этого 
используются специальные программы “оркестраторы”, которые реализуют функции управления 
жизненным циклом приложений. Однако существующие решения имеют множество ограничений и 
неприменимы в общем случае. Кроме того, не существует единого стандарта или протокола для 
взаимодействия с такими инструментами, из-за чего требуется адаптация программ для каждого 
частного случая. Основными задачами этой статьи являются выявление требований к оркестратору 
платформенного уровня облачных вычислений (PaaS), а также предложение подходов к построению 
гибкой архитектуры инструментов этого класса. 
Ключевые слова: облачные вычисления; оркестрация; PaaS; распределенные системы; TOSCA; OCCI 
Для цитирования: Лазарев Н.А., Борисенко О.Д.  Построение требований и архитектуры облачного 
оркестратора платформенных сервисов. Труды ИСП РАН, том 34, вып. 4, 2022 г., стр. 211-228 DOI: 
10.15514/ISPRAS-2022-34(4)-15 
Requirements and architecture design for cloud PaaS orchestrator 
N.A. Lazarev, ORCID: 0000-0002-1008-1022 <lazarevn@ispras.ru> 
O.D. Borisenko, ORCID: 0000-0001-8297-5861 <al@somestuff.ru> 
Ivannikov Institute for System Programming of the Russian Academy of Sciences, 
25, Alexander Solzhenitsyn st., Moscow, 109004, Russia 
Abstract. Cloud technologies provide abilities for simple and reliable scaling of resources, due to which they 
have become widespread. The task of managing distributed services in a cloud environment is especially 
relevant today. Special programs are used for that purpose named “orchestrators” which implement the 
functions of lifecycle management for applications. However, the existing solutions have many limitations and 
are not applicable in the general case. Also there is no single standard or protocol for interaction with such tools 
which requires adaptation of programs for each particular case. The main objectives of this paper are to identify 
the requirements for a platform-level cloud computing (PaaS) orchestrator, as well as to propose flexible 
architecture patterns for such tools. 
Keywords: cloud computing; orchestration; PaaS; distributed systems; TOSCA; OCCI 
For citation: Lazarev N.A., Borisenko O.D. Requirements and architecture design for cloud PaaS orchestrator. 
Trudy ISP RAN/Proc. ISP RAS, vol. 34, issue 4, 2022. pp. 211-228 (in Russian). DOI: 10.15514/ISPRAS2022-34(4)-15 
Lazarev N.A., Borisenko O.D. Requirements and architecture design for cloud PaaS orchestrator. Trudy ISP RAN/Proc. ISP RAS, vol. 34, 
issue 4, 2022, pp. 211-228 
212 
1. Введение 
Постоянное развитие технологий производства компьютерной техники приводит к её 
заметному удешевлению, что позволяет строить все большие вычислительные центры. В 
связи с этим получили толчок в развитии облачные вычисления – технология предоставления 
доступа к вычислительным ресурсам посредством сети Интернет, абстрагирующая 
пользователей от физических устройств. Согласно определению NIST [1], облачные 
вычисления подразделяются на три базовых уровня: 
● 
инфраструктурный (IaaS) – предоставление виртуальных машин, блочных устройств и 
настройка сетевого взаимодействия; 
● 
платформенный (PaaS) – размещение на ресурсах провайдера облачных услуг библиотек, 
сервисов 
и 
инструментов, 
поддерживаемых 
провайдером, 
для 
дальнейшего 
использования пользователем в своих программах без прямого управления нижележащей 
инфраструктурой; 
● 
сервисный (SaaS) – обеспечение доступа пользователя к конечным приложениям, 
расположенным на ресурсах облачного провайдера.  
Кроме того, облачные платформы подразделяются по признаку доступности ресурсов: 
публичные облака предоставляют услуги для любых пользователей, а приватные работают в 
рамках некоторой организации. 
Для каждого уровня предоставления услуг, как правило, требуется специальное программное 
обеспечения для управления соответствующими ресурсами, все чаще называемое 
оркестраторами [2]. На сегодняшний день достаточно полно описан уровень IaaS[3]: 
определены предоставляемые ресурсы и способы предоставления доступа к ним. Для более 
высоких уровней существует множество реализаций, имеющих различные сферы 
применения и наборы интерфейсов, которые современные стандарты не покрывают. Эта 
статья посвящена проблематике разработки PaaS-оркестратора: определению актуальных 
задач, требуемого функционала и необходимых компонентов для применимости как для 
развертывания заранее подготовленных сервисов, так и описания пользователями сценариев 
развертывания собственного программного обеспечения, например, в процессе разработки. 
Под PaaS-оркестратором здесь понимается программное обеспечение, обладающее 
следующей базовой функциональностью: 
● 
развертывание ресурсов по запросу; 
● 
абстрагирование пользователей от управления виртуальными машинами, сетями и 
дисками; 
● 
возможность развертывания сервисов на виртуальных машинах без использования 
технологий контейнеризации; 
С логической точки зрения, это программное обеспечение явным образом отделено от 
инфраструктурного слоя и использует доступные вызовы для получения инфраструктурных 
ресурсов облачного провайдера (виртуальных машин или контейнеров; программноопределяемых сетей и хранения). Сама же работа по управлению распределенными 
системами происходит уже не при помощи IaaS API облачного провайдера, а на уровне 
управления и настройки ОС и программных компонентов в экземплярах виртуальных машин 
или контейнеров. 
Остальная статья организована следующим образом. Во втором разделе рассмотрены 
вычислительные задачи из различных сфер, рассмотрены основные преимущества 
контейнерных технологий, на основе чего сформулированы требования к PaaS-оркестратору. 
В 
третьем 
разделе 
проведен 
обзор 
существующих 
инструментов 
оркестрации, 
соответствующих рассматриваемой области, а также описаны применяемые технологии. 
Далее, в четвертом разделе предложена общая архитектура оркестратора исходя из 
сформулированных требований и рассмотренных решений. Наконец, в пятом разделе 
подведены итоги и сформулированы направления для дальнейшей работы. 
Лазарев Н.А., Борисенко О.Д.  Построение требований и архитектуры облачного оркестратора платформенных сервисов. Труды 
ИСП РАН, том 34, вып. 4, 2022 г., стр. 211-228 
213 
2. Анализ применимости 
Несмотря на наличие множества инструментов, называемых оркестраторами, на 
сегодняшний день не устоялось ни однозначного определения данного термина, ни даже 
функционала, который должен предоставлять тот или иной оркестратор. Из этого также 
вытекает отсутствие однозначных критериев сравнения и оценки таких инструментов. 
Зачастую каждый автор выбирает интересующий его набор критериев, который формируется 
на основе некоторых конкретных задач. Например, в сравнении TOSCA-оркестраторов [4], 
наиболее значимыми критериями сравнения являются поддерживаемая версия стандарта 
TOSCA и поддерживаемые облачные платформы, но наряду с ними оцениваются также год 
выпуска, поддерживаемые платформы (операционные системы), сложность установки и 
другие параметры, не отражающие функциональности инструментов. В другой статье [5], 
посвященной сравнению облачных оркестраторов, достаточно подробно рассматриваются 
аспекты, касающиеся работы оркестраторов в облачной среде, однако не рассматриваются 
способы обработки пользовательских данных и доступа к предоставляемым сервисам. 
Одной из основных целей данной работы является формирование универсальных требований 
к инструментам оркестрации, которые позволили бы сравнивать их с точки зрения 
функциональности. 
2.1 Актуальные задачи 
На данный момент границы PaaS сильно размыты, так как набор услуг для работы конечных 
приложений сильно варьируется. Рассмотрим несколько сценариев использования 
оркестраторов в различных сферах. 
С распространением удаленного обучения стремительно возрос спрос на обеспечение 
виртуальных учебных мест. Облачные платформы позволяют удовлетворить этот спрос 
различными способами: предоставляя сервисы для видеоконференций, обмена файлами, 
проверки заданий, либо предоставляя сами вычислительные ресурсы напрямую. Однако, 
существуют случаи, когда необходимо предоставить доступ конечных пользователей, 
например, студентов, непосредственно к некоторой платформе для получения навыков 
работы с ней. Примером может служить обучение работе с HPC-платформами или написание 
программ, ориентированных на работу с GPU. Помимо развертывания таких платформ, 
требуется также оркестрация инструментов взаимодействия с ними: от установки и 
конфигурации компиляторов до развертывания дополнительных сервисов. 
Сегодня облачные вычисления применяются в естественных и гуманитарных науках[6], в том 
числе: физике и астрофизике, биоинформатике и медицине, социальных и других 
гуманитарных науках, науках о Земле и окружающей среде, химии (хемоинформатике). 
В некоторых направлениях успели сложиться свои традиции и популярные наборы 
программных инструментов, в то время как другие начинали формироваться в последние 
несколько лет. Однако, в большинстве случаев справедливы следующие общие проблемы 
[7,8,9,10,11].  
1) Необходимость хранения и обработки больших объемов данных. Современные задачи 
ориентированы на построение крупных и сложных моделей, в связи с чем возрастают 
требования на хранимые данные. Данные могут быть получены из открытых источников, 
собраны с помощью дополнительного оборудования, сгенерированы, а также получены 
от экспертов. Кроме того, в некоторых случаях требуется дополнительная предобработка 
данных: анонимизация и шифрование, устранение дубликатов и некорректных записей, 
а также другая подготовка к вычислениям. Таким образом, необходимо обеспечение как 
достаточного объема хранилища, так и возможность интеграции со сторонними 
программными и аппаратными средствами. 
2) Организация параллельных вычислений. Другое следствие роста объемов данных – 
нелинейное повышение времени работы алгоритмов их обработки. В то время как 
Lazarev N.A., Borisenko O.D. Requirements and architecture design for cloud PaaS orchestrator. Trudy ISP RAN/Proc. ISP RAS, vol. 34, 
issue 4, 2022, pp. 211-228 
214 
возможности вертикального масштабирования вычислительных ресурсов ограничены и 
требуют все больших затрат, современные платформы параллельной обработки данных 
на 
большом 
числе 
вычислительных 
узлов 
позволяют 
достигать 
повышения 
производительности программ без использования дорогостоящего оборудования. Кроме 
того, многие современные программные средства позволяют использовать графические 
ускорители для повышения эффективности программ. Хотя облачная среда позволяет 
автоматически создавать и настраивать виртуальные машины, особенно актуальна задача 
автоматизации настройки сложных распределенных систем. 
3) Массовое применение методов машинного обучения. Наряду с задачами использования 
вычислительных ресурсов свои требования накладывает развитие методов обработки 
данных. Методы машинного обучения также являются ресурсоемкими и чувствительны 
к обеспечению горизонтального масштабирования. Эта область постоянно развивается, в 
связи с чем набор инструментов, используемых в ML-платформах постоянно растет, в 
связи с чем необходимо наличие механизма постоянного обновления, используемого в 
конкретном 
решении 
используемого 
инструментария. 
Наконец, 
использование 
машинного обучения и нейронных сетей тесно связано с построением нелинейного 
рабочего процесса, требующего постоянного обновления данных и взаимодействия с 
пользователями.  
4) Потребность в обмене и организации общего доступа к данным. Современные задачи 
все чаще требуют участия множества специалистов из различных областей. В связи с 
этим требуются дополнительные службы, упрощающие взаимодействие между 
различными пользователями систем. 
5) Обработка ошибок. Вместе с повышением сложности вычислительных систем все 
критичнее становится необходимость в автоматическом обнаружении и исправлении 
ошибок, а также сборе информации об ошибках. Для этих нужд необходима сквозная 
интеграция систем мониторинга и сбора логов, учитывающая специфику отдельных 
приложений. 
Еще одной важной сферой для анализа задач оркестрации является коммерческая разработка 
приложений. С одной стороны, в современном бизнесе все чаще используют те же 
технологии, что и в науке, в том числе, машинное обучение и анализ больших данных. С 
другой стороны, в процессе коммерческого использования приложений критичны такие 
параметры, как время ответа пользователю и время недоступности сервиса, в то время как в 
науке и образовании этому уделяют меньше внимания. В связи с этим, например, задача 
масштабирования усложняется тем, что необходимо поддерживать постоянную доступность 
приложения для конечных пользователей. Также возникает явная необходимость 
обеспечения балансировки нагрузки, обеспечения корректного обновления системы.  
2.2 Контейнерная оркестрация 
Фактически, стандартным решением для построения коммерческих приложений стало 
использование контейнеров – легковесной виртуализации на уровне операционной системы 
и соответствующих оркестраторов, так как они обладают рядом преимуществ при 
необходимости обеспечения бесперебойной работы: 
● 
легковесность контейнеров относительно виртуальных машин; 
● 
образы контейнеров содержат все необходимое для запуска приложения; 
● 
прозрачное версионирование приложения; 
● 
абстракция от физических ресурсов и операционной системы при запуске; 
● 
простое создание идентичных экземпляров приложения; 
● 
предсказуемое поведение приложения при запуске. 
Лазарев Н.А., Борисенко О.Д.  Построение требований и архитектуры облачного оркестратора платформенных сервисов. Труды 
ИСП РАН, том 34, вып. 4, 2022 г., стр. 211-228 
215 
Перечисленные свойства позволяют полностью изолировать процесс развертывания 
распределенных приложений от внутреннего устройства отдельных его компонент и крайне 
важны с точки зрения моделирования и оркестрации сложных систем.  
Отдельно стоит отметить устоявшийся процесс разработки и развертывания прикладных 
систем при использовании контейнерных технологий:  
1) независимая разработка отдельных компонентов системы; 
2) написание файла, описывающего процесс создания образа для отдельных компонентов 
системы: Containerfile или Dockerfile; 
3) создание и загрузка образа в приватный или публичный репозиторий контейнеров; 
4) описание конфигураций запуска отдельных компонентов при развертывании всей 
системы, а также описание взаимодействия компонентов между собой; 
5) развертывание всей системы; 
6) обновление рабочей системы. 
Для оркестрации контейнерных приложений существует множество инструментов, 
например, Docker Compose, Docker Swarm, Apache Mesos, Kubernetes. При этом выбор 
оркестратора зависит от масштаба системы [12]. 
На сегодняшний день наиболее популярным инструментом для оркестрации больших 
программных систем является Kubernetes. В контексте анализа задач оркестрации стоит 
отметить ключевой функционал данной платформы: 
● 
устойчивое развертывание: формат описания сервисов в Kubernetes позволяет 
декларативно описывать желаемое состояние. Этот механизм позволяет контролируемо 
обновлять конфигурацию приложения и переводить на нее нагрузку только после того, 
как изменения полностью вступили в силу; 
● 
мониторинг: контроль за работой сервисов как на уровне статуса всего контейнера, так 
и за счет выполнения указанных команд внутри контейнера; 
● 
масштабирование: в процессе работы приложения Kubernetes позволяет в зависимости 
от актуальной нагрузки увеличивать или уменьшать количество контейнеров, 
соответствующих каждому сервису: автоматически или при помощи операторов; 
● 
конфигурация хранилища: пользователь может настраивать как локальную файловую 
систему для своих сервисов, так и подключать сторонние хранилища, доступные по сети; 
● 
распределение нагрузки: распределение трафика между контейнерами за счет анализа 
количества запросов по адресу каждого из контейнеров; 
● 
контроль 
за 
конфиденциальной 
информацией: 
обеспечивается 
сохранность 
пользовательских и системных аутентификационных данных; 
С другой стороны, использование контейнерной виртуализации накладывает ограничения на 
работу программ в части взаимодействия с аппаратной частью и хостовой операционной 
системой, а также имеют недостатки с точки зрения обеспечения безопасности. Кроме того, 
предполагаемый функционал работы PaaS-оркестратора шире, чем предполагаются при 
работе с контейнерными оркестраторами. Рассмотрим подробнее, какие типы платформ 
возникают при оркестрации уровня PaaS на основе приведенного обзора задач. 
2.3 Определение функциональности оркестратора 
Базовой функциональностью, предоставляемой разработчикам приложений, является среда 
сборки и выполнения программ. В данный уровень платформ входят компиляторы для 
различных языков программирования, настройка (возможно, виртуальных) окружений, 
обеспечение изоляции ресурсов. Отдельно стоит выделить настройку контейнерной среды 
исполнения, причем на данном уровне имеется в виду непосредственно инструменты для 
запуска контейнеров без дополнительного окружения. Используя такое окружение 
разработчики приложений (в том числе, ориентированных на запуск в контейнерном 
окружении) получают полноценную абстракцию как от физических ресурсов, так и от 
Lazarev N.A., Borisenko O.D. Requirements and architecture design for cloud PaaS orchestrator. Trudy ISP RAN/Proc. ISP RAS, vol. 34, 
issue 4, 2022, pp. 211-228 
216 
операционных систем и необходимости установки дополнительных пакетов, имеют 
стабильное окружение для сборки и запуска собственных проектов. Аналогом данной 
абстракции является раздел FROM при описании образа контейнера в формате Dockerfile. 
Заметим, что обеспечение только этого уровня подразумевает необходимость обеспечения 
непосредственного доступа пользователей к виртуальным машинам с настроенным 
окружением посредством ssh или других способов. 
Другим обязательным уровнем PaaS является обеспечение взаимодействия с различными 
инструментами хранения данных. Помимо подключения (виртуальных) блочных устройств 
к пользовательским виртуальным машинам, может потребоваться создание распределенной 
файловой системы или объектного хранилища, либо монтирование уже существующих 
хранилищ. За счет такой функциональности разработчики получают абстракцию для 
управления данными в различных форматах, сравнимую с подключением томов к 
контейнерам. При этом подразумевается лишь обеспечение доступа пользователей к данным 
инструментам по сети, в то время как вся логика взаимодействия с хранилищем остается вне 
оркестратора. 
Наконец, наиболее сложным уровнем функциональности PaaS является обеспечение работы 
с различными службами, предоставляющими программные интерфейсы по сети. Частым 
примером для данного уровня является развертывание СУБД по запросу, хотя зачастую его 
относят к уровню хранилищ. Другими примерами для данного уровня могут являться: 
инструменты для сбора и обработки логов, мониторинга, веб-разработки, платформы 
параллельного выполнения программ и планировщики использования ресурсов, а также 
платформы оркестрации контейнеров. В отличие от контейнерного окружения, в 
зависимости от платформы, её конфигурации, а также зависимостей могут потребоваться 
дополнительные вычислительные ресурсы, установка дополнительных пакетов, настройка 
других инструментов и другие действия. Тем не менее, такой функционал обеспечивает 
уровень 
абстракции, 
аналогичный 
оркестрации 
контейнеров: 
пользователи 
PaaSоркестратора получают полноценную возможность использовать сторонние платформы при 
разработке собственных инструментов. 
При этом важно заметить, что аналогично уровню хранения данных, при развертывании 
высокоуровневых платформ задача оркестратора состоит в обеспечении доступа 
пользователя к тому или иному инструменту, в то время как любая бизнес-логика требует 
участия пользователя. Например, рассмотрим задачу предоставления реляционной СУБД по 
запросу. В простейшем случае эта задача подразумевает создание виртуальной машины 
достаточной мощности и подключения к ней блочного (виртуального) устройства 
достаточного объема, установки необходимых для запуска СУБД пакетов, настройки 
параметров виртуальной машины и самой СУБД, создания пользователя СУБД, а также 
настройки сетевого файервола для обеспечения доступа. Также, в зависимости от 
конфигурации всей системы, может потребоваться дополнительная настройка на уровне 
исполнения программ. В конечном итоге пользователь получает адрес СУБД, а также 
аутентификационные данные. Однако, в некоторых случаях может потребоваться создание 
связанных кластеров реляционных СУБД, состоящих из нескольких узлов. В этом случае 
необходимо обозначить границы возможностей оркестратора: любые манипуляции, для 
которых необходима информация о схеме базы данных может быть настроена только самим 
пользователем. Так, оркестратор может настроить кластер PostgreSQL с полной репликацией 
данных, однако для распределения данных между узлами необходимо указывать параметр 
распределения в каждой таблице базы данных, поэтому без участия пользователя это сделать 
невозможно. 
Еще одним важным слоем является обеспечение масштабируемости распределенных систем. 
Использование распределенных систем может диктоваться как требованиями по 
соблюдению того или иного уровня соглашения о предоставлении услуг (SLA), 
требованиями к качеству и отзывчивости систем (QoS), а также тем, что для обеспечения 
Лазарев Н.А., Борисенко О.Д.  Построение требований и архитектуры облачного оркестратора платформенных сервисов. Труды 
ИСП РАН, том 34, вып. 4, 2022 г., стр. 211-228 
217 
работоспособности программы не хватает физических ресурсов самих инфраструктурных 
узлов, и программа в принципе не может быть запущена на одном вычислительном узле. Все 
перечисленные причины к использованию распределенных систем не накладывают 
специфических требований к оркестратору, поскольку SLA, требования QoS относятся 
больше к бизнес-преимуществам компаний-провайдеров облачных услуг, однако явным 
образом добавляют требование на возможность настройки сложных с точки зрения сетевой 
топологии 
программных 
систем, 
которые 
требуют 
корректного 
распределения 
конфигурационных файлов, учитывающего порядок запуска каждого из компонентов и их 
взаимосвязи, а также контроль версий входящих в состав компонентов. 
Контроль версий включает в себя множество вспомогательных задач: от учета версий 
пакетов, устанавливаемых на виртуальные машины, до отслеживания совместимости 
различных интерфейсов между собой. Так, при развертывании Apache Spark может 
потребоваться также настройка других инструментов, например, Apache Hadoop. При этом 
для Apache Spark версии 3.0.0 и выше возможна совместимость только с версиями Apache 
Hadoop не меньше 2.2.0. Кроме того, официальные сборки доступны только для 
ограниченного числа комбинаций версий данных платформ, в то время как при 
необходимости развертывания других потребуется дополнительно обеспечить сборку 
проектов. Также стоит отметить, что для работы данных платформ требуется настройка JVM 
определенной версии. В случае, если пользователю потребуется также JVM другой версии, 
необходима изоляция времени исполнения между платформой и пользовательскими 
программами. Таким образом, при оркестрации требуется поддерживать многоуровневый 
контроль совместимости различных компонентов. 
Таким образом, на основе проведенного анализа, а также с учетом других работ, можно 
сформулировать следующий набор функциональных возможностей PaaS-оркестратора для 
покрытия актуальных задач: 
1) механизм описания предоставляемых услуг, учитывающий уровни: 
a) предоставления среды исполнения; 
b) настройки хранилищ данных; 
c) предоставления программных интерфейсов по сети; 
2) обобщение пакетных менеджеров уровня операционной системы с возможностью 
обновления программного обеспечения и контролем совместимости поставляемых 
пакетов; 
3) управление 
конфигурационными 
файлами 
и 
их 
специализация 
под 
версии 
развертываемых программных систем, в том числе подстановка параметров; 
4) сквозная доставка пользовательских данных; 
5) интеграция 
систем 
мониторинга 
для 
обеспечения 
отказоустойчивости 
и 
масштабирования; 
6) обеспечение доступа к развернутым сервисам посредством ssh или инструментов 
аутентификации в соответствии с уровнем предоставляемых услуг; 
7) обеспечение балансировки нагрузки. 
Кроме того, в зависимости от контекста использования оркестратора требуется обеспечение 
программного или пользовательского интерфейса с разграничением доступа к развернутым 
сервисам и действиям с ними. Также в современных оркестраторах требуется предусмотреть 
возможность мультиоблачного режима работы: в рамках построения гетерогенной 
гибридной облачной среды (при одновременном использовании публичных и приватных 
вычислительных ресурсов), при работе в рамках географически распределенной облачной 
среды, либо для унифицированного использования ресурсов независимых облачных 
провайдеров. 
Lazarev N.A., Borisenko O.D. Requirements and architecture design for cloud PaaS orchestrator. Trudy ISP RAN/Proc. ISP RAS, vol. 34, 
issue 4, 2022, pp. 211-228 
218 
3. Обзор оркестраторов 
В данном разделе рассматриваются существующие решения, относящиеся к оркестрации 
уровня PaaS. Так как основной целью работы является определение архитектуры и 
таксономии 
мультиоблачных 
оркестраторов, 
коммерческие 
разработки 
будут 
рассматриваться только с точки зрения предоставляемого функционала. В свою очередь, 
основной интерес для текущего исследования представляют открытые оркестраторы, 
позволяющие решать задачи, описанные во второй главе. Общее сравнение рассматриваемых 
оркестраторов приведено в табл. 1. 
Табл. 1. Сравнение существующих оркестраторов 
Tabl. 1. Existing orchestrators comparison 
Критерий 
INDIGO-DC yorc 
Cloudify 
Heroku 
ElastiCluster Michman 
Механизм 
описания 
платформ 
TOSCA 
Simple Profile 
v1.0 
TOSCA 
Simple 
Profile v1.2 
DSL на основе 
TOSCA Simple 
Profile v1.3 
Heroku 
Buildpack на 
основе git 
Нет 
Собственный 
язык в 
формате 
JSON 
Управление 
пакетами 
Задается в 
шаблонах 
Задается в 
шаблонах 
Задается в 
шаблонах 
Обработка 
зависимостей 
Задается в 
Ansible 
Подключение 
репозиториев 
Управление 
конфигурациями 
Задается в 
шаблонах 
Задается в 
шаблонах 
Задается в 
шаблонах 
Обработка 
переменных 
окружения 
Задается в 
Ansible 
Задается в 
описании 
сервисов 
Доставка 
пользовательских 
данных 
Задается в 
шаблонах 
Задается в 
шаблонах 
Задается в 
шаблонах 
Нет 
Задается в 
Ansible 
Нет 
Мониторинг 
zabbix 
consul 
Плагины 
Дополнения 
Нет 
consul 
Доступ к 
ресурсам 
Выделенный 
сервис 
 
ssh 
ssh, 
подключение 
к ldap 
ssh 
ssh 
ssh 
Балансировка 
нагрузки 
Нет 
Нет 
Плагины 
Встроенная 
маршрутизация 
Нет 
Нет 
3.1 Формат описания предоставляемых услуг 
В соответствии с предложенными критериями, в первую очередь необходимо рассмотреть 
возможность описания предоставляемых услуг. Хотя для конечных пользователей формат 
описания зачастую скрыт за счет использования какого-либо пользовательского интерфейса, 
используемое внутреннее представление оркестратора определяет как технологические 
возможности оркестратора, так и удобство его интеграции. Сегодня существует множество 
форматов и языков описания распределенных систем, имеющих различные области 
применимости, модель предоставления услуг и назначение[13]. 
Наибольший интерес среди них в рамках этой работы представляют независимые от 
облачной платформы языки и стандарты, позволяющие описать как топологию 
предоставляемых услуг уровня PaaS, так и процесс развертывания таких топологий. К таким 
можно отнести OCCI [14], mOSAIC [15], SOCCA [16], STRATOS [17], и другие, однако 
наиболее современным стандартом для описания топологий в облачной инфраструктуре 
любых уровней является TOSCA [18]. 
Основные сущности в данном стандарте делятся на два уровня: типы узлов и 
взаимоотношений, содержащие информацию о возможных компонентах топологий, а также 
различные шаблоны, которые конкретизируют описанные типы узлов и взаимосвязей и 
Лазарев Н.А., Борисенко О.Д.  Построение требований и архитектуры облачного оркестратора платформенных сервисов. Труды 
ИСП РАН, том 34, вып. 4, 2022 г., стр. 211-228 
219 
описывают их объединение в топологии. TOSCA также определяет функционал 
инструментов, реализующих обработку документов, соответствующих стандарту: 
● 
парсер (Parser): получает отдельные шаблоны и дополнительные “блоки” из одного или 
нескольких репозиториев, производит валидацию и нормализацию шаблонов; 
● 
резолвер (Resolver): применяет входные данные для шаблонов, конвертирует 
нормализованные шаблоны в представление узлов, вызывает встроенные функции, а 
также вычисляет требования для узлов и создает граф взаимоотношений; 
● 
оркестратор 
(Orchestrator): 
непрерывно 
создает 
или 
удаляет 
реализации 
для 
представления узлов, обновляет значения атрибутов узлов, обновляет результаты работы 
Resolver, а также может изменять представления узлов. 
Отметим, что стандарт не ограничивает область применимости, в связи с чем не все TOSCAоркестраторы могут быть использованы для управления ресурсами уровня PaaS. Поэтому, 
хотя многие инструменты заявляют о поддержке или реализации данного стандарта, в данной 
работе будут рассмотрены инструменты, получившие активное применение в научной и 
коммерческой среде. 
● 
Indigo-DC [19] – облачная вычислительная платформа, предназначенная для научных 
сообществ и используемая в рамках европейского проекта Horizon 2020. В частности, в 
Indigo-DC реализуется управление ресурсами уровня PaaS; 
● 
Yorc [20] – инструмент управления жизненным циклом приложений для работы в 
различных облачных платформах, а также в контейнерной среде Kubernetes, 
планировщике Slurm и при подключении к физическим серверам без виртуализации, 
который используется в рамках платформы Lexis [21]; 
● 
Cloudify [22] – мультиоблачная платформа оркестрации, ориентированная на 
автоматизацию развертывания программного обеспечения в процессе разработки. 
Использует собственный язык описания услуг, основанный на TOSCA. 
Стандарт TOSCA позволяет связывать с различными сущностями специальные архивы, 
содержащие необходимые для развертывания скрипты, данные и прочие артефакты. Таким 
образом, настройка развертывания полностью лежит на разработчиках шаблонов топологий, 
в связи с чем снижается гибкость использования TOSCA-оркестраторов. Например, при 
необходимости добавления новой версии некоторого сервиса, пользователю необходимо 
либо самому добавить соответствующие артефакты к существующему описанию, либо 
запросить обновление у автора шаблона. 
К минусам этого стандарта можно также отнести большое количество глав стандарта, 
находящихся в разработке, сложность использования, а также отсутствие соответствия 
описанных топологий конкретным ресурсам в облачной среде. 
В большинстве публичных облачных платформ существуют свои внутренние форматы 
описания предоставляемых услуг, например, шаблоны Azure Resource Manager (ARM 
templates [23]), шаблоны AWS CloudFormation [24], шаблоны Google Cloud [25]. Ключевым 
недостатком такого подхода является непереносимость описаний между различными 
облачными платформами. С другой стороны, такие форматы ориентированы на конкретные 
возможности облачных платформ и позволяют более точно описывать детали развертывания 
требуемых услуг. Такой подход используют также некоторые открытые инструменты 
оркестрации: в то время как стандарт TOSCA ориентирован на описание наиболее широкого 
набора облачных 
услуг, в существующих 
инструментах может предоставляться 
ограниченный функционал, за счет чего снижается сложность описания услуг и 
использования оркестратора. Среди таких оркестраторов можно выделить следующие. 
● 
Heroku [26] – публичная PaaS платформа с открытым исходным кодом, ориентированная 
на настройку среды разработки в мультиоблачной среде на основе контейнерной 
виртуализации. Позволяет предоставлять сценарии развертывания в любых форматах с 
использованием репозиториев git; 
Lazarev N.A., Borisenko O.D. Requirements and architecture design for cloud PaaS orchestrator. Trudy ISP RAN/Proc. ISP RAS, vol. 34, 
issue 4, 2022, pp. 211-228 
220 
● 
ElastiCluster [27] – инструмент командной строки для автоматизации управления 
вычислительными кластерами в облачной среде, используемый в проекте ATLAS [28]. В 
качестве средства описания развертывания систем используется Ansible[29]; 
● 
Michman [30] – PaaS оркестратор с открытым исходным, ориентированный на 
предоставление вычислительных платформ для научных исследований. На данный 
момент используется собственный формат описания предоставляемых услуг в формате 
JSON в совокупности с ролями Ansible. 
3.2 Управление пакетами 
В некоторых случаях может быть необходимо использовать специфичные пакеты при 
развертывании пользовательских платформ. Например, это может потребоваться при 
использовании модифицированных в целях безопасности или изменения функционала 
библиотек. Также, в рамках приватных облачных платформ может возникнуть 
необходимость установки пакетов без доступа в интернет – для этого необходимо 
подключать локальные для этой платформы репозитории, либо зеркала. Также как при 
оркестрации контейнерных окружений, в рамках PaaS-оркестратора необходимо обеспечить 
подключение пользовательских репозиториев. 
Коммерческие 
публичные 
решения 
зачастую 
скрывают 
детали 
развертывания 
предоставляемых услуг, либо используют публичные репозитории без возможности 
подключения собственных репозиториев. С другой стороны, при определении собственного 
шаблона в каждой платформе возможно определение скриптов установки и настройки 
системы, что позволяет при необходимости указать необходимый способ установки пакетов 
при развертывании услуги из этого шаблона. В этом случае пользовательские репозитории 
также должны быть публичными. 
В облачных оркестраторах, основанных на стандарте TOSCA, управление установкой 
пакетов зачастую реализуется разработчиками шаблонов топологий предоставляемых услуг. 
В случае использования оркестратора в рамках приватной облачной платформы, в шаблонах 
могут использоваться репозитории, развернутые в этой платформе. Недостатком такого 
подхода является недостаточный контроль за конфликтами между пакетами. Например, при 
установке двух платформ, использующих JVM различных версий, может потребоваться 
дополнительная настройка среды исполнения, что может приводить к непредвиденным 
ошибкам на этапе развертывания. 
В мультиоблачных PaaS-оркестраторах, использующих собственный формат описания, 
зачастую применяется один из описанных выше подходов в зависимости от того, кем 
описывается процесс развертывания. 
3.3 Управление конфигурационными файлами 
Для большинства служб уровня PaaS требуется указание пользователями параметров, 
применяемых в процессе разворачивания, например, подключаемые плагины, квоты на 
использование вычислительных ресурсов, параметры авторизации и другие. В зависимости 
от используемого представления и пользовательских интерфейсов, оркестраторы могут 
предоставлять гибкую обработку любых параметров, описанных для каждой услуги, либо 
обеспечивать обработку только для заранее определенных параметров. 
3.4 Доставка пользовательских данных 
При разработке приложений, ориентированных на обработку больших объемов данных, 
требуется подключать сторонние системы хранения данных помимо настройки среды 
исполнения. В то время как публичные облачные платформы обеспечивают сквозную 
интеграцию с собственными объектными хранилищами, при работе в приватных облаках 
Лазарев Н.А., Борисенко О.Д.  Построение требований и архитектуры облачного оркестратора платформенных сервисов. Труды 
ИСП РАН, том 34, вып. 4, 2022 г., стр. 211-228 
221 
могут использоваться открытые и закрытые объектные хранилища, подключаемые блочные 
хранилища, а также внешние распределенные файловые системы. 
На текущий момент реализации стандарта TOSCA позволяют явно описывать только 
подключение 
блочных 
устройств 
к 
пользовательским 
виртуальным 
машинам. 
Альтернативно, разработчики шаблонов могут использовать фиктивное определение служб 
для реализации подключения к другим типам хранилищ, однако в этом случае теряется 
основное использование стандарта – независимость от облачных платформ и других 
инструментов. 
В свою очередь, инструменты оркестрации, использующие другие форматы описания 
ресурсов, могут предоставлять сквозную интеграцию с системами хранения данных и 
монтирование внешних файловых систем. 
3.5 Обеспечение отказоустойчивости и масштабирования 
В отличие от контейнерной среды, при работе с виртуальными машинами оркестратор не 
всегда имеет прямой доступ к управляемым ресурсам и требует настройки дополнительных 
систем мониторинга. В зависимости от целей, мониторинг может осуществляться как на 
уровне операционной системы – например, через сбор метрик использования ресурсов 
системы, так и через специализированные запросы к предоставляемым сервисам – через 
сетевые запросы к ним. 
Таким образом, требование получения информации о способах проверки жизнеспособности 
и загруженности ресурсов подразумевают явное или неявное использование систем 
мониторинга, а также способы описания проверок в формате описания. 
В оркестраторах также могут использоваться встроенные системы анализа состояния 
управляемыми ресурсами. 
3.6 Доступ к ресурсам 
Важным аспектом предоставления ресурсов по сети является обеспечение безопасного 
доступа к ним. В случае предоставления платформы для разработки ПО основным способом 
доступа является подключение к виртуальным машинам посредством SSH, поэтому зачастую 
задача сводится к добавлению пользовательских публичных ключей на виртуальные 
машины. 
С другой стороны, при настройке более высокоуровневых платформ требуется обеспечить 
безопасную доставку паролей при инициализации платформ, что особенно критично при 
работе в публичных облачных платформах. 
3.7 Балансировка нагрузки 
Основным 
преимуществом 
облачных 
платформ 
является 
гибкость 
в 
настройке 
вычислительных ресурсов. Однако, при горизонтальном масштабировании методом создания 
идентичных экземпляров служб возникает задача корректного распределения нагрузки 
между экземплярами. Такая функциональность может решаться различными способами: за 
счет использования балансировки на инфраструктурном уровне, с использованием 
специальных встроенных плагинов, а также за счет подключения дополнительных служб в 
разворачиваемых платформах. 
4. Основные компоненты оркестратора 
В этом разделе инструменты оркестрации подробнее рассматриваются с точки зрения 
входящих в их состав служб для выявления общих составляющих, необходимых для работы 
оркестратора. Также здесь приводится обобщение входящих в оркестраторы компонент и 
предлагается унифицированная архитектура. 
Lazarev N.A., Borisenko O.D. Requirements and architecture design for cloud PaaS orchestrator. Trudy ISP RAN/Proc. ISP RAS, vol. 34, 
issue 4, 2022, pp. 211-228 
222 
4.1 Архитектура существующих оркестраторов 
4.1.1 Indigo-DC 
Архитектура Indigo-DC основывается на концепции микросервисов. В состав оркестратора 
входят следующие компоненты: 
● 
сервис оркестрации (Orchestrator Service) – центральный компонент системы, 
обеспечивающий развертывание и управление ресурсами; 
● 
сервис аутентификации (IAM Service) – обеспечивает проверку прав пользователей; 
● 
графический интерфейс (GUI) – обеспечивает доступ пользователей к имеющимся 
шаблонам TOSCA и сервису оркестрации; 
● 
репозиторий шаблонов TOSCA (Repository) – хранит шаблоны TOSCA; 
● 
сервис оркестрации (Orchestrator Service) – центральный компонент системы, 
обеспечивающий развертывание и управление ресурсами; 
● 
инфраструктурный менеджер (Infrastructure Manager) – разворачивает инфраструктурные 
ресурсы в различных облаках; 
● 
сервисы управления данными (Data Management Services) – набор сервисов, 
предоставляющих доступ к хранилищам данных в унифицированном виде. 
● 
служба мониторинга (Monitoring) – обеспечивает сбор метрик из различных облаков и 
передает в унифицированном виде в сервис оркестрации; 
● 
служба обработки политик (Brokering/Policy Service) – обеспечивает контроль за 
политиками использования ресурсов; 
● 
служба управления QoS и SLA (QoS/SLA Service) – обеспечивает контроль за QoS и SLA 
как для отдельных пользователей, так и для всей системы в целом. 
4.1.2 Yorc 
Оркестратор Yorc основан на модели приложения без сохранения состояния с целью 
упрощения горизонтального масштабирования. Основной функционал оркестратора 
реализован в виде одного сервиса, который управляет ходом развертывания ресурсов. К нему 
также быть подключены дополнительные плагины. Для взаимодействия с оркестратором 
реализованы сервис, предоставляющий REST API, и интерфейс командной строки. Также для 
работы с оркестратором может быть использован инструмент Alien4Cloud[31]. 
Кроме того, в Yorc используются сторонние сервисы: Consul[32] для управления сервисами 
и мониторинга и Vault[33] для хранения конфиденциальных данных. 
4.1.3 Cloudify 
Cloudify реализует клиент-серверный подход: основным компонентом является Cloudify 
Manager, который взаимодействует с агентами на управляемых виртуальных машинах. В 
свою очередь, Cloudify Manager включает следующие сервисы: 
● 
REST API; 
● 
база данных; 
● 
брокер сообщений; 
● 
сервис управления развертыванием. 
Помимо использования REST API напрямую, в Cloudify также реализованы графический 
интерфейс и интерфейс командной строки. Также предоставляется доступ к логам и 
мониторингу. 
4.1.4 Heroku 
В основе работы Heroku используется принцип подготовки образов,  готовых к запуску. Для 
взаимодействия с инструментом предоставляется API, CLI, а также могут использоваться 
Лазарев Н.А., Борисенко О.Д.  Построение требований и архитектуры облачного оркестратора платформенных сервисов. Труды 
ИСП РАН, том 34, вып. 4, 2022 г., стр. 211-228 
223 
файлы настройки непосредственно в публичных репозиториях. Кроме того, в состав 
оркестратора входят службы для маршрутизации трафика и других сетевых настроек, 
логирования и мониторинга разворачиваемых сервисов, а также системы аутентификации и 
контроля потребления ресурсов. 
Система развертывания, встроенная в Heroku основана на технологии контейнерной 
виртуализации, однако подразумевает сборку образа самим оркестратором из базовых 
образов операционных систем. Также для сборки и развертывания сервисов в Heroku 
используются службы для: 
● 
учета пакетных зависимостей; 
● 
правил сборки программ; 
● 
настройки переменных окружения; 
● 
подключения внешних служб; 
● 
контроля версий сервисов. 
4.1.5 ElastiCluster 
ElastiCluster является инструментом командной строки, упрощающий использование Ansible 
для 
развертывания 
распределенных 
систем 
в 
облачной 
среде. 
Разработчиками 
предоставляются готовые сценарии для создания кластеров Slurm, Spark и Hadoop. 
Инструмент позволяет настраивать подключение к различным облачным провайдерам 
уровня IaaS, отслеживать текущее состояние и масштабировать развернутые кластеры, 
подключать и настраивать различные распределенные файловые системы. 
4.1.6 Michman 
Michman также использует микросервисную архитектуру, основными компонентами 
которой являются сервисы REST и LAUNCHER. REST обеспечивает доступ пользователей к 
описаниям 
поддерживаемых 
оркестратором 
сервсисов, 
а 
также 
к 
управлению 
разворачиваемым кластерам. В LAUNCHER реализована основная логика управления 
развертывания кластеров. Michman использует Consul для мониторинга, Vault для хранения 
конфиденциальных данных и LogStash для доступа к логам запуска кластеров. Также в 
оркестраторе реализовано подключение пакетных репозиториев для обеспечения режима 
работы без доступа в Интернет. 
4.2 Анализ и обобщение рассмотренных оркестраторов 
Рассмотренные выше оркестраторы используют различные подходы в организации работы, 
так как рассчитаны на разные области применимости: от упрощения ручной настройки 
распределенных вычислительных кластеров до использования в публичных облаках. 
Оркестраторы, основанные на микросервисной архитектуре, могут быть явным образом 
разделены на компоненты, имеющие четкие границы применимости. Для монолитных 
инструментов выделение компонентов предполагает лишь логическое разделение. При этом 
некоторые компоненты находят реализацию в каждом инструменте в том или ином виде. 
Основные компоненты и их взаимодействие представлены на рис.1. 
В первую очередь, в каждом рассмотренном инструменте используется некоторая абстракция 
управления вычислительными ресурсами инфраструктурного уровня, в связи с чем может 
быть выделен соответствующий компонент – IaaS manager. В качестве этого компонента 
может использоваться интерфейс некоторой облачной платформы, сторонний IaaSоркестратор, либо встроенный программный компонент. Также в некоторых случаях в 
качестве альтернативы инфраструктурному уровню может использоваться контейнерная 
среда, либо физические узлы без виртуализации. 
Lazarev N.A., Borisenko O.D. Requirements and architecture design for cloud PaaS orchestrator. Trudy ISP RAN/Proc. ISP RAS, vol. 34, 
issue 4, 2022, pp. 211-228 
224 
 
Рис. 1. Унифицированная архитектура PaaS оркестратора 
Fig. 1. Unified PaaS orchestrator architecture 
В инструментах, основанных на TOSCA, инфраструктурные ресурсы описываются наравне с 
остальными 
компонентами 
топологии 
платформы: 
для 
них 
существуют 
общие 
“нормативные” типы, которые могут быть уточнены для конкретного облачного провайдера, 
либо могут быть использованы собственные типы. Однако на сегодняшний день во всех 
рассматриваемых оркестраторах процесс создания экземпляров таких узлов реализован при 
помощи дополнительных типов узлов. Рассмотрим данный процесс на примере создания 
виртуальной 
машины, 
для 
которой 
в 
стандарте 
существует 
нормативный 
тип 
tosca.Nodes.Compute. В каждом оркестраторе созданы собственные типы узлов, выведенные 
из 
нормативного: 
“yorc.nodes.openstack.Compute”, 
“tosca.nodes.indigo.Compute”, 
“cloudify.nodes.Compute”. Далее в каждом из инструментов реализованы обработчики этих 
типов в виде функций или подключаемых плагинов для каждого из поддерживаемых 
облачных сред. 
Оркестратор Heroku в большей степени ориентирован на работу в контейнерной среде, 
однако поддерживает работу с образами операционных систем для дальнейшей настройки на 
них платформ. ElastiCluster позволяет настраивать подключение к различным провайдерам, 
указывать образы и размеры виртуальных машин, которые будут в дальнейшем 
использоваться. Наконец, в Michman на текущий момент также настраивается подключение 
к облачной платформе при конфигурации оркестратора и выставляются параметры для 
создания виртуальных машин при создании кластеров. Далее в каждом оркестраторе 
реализовано создание по заданным параметрам вычислительных ресурсов, однако для 
пользователя эта информация скрыта, вместо этого пользователь оперирует конечными 
платформами. 
Лазарев Н.А., Борисенко О.Д.  Построение требований и архитектуры облачного оркестратора платформенных сервисов. Труды 
ИСП РАН, том 34, вып. 4, 2022 г., стр. 211-228 
225 
Для обеспечения работы пользователя в рассмотренных инструментах предлагаются 
различные пользовательские интерфейсы – UI. В зависимости от области применимости 
оркестратора могут быть реализованы: графический интерфейс (GUI), интерфейс командной 
строки (CLI), а также может быть предоставлен  REST API для программного использования. 
Кроме того, в ходе работы с  UI в некоторых инструментах подразумевается также 
обеспечение аутентификации и доступа к описаниям поддерживаемых сервисов. 
Также в большинстве инструментов в том или ином виде реализован доступ пользователей к 
службам мониторинга, необходимым для масштабирования и отказоустойчивости 
разворачиваемых сервисов. В общем случае для этого может быть выделен компонент 
Monitoring. 
Для отслеживания процесса запуска разворачиваемых сервисов и получения дополнительной 
информации в случаях ошибок в оркестраторах обеспечивается доступ к логам. В 
большинстве рассмотренных инструментов хранение логов разделяется на два случая: 
1) для 
доступа 
к 
логам 
развернутых 
сервисов 
предполагается 
развертывание 
дополнительных служб, либо предоставляется доступ к виртуальным машинам с 
сервисами.  
2) информация о работе самого оркестратора и процессе запуска сервисов записывается 
локально, либо для этого явным образом выделен отдельный компонент – Log Service. 
При развертывании приложений требуется настройка пакетов уровня операционной 
системы. В общем случае это может подразумевать две задачи:  
1) Обобщение различных пакетных менеджеров (apt, yum, pip и другие) и разрешение 
конфликтов при установке пакетов, необходимых для различных сервисов; 
2) Подключение к локальным пакетным репозиториям для установки пакетов без доступа в 
Интернет, а также для использования приватных пакетов. 
В 
большинстве 
рассмотренных 
оркестраторов 
решение 
этих 
задач 
полностью 
контролируется разработчиками шаблонов и ролей разворачиваемых сервисов. С другой 
стороны, Heroku для подготовки образа к развертыванию описываются используемые в 
сервисе пакетные зависимости, доступные из публичных репозиториев. В оркестраторе 
Michman доступно подключение дополнительных либо локальных репозиториев. В общем 
случае оркестраторе для этих целей могут быть выделены компоненты для контроля 
пакетных зависимостей – Package Manager, и контроля репозиториев – Repository Manager. 
Для запуска пользовательских сервисов необходима настройка конфигурационных файлов и 
переменных окружения. На данный момент явным образом этот уровень не выделен ни в 
одном инструменте оркестрации, хотя в Heroku используется механизм описания 
переменных окружения, доступных при развертывании приложений, а в Michman предложен 
механизм параметризации сервисов. В оркестраторах, основанных на TOSCA, обработка 
параметров запуска сервисов также возможна при создании шаблонов. Для подстановки 
пользовательских и системных значений в конфигурационные файлы может использоваться 
прямая модификация файлов, а также шаблоны файлов, например, в формате Jinja[34]. 
Заметим, что при совместном использовании нескольких сервисов могут быть 
модифицированы одни и те же конфигурационные файлы, что может приводить к 
конфликтам. Для корректной обработки пользовательских параметров и изоляции окружения 
различных сервисов предлагается выделить дополнительного компонента оркестратора – 
Config Manager. 
Следующим важным аспектом работы оркестратора является хранение и обработка 
пользовательских данных: как необходимых для работы платформы, так и ключей и паролей 
для доступа к ресурсам. В связи с этим возникает необходимость добавления 
соответствующих компонентов: Data Manager и Secure Data Storage. В то время как основная 
задача Secure Data Storage – обеспечение безопасного хранения небольших объемов данных, 
Data Manager необходим для обеспечения доставки пользовательских архивов и 
Lazarev N.A., Borisenko O.D. Requirements and architecture design for cloud PaaS orchestrator. Trudy ISP RAN/Proc. ISP RAS, vol. 34, 
issue 4, 2022, pp. 211-228 
226 
подключения внешних хранилищ и файловых систем. Для подключения внешних источников 
данных в Indigo-DC явным образом выделена служба обработки данных, в ElastiCluster 
реализовано подключение распределенных файловых систем, а в Heroku доступны 
дополнения. В свою очередь, в оркестраторах Yorc и Michman используется Vault – 
инструмент для безопасного хранения паролей, сертификатов, ключей доступа и других 
конфиденциальных данных. 
Наконец, с повышением сложности управляемых оркестраторами сервисов между ними 
возникают зависимости разного рода. Кроме того, отдельные операции в процессе 
развертывания платформы требуют значительного времени. В связи с этим в большинстве 
оркестраторов явным образом выделены управляющие компоненты, которые могут быть 
обозначены в общем случае Controller и Task Executor. В то время как Controller необходим 
для координации всех компонентов оркестратора и обновления текущего статуса всех 
развернутых сервисов, Task Executor выполняет функции непосредственного выполнения 
отдельных команд на виртуальных машинах. При этом выполнение команд может 
выполняться в асинхронном режиме. 
Отметим, что выделенные компоненты являются логическими составляющими, а конечная 
реализация может иметь как монолитную архитектуру, так и распределенную. Кроме того, 
предложенная архитектура может иметь дополнения в зависимости от формата 
использования. Например, в коммерческом использовании необходимо добавление 
компонентов, отвечающих за поддержку SLA, аудит пользовательских действий, а при 
работе в мультиоблачном режиме оркестратор должен поддерживать технологию единого 
входа (SSO). Однако в текущей работе основное внимание уделяется специфике организации 
развертывания распределенных платформ, в то время как такие дополнительные компоненты 
нацелены на обеспечение работы с пользователями. 
5. Заключение 
Современные учебные, научные и коммерческие задачи требуют не только больших 
вычислительных мощностей, но и сложного распределенного программного обеспечения. 
Платформы обработки больших данных могут требовать до тысяч вычислительных узлов, 
что делает невозможным ручное управление такими системами. В статье рассмотрены 
актуальные задачи, для которых используются облачные вычисления и современные способы 
описания распределенных платформенных сервисов. Исходя из этих задач, сформулирован 
набор требований к функциональности PaaS-оркетраторов. 
На основе проведенного обзора существующих оркестраторов, предложена обобщенная 
архитектурная модель PaaS-оркестратора. Описанная архитектура позволяет реализовать 
развертывание распределенных вычислительных систем в устоявшемся при использовании 
контейнерной виртуализации формате: 
● 
разработчики отдельных сервисов аналогично написанию Containerfile предоставляют 
информацию о том, как развертывать их программное обеспечение, включающую 
зависимости, требования к ресурсам, предоставляемые интерфейсы и скрипты 
развертывания; 
● 
разработчики распределенных систем могут объединять доступные сервисы в 
распределенные топологии, предоставляя готовые к запуску шаблоны, аналогичные, 
например, декларативным описаниям сервисов в Kubernetes; 
● 
конечные пользователи могут использовать готовые шаблоны, уточнять параметры в 
шаблонах, настраивать параметры масштабируемости отдельных компонент, а также 
управлять развернутыми сервисами в процессе работы. 
Дальнейшая работа в этой области требует развития предложенной модели и построения 
полноценной таксономии облачных вычислений уровня PaaS. Также планируется 
применение предложенной архитектуры в оркестраторе Michman. 
Лазарев Н.А., Борисенко О.Д.  Построение требований и архитектуры облачного оркестратора платформенных сервисов. Труды 
ИСП РАН, том 34, вып. 4, 2022 г., стр. 211-228 
227 
Кроме того, помимо предложенного набора требований к PaaS-оркестратору требуется 
разработка модельной задачи, которая могла бы позволить объективно оценивать такие 
инструменты. 

 
 
© ¿. À. Ã‡рı‡Í¯ËÌÓ‚, ¿. ¿. “ÓÌıÓÌÓÂ‚‡, ≈. —. ”рÏ‡Í¯ËÌÓ‚‡, 2019 
 
 
ISSN 1818-7900 (Print). ISSN 2410-0420 (Online) 
¬ÂÒÚÌËÍ Õ√”. –ÂрËˇ: »ÌÙÓрÏ‡ˆËÓÌÌ˚Â ÚÂıÌÓÎÓ„ËË. 2019. “ÓÏ 17, № 4 
Vestnik NSU. Series: Information Technologies, 2019, vol. 17, no. 4 
 
 
. 
 
 
 
 
 
УДК 004.75 
DOI 10.25205/1818-7900-2019-17-4-5-66-73 
 
Разработка бессерверных мобильных приложений 
 
А. Л. Мархакшинов, А. А. Тонхоноева, Е. Р. Урмакшинова 
 
Бурятский государственный университет им. Доржи Банзарова 
Улан-Удэ, Россия 
 
Аннотация 
Описаны принципы создания мобильных приложений, не требующих написания программного кода серверной части. Базу данных, функции аутентификации пользователей и отправки уведомлений предлагается реализовывать по модели BaaS (Backend as a Service – «бэкенд как услуга»). В качестве примера продемонстрирована архитектура бессерверного мобильного приложения, использующего облачные сервисы Google 
Firebase. 
Ключевые слова 
мобильные приложения, бессерверная архитектура, облачные сервисы, разработка приложений 
Для цитирования 
Мархакшинов А. Л., Тонхоноева А. А., Урмакшинова Е. Р. Разработка бессерверных мобильных приложений // 
Вестник НГУ. Серия: Информационные технологии. 2019. Т. 17, № 4. С. 66–73. DOI 10.25205/1818-7900-201917-4-66-73 
 
 
Serverless Mobile Applications Development 
 
A. L. Markhakshinov, A. A. Tonkhonoeva, E. R. Urmakshinova 
 
Banzarov Buryat State University 
Ulan-Ude, Russian Federation 
 
Annotation 
Principles of mobile applications development, requiring no server code, are described. Database, user authentication 
and notification sending functions are suggested to be implemented by BaaS model (Backend as a Service). Example 
architecture of serverless mobile application using Google Firebase cloud services is shown. Brief description of basic 
interactions between serverless architecture components is given. 
Keywords 
mobile applications, serverless architecture, cloud services, application development 
For citation 
Markhakshinov A. L., Tonkhonoeva A. A., Urmakshinova E. R. Serverless Mobile Applications Development. 
Vestnik NSU. Series: Information Technologies, 2019, vol. 17, no. 4, p. 66–73. (in Russ.) DOI 10.25205/1818-79002019-17-4-66-73 
 
 
 
Введение 
 
В настоящее время неотъемлемой частью практически любого мобильного приложения 
стали функции предоставления пользователям индивидуальных учетных записей, хранения и 
обработки информации с помощью единой базы данных, рассылки Push-уведомлений для 
оповещения о различных событиях. Как правило, реализация этих функций подразумевает 
—‡Áр‡·ÓÚÍ‡ ·ÂÒÒÂр‚ÂрÌ˚ı ÏÓ·ËÎ¸Ì˚ı ÔрËÎÓÊÂÌËÈ                      67 
 
 
 
ISSN 1818-7900 (Print). ISSN 2410-0420 (Online) 
¬ÂÒÚÌËÍ Õ√”. –ÂрËˇ: »ÌÙÓрÏ‡ˆËÓÌÌ˚Â ÚÂıÌÓÎÓ„ËË. 2019. “ÓÏ 17, № 4 
Vestnik NSU. Series: Information Technologies, 2019, vol. 17, no. 4 
 
 
. 
 
 
написание дополнительного программного кода серверной части приложения (в современной 
разработке ПО часто называемой backend-частью приложения или просто backend’ом), помимо непосредственно мобильного клиента. 
Кроме описанных, по сути, базовых серверных функций, разработчики часто стремятся 
встраивать функции аналитики и сбора статистики для дальнейшего улучшения процесса 
взаимодействия пользователей с приложением [1]. 
Очевидно, что сокращение затрат на создание backend-части, или даже полный ее перенос 
в готовые облачные сервисы позволит существенно сократить сроки выпуска приложения 
и/или освободить ресурсы для работы над клиентской частью приложения, что особенно актуально для небольших команд разработчиков [2; 3]. 
К требованиям, предъявляемым к сервисам BaaS, можно отнести [4]: 
 независимость от конкретной платформы клиентского приложения; 
 масштабируемость предлагаемых решений; 
 наличие гибкого тарифного плана. 
Наиболее известными поставщиками BaaS-услуг на сегодняшний день являются Google 
Firebase, Microsoft Azure, AWS Lambda, IBM Cloud Functions 1. Далее в статье рассматривается бессерверная архитектура приложения, основанная на использовании сервисов Google 
Firebase. Выбор поставщика обосновывается наличием наиболее полного набора backendуслуг: аутентификация пользователей, облачное файловое хранилище, база данных, облачные функции, аналитика сбоев приложения и его производительности, сервис тестирования 
приложений, рассылка уведомлений и др. Особый интерес представляет база данных Cloud 
Firestore, имеющая возможность автоматической синхронизации информации на подключенных клиентских устройствах. 
 
Архитектура бессерверного мобильного приложения 
 
На рис. 1 показана диаграмма возможных взаимодействий в бессерверном мобильном 
приложении, построенном на базе сервисов Google Firebase. В приложениях, реализованных 
по одному из наиболее распространенных паттернов MVC, MVP или MVVM, обработка бизнес-логики выносится в отдельный слой, называемый слоем модели. Модель отвечает за различные операции над данными приложения, которые затем некоторым образом, зависящим 
от выбранного паттерна, визуализируются в пользовательском интерфейсе [5]. 
В архитектуре, использующей BaaS-подход, взаимодействия с локальной базой данных на 
устройстве клиента, отмеченные на рис. 1 цифрами 1 и 2, фактически могут понадобиться 
лишь для кэширования данных с целью предоставления возможности работы с приложением 
в период отсутствия подключения к сети Интернет. С учетом того, что сервис Cloud Firestore 
также поддерживает функцию кэширования часто используемых данных, возможен вариант 
архитектуры без локальной базы данных. 
Множество современных приложений предлагает своим пользователям персонализированный контент, используя для этого систему учетных записей для идентификации и различные методы аутентификации для подтверждения достоверности учетных записей. Сервис 
Firebase Authentication предлагает готовое решение для создания учетных записей на основе 
адреса электронной почты или номера телефона. Кроме того, существует возможность использовать учетные записи некоторых других популярных сайтов (Google, Facebook, Twitter, 
Github). 
В процессе аутентификации (3) введенные пользователем логин и пароль пересылаются 
по защищенному соединению в облачный сервис Firebase, где выполняется их сверка с базой 
существующих учетных записей приложения. Результат запроса на аутентификацию (4) воз                                                            
1 Comparing Serverless Architecture Providers: AWS, Azure, Google, IBM, and Other FaaS Vendors. URL: 
https://dzone.com/articles/comparing-serverless-architecture-providers-aws-az 
68  
¿. À. Ã‡рı‡Í¯ËÌÓ‚, ¿. ¿. “ÓÌıÓÌÓÂ‚‡, ≈. —. ”рÏ‡Í¯ËÌÓ‚‡ 
 
 
 
ISSN 1818-7900 (Print). ISSN 2410-0420 (Online) 
¬ÂÒÚÌËÍ Õ√”. –ÂрËˇ: »ÌÙÓрÏ‡ˆËÓÌÌ˚Â ÚÂıÌÓÎÓ„ËË. 2019. “ÓÏ 17, № 4 
Vestnik NSU. Series: Information Technologies, 2019, vol. 17, no. 4 
 
 
. 
 
 
вращается в клиент-приложения. При положительном результате становится доступен уникальный идентификационный номер пользователя и данные его учетной записи. Эти сведения затем могут использоваться для обращений к базе данных и получению информации, 
актуальной для конкретного пользователя. 
 
 
 
Рис. 1. Пример архитектуры бессерверного мобильного приложения 
Fig. 1. Example of serverless mobile application architecture 
 
 
В качестве базы данных в предложенной архитектуре используется сервис Cloud Firestore, 
представляющий собой облачную NoSQL-базу данных. В Firestore данные имеют гибкую 
структуру и хранятся в виде документов, которые индексируются по умолчанию и организовываются в коллекции и субколлекции. Преимуществами Firestore, по заявлению разработчика, являются система запросов с удобной фильтрацией и сортировкой результатов, синхронизация с клиентскими устройствами в реальном времени, поддержка офлайн-режима 
работы с базой данных и легкая масштабируемость. 
Обращения к Firestore (5) делятся на два типа: однократный запрос данных и подписка на 
события изменения данных в определенном узле базы. Результатом обращения (6) в обоих 
случаях может являться как отдельный документ, так и группа документов, отобранная и отсортированная в соответствии с параметрами запроса. При однократном запросе существует 
возможность выбрать источник данных: только облачная база данных, только локальный кэш 
или облачная база с возвратом результата из кэша в случае недоступности базы данных. При 
подписке на изменения в определенном документе, коллекции или группе коллекций сервис 
Firestore в автоматическом режиме отслеживает события изменения, удаления или создания 
данных, присылая на клиент обновленную информацию. 
Для работы с сервисами Firebase Authentication и Cloud Firestore доступен API для наиболее популярных языков программирования (Java, Swift, Objective-C, Kotlin, PHP, Python, Go, 
C#, Ruby), а также REST API. 
Сервис Cloud Functions позволяет автоматически запускать серверный код по триггеру 
или по HTTPS-запросу. Код функций пишется на языке JavaScript и исполняется на облачных серверах Google, избавляя разработчиков от необходимости запускать и поддерживать 
собственные серверы. 
Триггерами для Cloud Functions являются различные события сервисов Firebase. Например, для Firebase Authentication триггерами (7) являются события создания и удаления учетных записей. Для Cloud Firestore – события (8) создания, изменения и удаления документов  
в базе данных. Функции, вызываемые из Cloud Functions, могут использоваться для обслужи—‡Áр‡·ÓÚÍ‡ ·ÂÒÒÂр‚ÂрÌ˚ı ÏÓ·ËÎ¸Ì˚ı ÔрËÎÓÊÂÌËÈ                      69 
 
 
 
ISSN 1818-7900 (Print). ISSN 2410-0420 (Online) 
¬ÂÒÚÌËÍ Õ√”. –ÂрËˇ: »ÌÙÓрÏ‡ˆËÓÌÌ˚Â ÚÂıÌÓÎÓ„ËË. 2019. “ÓÏ 17, № 4 
Vestnik NSU. Series: Information Technologies, 2019, vol. 17, no. 4 
 
 
. 
 
 
вания базы данных Firestore (9), либо для взаимодействия с клиентскими приложениями 
пользователей (10). 
 
Бессерверная архитектура на примере Android-приложения 
 
Коротко продемонстрируем практическое использование сервисов Google Firebase на 
примере простого приложения, позволяющего пользователям обмениваться сообщениями в 
реальном времени в тематических каналах. 
Для того чтобы начать пользоваться сервисами Firebase в своем проекте, необходимо 
пройти процедуру регистрации в консоли Firebase2. После этого можно добавлять в приложение требующиеся сервисы. Начнем с аутентификации пользователей с помощью Firebase 
Authentication. В зависимости Android-проекта следует добавить соответствующую библиотеку: 
 
implementation 'com.google.firebase:firebase-auth:17.0.0' 
 
Доступ к услугам авторизации пользователей в программном коде предоставляется через 
экземпляр объекта FirebaseAuth: 
 
private FirebaseAuth mAuth; 
 
Данный экземпляр необходимо инициализировать в методе onCreate() стартовой активности приложения: 
 
mAuth = FirebaseAuth.getInstance(); 
 
В методе onStart() можно получить сведения о текущем пользователе: 
 
FirebaseUser currentUser = mAuth.getCurrentUser(); 
 
Если объект currentUser равен null, то пользователь не авторизовался в приложении. 
В противном случае данный объект может предоставить такие сведения о пользователе, как 
имя, электронный адрес, ссылку на фото профиля, уникальный идентификатор. 
Вызывая у объекта mAuth методы createUserWithEmailAndPassword и signInWithEmailAndPassword, можно выполнять операции создания новой учетной записи и 
входа в приложение. 
Сервис облачной базы данных Cloud Firestore добавляется в проект путем включения следующей зависимости: 
 
implementation 'com.google.firebase:firebase-firestore:19.0.1' 
 
Как упоминалось выше, Cloud Firestore является NoSQL-базой, в которой данные не имеют жестко заданной структуры, хранятся в виде документов и организовываются в коллекции или субколлекции. Информация в документах представляется согласно схеме JSON, т.е. 
в виде наборов пар «ключ-значение». 
В качестве примера на рис. 2 изображена структура базы данных Firestore для приложения, позволяющего пользователям обмениваться сообщениями в отдельных групповых каналах. 
                                                            
2 Firebase console. URL: https:// console.firebase.google.com 
70  
¿. À. Ã‡рı‡Í¯ËÌÓ‚, ¿. ¿. “ÓÌıÓÌÓÂ‚‡, ≈. —. ”рÏ‡Í¯ËÌÓ‚‡ 
 
 
 
ISSN 1818-7900 (Print). ISSN 2410-0420 (Online) 
¬ÂÒÚÌËÍ Õ√”. –ÂрËˇ: »ÌÙÓрÏ‡ˆËÓÌÌ˚Â ÚÂıÌÓÎÓ„ËË. 2019. “ÓÏ 17, № 4 
Vestnik NSU. Series: Information Technologies, 2019, vol. 17, no. 4 
 
 
. 
 
 
Документ является базовой единицей хранения информации в Firestore, и содержит в себе 
набор ключей и значений, причем структура набора может быть произвольной. Название документа должно быть уникальным в пределах его коллекции, а ограничение на максимальный объем документа – 1 МБ. Коллекции фактически представляют собой контейнеры для 
группировки документов. Иерархия данных в базе Firestore обеспечивается за счет привязки 
субколлекций к определенному документу. 
 
 
 
Рис. 2. Хранение данных в базе Cloud Firestore 
Fig. 2. Data storing in Cloud Firestore database 
 
 
В программном коде работа с базой данных осуществляется путем обращения к экземпляру класса FirebaseFirestore: 
 
FirebaseFirestore db = FirebaseFirestore.getInstance(); 
 
Добавление документа происходит с помощью создания структуры Map с соответствующими полями и ее передачи в метод add(): 
 
Map<String, Object> user = new HashMap<>(); 
user.put("name", "Михаил"); 
user.put("lastName", "Иванов"); 
db.collection("users").add(user); 
 
—‡Áр‡·ÓÚÍ‡ ·ÂÒÒÂр‚ÂрÌ˚ı ÏÓ·ËÎ¸Ì˚ı ÔрËÎÓÊÂÌËÈ                      71 
 
 
 
ISSN 1818-7900 (Print). ISSN 2410-0420 (Online) 
¬ÂÒÚÌËÍ Õ√”. –ÂрËˇ: »ÌÙÓрÏ‡ˆËÓÌÌ˚Â ÚÂıÌÓÎÓ„ËË. 2019. “ÓÏ 17, № 4 
Vestnik NSU. Series: Information Technologies, 2019, vol. 17, no. 4 
 
 
. 
 
 
Запрос документа или документов выполняется с помощью метода get(), которому могут предшествовать методы отбора результатов: 
 
//Запрос всех сообщений пользователя с идентификатором m_ivanov 
//в канале с идентификатором channelA 
db.collection("channels").document("channelA") 
.collection("messages").whereEqualTo("from", "m_ivanov") 
.get(); 
 
Для получения обновлений в реальном времени, например, для автоматического отображения новых сообщений в интерфейсе пользователя, Firestore позволяет подписаться на события коллекций, субколлекций или документов. Для осуществления подписки необходимо 
создать ссылку на нужный узел базы данных и реализовать слушатель изменений, который 
будет обрабатывать поступающие данные, а также потенциальные ошибки: 
 
final CollectionReference chRef = db.collection("channels") 
                .document("channelA").collection("messages"); 
chRef.addSnapshotListener(new EventListener<DocumentSnapshot>() { 
    @Override 
    public void onEvent(@Nullable DocumentSnapshot snapshot, 
                        @Nullable FirebaseFirestoreException e) { 
        if (e != null) { 
            //Обработать ошибку 
            return; 
        } 
        if (snapshot != null && snapshot.exists()) { 
            //Извлечь и отобразить новые сообщения 
        } else { 
            //Новых сообщений не было; 
        } 
    } 
}); 
 
Сервис Cloud Functions предназначен для выполнения программного кода на серверной 
стороне и, в отличие от Firebase Authentication и Cloud Firestore, взаимодействие с которыми 
происходит в коде клиентского приложения, предполагает написание отдельных функций на 
языке JavaScript. После предварительной настройки окружения на компьютере разработчика, 
включающей в себя установку платформы Node.js и плагина Firebase CLI для взаимодействия с Firebase через командную строку, можно создавать JavaScript-функции, которые будут 
запускаться в облаке по HTTPS-запросу либо в ответ на события сервисов Firebase. 
Ниже приведен пример функции makeUpperCaseMessage, которая автоматически исполняется каждый раз при поступлении нового сообщения в базу данных Firestore и изменяет 
регистр букв в тексте сообщения на прописной: 
 
exports.makeUpperCaseMessage = functions.firestore 
        .document('channels/{channelId}/messages/{messageId}') 
        .onCreate((snap, context) => { 
    //Получить объект документа, т.е. сообщение 
    const newMessage = snap.data(); 
    //Получить текст сообщения и изменить регистр букв 
    const text = newMessage.text; 
    const upperCaseText = text.toUpperCase(); 
 
72  
¿. À. Ã‡рı‡Í¯ËÌÓ‚, ¿. ¿. “ÓÌıÓÌÓÂ‚‡, ≈. —. ”рÏ‡Í¯ËÌÓ‚‡ 
 
 
 
ISSN 1818-7900 (Print). ISSN 2410-0420 (Online) 
¬ÂÒÚÌËÍ Õ√”. –ÂрËˇ: »ÌÙÓрÏ‡ˆËÓÌÌ˚Â ÚÂıÌÓÎÓ„ËË. 2019. “ÓÏ 17, № 4 
Vestnik NSU. Series: Information Technologies, 2019, vol. 17, no. 4 
 
 
. 
 
 
    //Применить изменения 
    return snap.ref.set({text: upperCaseText}, {merge: true}); 
}); 
 
Заключение 
 
Рассмотренный подход к построению серверной части мобильных приложений все чаще 
применяется на практике. Значительное снижение затрат на разработку ускоряет создание 
как прототипов приложений, так и полноценных продуктов, готовых к коммерческому запуску. Стоит отметить, что нужды приложений с небольшой пользовательской базой и относительно редкими сетевыми взаимодействиями могут быть удовлетворены даже полностью 
бесплатными тарифами описанных облачных сервисов [6]. 
Построенная по принципу BaaS серверная часть может использоваться без каких-либо 
изменений не только в мобильной разработке, но и в настольных и веб-приложениях [7; 8], 
являясь полноценным «бэкендом», не зависящим от конкретного типа клиентских платформ. 
 

УДК 004.031.43 
  DOI: 10.14529/cmse200303 
ОБЗОР ТЕХНОЛОГИЙ ОРГАНИЗАЦИИ  
ТУМАННЫХ ВЫЧИСЛЕНИЙ 
 
© 2020 А.А. Кирсанова1, Г.И. Радченко1, А.Н. Черных1,2 
1Южно-Уральский государственный университет 
(454080 Челябинск, пр. им. В.И. Ленина, д. 76), 
2Научно-исследовательский центр Энсенады 
(22860 Энсенада, Мексика, Carretera Ensenada - Tijuana No. 3918) 
E-mail: alexander.a.kirsanov@susu.ru, gleb.radchenko@susu.ru, chernykh@cicese.mx 
Поступила в редакцию: 14.06.2020 
 
Поскольку Интернет вещей (IoT) становится частью нашей повседневной жизни, наблюдается быстрый 
рост числа подключенных устройств. Устоявшийся подход, основанный на технологиях облачных вычислений, не может обеспечить необходимое качество обслуживания в таких условиях, в частности, в вопросах 
уменьшения времени задержки при передаче данных. Технология туманных вычислений сегодня рассматриваются как многообещающее решение для обработки большого объема критически важных и чувствительных ко времени данных. В этой статье рассмотрена технология облачных вычислений, а также анализируются предпосылки к эволюционному развитию этого подхода и появлению концепции туманных вычислений. В рамках обзора ключевых особенностей туманных вычислений произведен разбор часто встречающейся путаницы с объединением понятий туманных и краевых вычислений. Приведен обзор технологий 
организации туманных вычислений: виртуализация, контейнеризация и оркестрация, а также систематический анализ наиболее популярных платформ, обеспечивающих поддержку туманных вычислений. В результате анализа нами предлагается два подхода к классификации платформ туманных вычислений: по принципу открытости/закрытости компонентов, а также трехуровневая классификация на основе предоставляемого функционала платформы (Deploy-, Platform- и Ecosystem as a Service).  
Ключевые слова: облачные вычисления, туманные вычисления, краевые вычисления, интернет вещей. 
ОБРАЗЕЦ ЦИТИРОВАНИЯ 
Кирсанова А.А., Радченко Г.И., Черных А.Н. Обзор технологий организации туманных вычислений // Вестник ЮУрГУ. Серия: Вычислительная математика и информатика. 2020. Т. 9, № 3. С. 35–63. DOI: 10.14529/cmse200303. 
Введение 
В нынешнюю эпоху данные являются основным товаром, а наличие большего количества данных и возможность их эффективного интеллектуального анализа создает 
большу́ю ценность для предприятий, управляемых на основе данных [26]. По данным 
Международной корпорации данных (IDC), объем генерируемых цифровых данных в 
2010 году превысил 1 зеттабайт [60]. Кроме того, с 2012 года ежедневно генерируется 
2,5 эксабайта новых данных [49]. По оценкам Cisco, к 2020 году будет около 50 миллиардов подключенных устройств [16]. Эти подключенные устройства составляют Интернет вещей (англ. Internet of Things — IoT) и генерируют огромное количество данных в 
реальном времени. Современные архитектуры мобильных сетей уже сейчас планируются 
с учетом тех нагрузок, которые возникают при передаче и обработке таких астрономических объемов данных.  
В текущих реализациях облачных приложений большая часть данных, требующих 
хранения, анализа и принятия решений, отправляется в центры обработки данных в облаке [59]. По мере увеличения объема данных, перемещение информации между IoTустройством и облаком может быть неэффективным или даже невозможным в некоторых случаях из-за ограничений пропускной способности или требований к латентности 
2020, т. 9, №3
35
вычислительной сети. По мере появления приложений, чувствительных ко времени отклика (таких как мониторинг пациентов, автомобили с автоматическим управлением и 
другое), удаленное облако не сможет удовлетворить потребность этих приложений в 
обеспечении сверхнадежной связи с минимальной задержкой [81]. Более того, в некоторых приложениях отправка данных в облако может оказаться невозможной из-за проблем конфиденциальности. 
Для решения проблем приложений, требующих высокой пропускной способности 
вычислительной сети, возможности работы с географически рассредоточенными источниками данных, сверхнизкими задержками и обеспечением локальности обработки данных существует типичная потребность в вычислительной парадигме, которая обеспечивала бы универсальный подход к организации вычислений как в облаке, так и на базе 
вычислительных узлов ближе к подключенным устройствам. Концепция туманных вычислений  (англ. Fog computing) была предложена индустрией и научным сообществом 
для устранения разрыва между облаком и устройствами IoT путем предоставления вычислительных возможностей, хранения, организации сетевого взаимодействия и управления данными на узлах сети, расположенных в непосредственной близости от 
устройств IoT [4, 54]. Исследовательское сообщество предложило ряд подобных вычислительных парадигм для решения упомянутых проблем, таких как краевые вычисления 
(англ. Edge computing), мглистые вычисления (англ. Mist computing), росистые вычисления (англ. Dew computing) и другие. В этом обзоре мы рассматриваем туманные вычисления и технологии их обеспечения и утверждаем, что туманные вычисления являются более общей формой вычислений, главным образом, из-за их всеобъемлющей области определения и гибкости. Также, мы представляем анализ наиболее популярных 
платформ, обеспечивающих поддержку туманных вычислений, в части базовых технологий их организации и предоставляемых сервисов. На основе проведенного анализа мы 
предлагаем два подхода к классификации платформ туманных вычислений: по принципу открытости/закрытости компонентов, а также трехуровневую классификацию на основе предоставляемого функционала платформы (Deploy-, Platform- и Ecosystem as a 
Service). 
Статья организована следующим образом. В разделе 1 рассмотрены облачные вычисления как основа новых вычислительных концепций, предпосылки появления облачных вычислений, их ключевые характеристики и предпосылки к появлению новых вычислительных концепций. Раздел 2 посвящен туманным и краевым вычислениям, их истории появления, а также определению и их ключевым характеристикам. В разделе 3 
рассматриваются технологии, обеспечивающие поддержку туманных вычислений: виртуализация и оркестрация. В разделе 4 представлен обзор платформ туманных вычислений: приватных, публичных, с открытым исходным кодом, а также сделано предложение о классификации туманных платформ. В заключении приводится краткая сводка 
результатов, полученных в рамках данного исследования, и указаны направления дальнейших исследований. 
1. Облачные вычисления как основа новых вычислительных 
концепций 
1.1. Предпосылки появления облачных вычислений 
Наиболее ранним предком облачных технологий принято считать зародившуюся в 
1960-е годы концепцию коммунальных вычислений (англ. utility computing) [18, 75]. Подход коммунальных вычислений подразумевал предоставление вычислительных ресурсов 
Обзор технологий организации туманных вычислений
36
Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»
по тем же принципам, по которым предоставляются такие коммунальные услуги как 
водо-, электро- и газоснабжение. Идея была призвана обеспечить пользователей «вычислениями по требованию» для получения максимальной эффективности при гибком 
ценообразовании [46]. В пакет предоставляемых услуг включались различные типы ресурсов: собственно вычислительные ресурсы, ресурсы хранения данных и другие вычислительные сервисы. Данные идеи описали Джон МакКарти [75] и Дуглас Пархил [66], 
который описал практически все основные характеристики существующих сегодня облаков, а также впервые употребив сравнение с электрической сетью. 
Данная концепция не нашла широкого распространения вплоть до 90-х годов XX 
века из-за технических сложностей, возникающих при развертывании и использовании 
данной архитектуры [3, 6, 27, 44, 46, 75]. Основной причиной аналитики называют недостатки организации сетей того времени, не обеспечивающих достаточной пропускной 
способности для реализации коммунальных вычислений [30]. 
В середине 90-х годов, совершенствование сетевых технологий и увеличение скорости передачи данных, привело к новому витку исследований коммунальных вычислений, 
но уже в рамках понятия грид вычислений (англ. Grid computing от англ. grid — 
сеть) [20, 27, 44] — по аналогии с electric power grid — электрической сетью. Идея коммунальных вычислений в рамках грид вычислений претерпела преображение [24, 61] и 
под грид-вычислениями стали понимать объединение ресурсов отдельных высокопроизводительных вычислительных систем в единую сеть.  
В [72] указываются следующие ключевые характеристики грид-вычислений. 
‒ 
Децентрализованная координация ресурсов. Грид-система должна объединять и координировать ресурсы и пользователей в разных частях сети и решать вопросы политики безопасности, оплаты и членства. 
‒ 
Стандарты и протоколы с открытым исходным кодом должны использоваться для 
аутентификации, авторизации, обнаружения ресурсов и доступа к ресурсам гридсистемы. 
‒ 
Качество предоставления услуг: ресурсы должны использоваться скоординированным образом для обеспечения необходимого качества обслуживания, времени отклика, пропускной способности и доступности для удовлетворения сложных требований 
пользователей. 
В качестве недостатков такого подхода, можно выделить то, что принципиальная 
децентрализованность грид-систем, отсутствие единого сетевого адресного пространства 
и единого администратора базовой вычислительной инфраструктуры усложняет обеспечение отказоустойчивости и динамическое управление ресурсами, доступными конечным 
пользователям [29, 41, 61]. Данные недостатки обусловили дальнейшее эволюционное 
развитие и появление облачных вычислений, которые зачастую используют модель 
грид-вычислений для расширения вычислительных ресурсов [41]. 
1.2. Ключевые характеристики облачных вычислений 
Развитие облачного рынка таким, каким мы его знаем сегодня, стало возможным с 
появлением Amazon Web Services (AWS) в 2002 году [53]. В 2006 году Amazon запустил 
Elastic Compute Cloud (EC2) в качестве коммерческого веб-сервиса, который позволял 
небольшим компаниям и отдельным лицам арендовать часть ИТ-инфраструктуры, на 
которой можно запускать любые приложения [14]. Именно данным шагом Amazon обеспечил дальнейшую популяризацию облачных вычислений. На сегодняшний день компания Amazon предоставляет полный пакет облачных инфраструктурных услуг, включая 
А.А. Кирсанова, Г.И. Радченко, А.Н. Черных
2020, т. 9, №3
37
хранение, вычисление и даже возможности человеческого интеллекта через Amazon 
Mechanical Turk. 
В 2011 году Национальный институт стандартов и технологий США (The National 
Institute of Standards and Technology, NIST) опубликовал определение облачных вычислений, их основные характеристики, а также модели их развертывания и обслуживания. 
Так, облачные вычисления (англ. Cloud computing) определены как модель для обеспечения повсеместного, удобного сетевого доступа по требованию к общему пулу настраиваемых вычислительных ресурсов (например, сетей, серверов, хранилищ, приложений и 
услуг), которые могут быть быстро предоставлены и освобождены с минимальными 
усилиями по управлению или взаимодействию с поставщиком услуг [50]. В рамках данного документа отмечается, что ключевыми характеристиками облачных вычислений 
являются: 
‒ 
самообслуживание по требованию — потребитель самостоятельно определяет 
свои вычислительные потребности: серверное время, скорости доступа и обработки 
данных, объем хранимых данных, без необходимости прямого взаимодействия с 
представителем поставщика услуг. Так, в статьях [40, 52, 79] приведены примеры 
решений, обеспечивающих повышение быстродействия в системах облачных вычислений с обеспечением самообслуживания по требованию; 
‒ 
универсальный доступ по сети — услуги облачных систем доступны потребителям по сети передачи данных вне зависимости от используемого терминального 
устройства; 
‒ 
объединение ресурсов — поставщик услуг объединяет ресурсы вычислительной 
системы в единый пул, обеспечивая возможность динамического перераспределения 
мощностей между большим числом независимых потребителей в условиях постоянного изменения спроса на мощности. При этом, потребители контролируют только 
основные параметры запрашиваемых услуг (например, объем необходимой памяти, 
скорость доступа), но фактическое распределение ресурсов, предоставляемых потребителю, осуществляет поставщик. Эффективные механизмы объединения ресурсов 
позволяют оптимизировать расход ресурсов памяти, хранения и передачи данных в 
облачных системах от 50% до 65% [78]. 
‒ 
эластичность — облачные услуги могут быть предоставлены, расширены, сужены 
в любой момент времени, без дополнительных издержек на взаимодействие с поставщиком, как правило, в автоматическом режиме [6, 41, 83]; 
‒ 
учет потребления — поставщик услуг автоматически исчисляет потребленные ресурсы на определенном уровне абстракции (например, объем хранимых данных, пропускная способность, количество пользователей, количество транзакций) и на основе 
этих данных оценивает объем предоставленных потребителям услуг. Такой подход, 
как отмечено в работе [42], позволяет получить эффективные вычислительные мощности с гибкой ценой в зависимости от предъявляемых требований. 
С точки зрения потребителя эти характеристики позволяют получить услуги с высоким уровнем доступности (англ. high availability) и низкими рисками неработоспособности, обеспечить быстрое масштабирование вычислительной системы благодаря эластичности без необходимости создания, обслуживания и модернизации собственной аппаратной инфраструктуры [29, 41, 61]. 
Период попыток внедрения облачных вычислений для различных целей и требований, привел к выделению следующих моделей развертывания облачных систем: частное 
облако, публичное облако, а также гибридное облако [15, 76]. 
Обзор технологий организации туманных вычислений
38
Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»
Частное облако разворачивается в рамках одной организации, доступно только 
внутренним пользователям и не предоставляет свои ресурсы пользователям вне этой организации. Публичное облако разворачивается сторонними организациями и предоставляет свои ресурсы внешним пользователям на условиях договора на право пользования. Гибридное облако является комбинацией из двух выше описанных типов развертывания, что позволяет выстраивать баланс между частными и публичными вычислениями [15]. 
Частные облака хороши тем, что чаще всего физически разворачиваются как можно 
ближе к конечному пользователю облака, что снижает время отклика вычислительного 
узла и повышению быстродействия передачи данных между узлами системы. Однако 
частное облако настраивается исключительно под вычислительные потребности своего 
владельца, что одновременно является как плюсом, так и минусом частных облаков. Не 
каждая организация имеет достаточно ресурсов для содержания собственного частного 
облака, которое должно удовлетворять как техническим требованиям по доступности и 
надежности, так и требованиям закона того государства, на территории которого находится как облако, так и сама организация [31, 62]. 
Пользователи публичных облаков, в свою очередь, часто сталкиваются с проблемой 
отсутствия прямого контроля над базовой вычислительной инфраструктурой. Это может привести к целому ряду проблем, таких как неконтролируемый доступ третьих лиц 
к приватным данным, размещенным в публичном облаке; блокировка серверов пользователя, которые могут быть развернуты в одной подсети с узлами, заблокированными в 
тех или иных государствах; неопределенность в качестве доступных ресурсов, так как 
они разворачиваются на серверах, используемых совместно с третьими лицами [31]. 
Также встает вопрос миграции и конвертации данных при необходимости смены провайдера, предоставляющего облако в пользование. 
В связи с этими недостатками каждого типа развертывания, часто провайдеры, 
предоставляющие облака частным компаниям, разворачивают именно гибридные облака [67], которые по требованию могут вести себя, как частные или публичные, что снимает проблемы с задержками передачи данными, безопасностью и вопросами миграции, 
а также эластичной настройки вычислительных ресурсов под каждую требуемую задачу. 
1.3. Предпосылки к появлению новых вычислительных концепций 
Несмотря на все существенные достоинства, гарантируемые публичными облачными 
платформами, в последние 5 лет активно стали появляться задачи, которые не могут 
быть эффективно решены этими подходами [21]. Так, большое число пользователей мобильных приложений, «умных» систем, таких как «умный дом», «умное предприятие», 
«умный город» и других IoT-решений, не всегда могут быть удовлетворены качеством 
услуг, предоставляемых облачными решениями, в частности, из-за увеличения объема 
пересылаемых данных между пользователем/устройством и облаком [33]. 
Появление подхода «умных» домов, производств, городов и др., наполненных множеством датчиков, исполнительных механизмов и других систем привело к пересмотру 
концепции архитектуры систем сбора и анализа данных. Концепция интернета вещей 
требует новых подходов к решениям хранения и быстрой обработки данных, а также 
возможностей быстрого отклика на изменение состояния конечных устройств [55, 56, 
80]. Также, распространение мобильных устройств в качестве основных платформ для 
клиентских приложений затрудняет передачу и обработку большого количества данных 
А.А. Кирсанова, Г.И. Радченко, А.Н. Черных
2020, т. 9, №3
39
без возникновения проблем с задержками отклика, в связи с постоянным перемещением 
мобильных устройств. 
С увеличением объема пересылаемых данных между устройствами IoT, клиентами и 
облаком, появляются проблемы увеличения времени отклика, связанные с ограничениями физической ширины сетевых каналов [46]. С другой стороны, появились чувствительные к времени отклика приложения и устройства, такие как системы жизнеобеспечения, машины-автопилоты, дроны и другие. В этих условиях удаленное централизованное облако стало неспособно удовлетворить требованиям сверхнизких временных задержек [80]. Также передача данных через множество шлюзов и подсетей поднимает 
вопрос о передаче чувствительных к конфиденциальности данных [36]. 
В ответ на данные проблемы частные предприятия и академическое сообщество 
подняли вопрос о необходимости разработки вычислительной парадигмы, удовлетворяющей требованиям новых концепций, таких как IoT [4, 47, 56]. Данная парадигма 
должна была заполнить разрыв между облаком и конечными устройствами, обеспечив 
вычисления, хранение и передачу в промежуточных сетевых узлах, наиболее приближенных к конечным устройствам. На данный момент разработано и применяется несколько парадигм, решающих данную проблему, включая туманные (fog) и краевые 
(edge) вычисления [12]. Каждая из этих парадигм имеет свои особенности, но все они 
сводятся к общему принципу — уменьшение временных задержек обработки и передачи 
данных, за счет переноса вычислительных задач ближе к конечному устройству. 
  
Рис. 1. Схема размещения облачных, туманных и краевых вычислений 
На рис. 1 изображена диаграмма относительного распределения вычислительных 
ресурсов, определяемых концепциями краевых, туманных и облачных вычислений. В 
соответствии с ней, облачные вычисления представляют собой отдельный центр обработки данных (ЦДО) или же сеть ЦОД, расположенных далеко от пользователя, но 
обеспечивающих высокие вычислительные возможности. С другой стороны, краевые 
вычисления расположены непосредственно на краю вычислительной системы и обеспечивают небольшие вычислительные возможности, но в непосредственной близости к потребителю этих ресурсов. Туманные вычисления располагаются между краем сети и обОбзор технологий организации туманных вычислений
40
Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»
лачным ЦОД, обеспечивая существенные вычислительные ресурсы близко к конечному 
пользователю, которые, с другой стороны, не сопоставимы с общим объемом облачных 
вычислений, но могут быть настраиваемы и масштабируемы в зависимости от задач конечного пользователя. 
В данной статье будут рассматриваться туманные вычисления как отправная точка 
для появления остальных парадигм [36]. Также будут рассмотрены отличительные особенности краевых вычислений, с которыми часто объединяют туманные. 
2. Туманные и краевые вычисления 
2.1. История и определение 
Первым из вариантов технологии для устранения проблем с временными задержками стала технология туманных вычислений. Термин «Туманные вычисления» впервые 
был предложен компанией CISCO в 2002 году [28] и был описан как «расширение для 
облачных вычислений, обеспечивающее предоставление вычислительных и сетевых ресурсов, а также ресурсов для хранения информации, расположенных между конечными 
устройствами и облачными вычислительными центрами» [4]. В 2015 году был основан 
консорциум OpenFog — группа компаний и академических организаций, таких как 
Cisco, Dell, Intel и Microsoft Corp, а также Принстонского университета, ориентированная на стандартизацию туманных вычислений (18 декабря 2018 консорциум OpenFog 
стал частью The Industrial Internet Consortium) [92]. 
В 2018 году Национальный институт стандартов и технологий США сформулировал 
официальное определение термина туманные вычисления:  
«Туманные вычисления (ТВ) — это многоуровневая модель, обеспечивающая повсеместный доступ к общей совокупности масштабируемых вычислительных ресурсов. 
Модель ТВ облегчает развертывание распределенных приложений и услуг, учитывающих сетевые задержки, и состоит из туманных узлов (англ. fog nodes) (физических или 
виртуальных), располагающихся между умными конечными устройствами и централизованными (облачными) сервисами. Туманные узлы являются контекстно-зависимыми и 
поддерживают единую систему управления данными и организации связи. Они могут 
быть организованы в кластеры вертикально (для поддержки изоляции), горизонтально 
(для поддержки федераций сервисов) или в привязке к сетевой близости от конечных 
умных устройств. Туманные вычисления минимизируют время сетевого отклика поддерживаемых приложений а также обеспечивают конечные устройства локальными вычислительными ресурсами и, при необходимости, сетевым подключением к централизованным сервисам» [36]. 
Устранение разрыва между облаком и конечными устройствами за счет вычисления, 
хранения и управления данными не только в облаке, но и на промежуточных узлах [45] 
расширило область применения туманных вычислений, что позволило применять их в 
новых 
задачах: 
IoT, 
транспортные 
средства [34], 
«умный» 
город [11], 
здравоохранение [23], «умная» доставка (в том числе с использование использованием 
беспилотников) [74], подземная съемка в реальном времени, видеонаблюдение и др. [82] 
2.2. Ключевые характеристики туманных вычислений 
Из-за позднего отделения понятий туманных и краевых вычислений многие компании вводили собственные характеристики [1] и определения для туманных и краевых 
вычислений, зачастую объединяя их в одно [45]. В табл. 1 представлены ключевые характеристики, которые выделялись различными авторами для туманных и краевых вычислений. 
А.А. Кирсанова, Г.И. Радченко, А.Н. Черных
2020, т. 9, №3
41
В 2017 году консорциум OpenFog выпустил эталонную архитектуру туманных вычислений, которая построена на восьми основных принципах: программируемость, 
иерархия, гибкость, удобство обслуживания (Reliability, availability and serviceability — 
RAS), доступность, надежность, автономность, открытость и безопасность [54]. 
Таблица 1  
Характеристики туманных вычислений [51] 
Кто 
предложил 
характеристику 
Характеристика 
Bonomi et al. [68] 
Высокая степень виртуализации 
Размещается между конечными устройствами и облаком 
Может быть расположено не только на самом краю сети 
Cisco Systems [37] 
Расширяет Облако 
Чаще используется в сфере IoT 
Может быть развернуто, где угодно 
Туманные устройства отвечают за обработку, хранение и подключение к сети 
Vaquero and  
Rodero–Merino [86] 
Гетерогенное, вездесущее и децентрализованное общение 
устройств между собой 
Хранение и обработка данных выполняются без использования 
сторонних устройств и ресурсов 
Выполнение процессов на устройстве в режиме песочницы 
«Аренда» пользовательских устройств для нужд поддержки 
системы 
IBM [3] 
Определили Туманные и Краевые вычисления как единую 
концепцию 
Не зависит от централизованного облака 
Располагается на конечных точках сети 
Размещение некоторых вычислительных ресурсов на краю облака 
Обобщение [51] 
Может использовать как виртуализацию, так и другие средства развертывания 
Обращение в облако для выполнения нечувствительных к 
временным задержкам операций и хранения информации 
Любое краевое устройство с достаточными вычислительными 
мощностями и объемом для хранения данных может выступать Туманным узлом 
Всегда размещается между конечным пользователем и облаком 
 
В [36] выделены следующие ключевые характеристики туманных вычислений. 
‒ 
Осведомленность о местоположении и низкая латентность — туманные узлы 
осведомлены о логическом расположении друг относительно друга, что позволяет 
производить расчет временных затрат на связь с другими узлами. 
‒ 
Географическая распределенность — туманные сервисы и приложения 
способны работать с расположенными в различных географических точках 
шлюзами, через которые и осуществляется подключение в туман. 
Обзор технологий организации туманных вычислений
42
Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»
‒ 
Поддержка разнородных данных — поддержка сбора и обработки данных 
различных 
форматов, 
полученных 
с 
помощью 
различных 
типов 
сетевых 
коммуникационных возможностей. 
‒ 
Операционная совместимость и федеративность — компоненты туманных 
вычислений должны быть способны взаимодействовать друг с другом вне 
зависимости от различий между собой, а сервисы должны быть распределены по 
различным доменам для обеспечения доступа. 
‒ 
Взаимодействия в реальном времени — туманные приложения должны иметь 
возможность работать в режиме реального времени, а не с использованием пакетной 
обработки запросов. 
‒ 
Масштабируемость 
и 
динамичность 
федеративных, 
туманно-узловых 
кластеров — туманные вычисления должны быть адаптивными по своей сути. 
Должна быть обеспечена поддержка следующих ключевых механизмов адаптации: 
эластичность вычислений, объединение возможностей ресурсов, подстройка под 
изменения в нагрузке данных и изменения состояния сети. 
2.3. Понятия туманных и краевых вычислений 
В некоторых источниках туманные вычисления называют краевыми или граничными вычислениями, основываясь на ключевом описании технологии, что сбор и анализ 
данных организован не в централизированном облаке, а как можно ближе к конечному 
устройству, «на краю сети» [4, 21, 33, 36]. Однако в [80] указано, что хотя туманные и 
краевые вычисления перемещают вычисления и хранение данных на край сети, ближе к 
конечным узлам, эти парадигмы не идентичны. 
В парадигме туманных вычислений туманные узлы размещаются на границе локальной сети, зачастую они разворачиваются на базе роутеров, беспроводных точек доступа (если данные устройства поддерживают требуемые технологии для разворачивания туманного узла) [74]. В отличие от туманных вычислений, краевые вычисления 
размещаются еще «ближе» к конечным устройствам, уже внутри самой локальной сети 
на промежуточных точках доступа, а иногда и сами конечные устройства могут выступать краевыми вычислительными узлами: смартфоны, планшеты, другие вычислительные устройства с достаточными вычислительными возможностями и поддержкой развертывания вычислительных узлов [70]. Однако это в то же самое время ограничивает 
их вычислительные мощности, и поэтому имеются некоторые ограничения в области их 
применения. На данный момент краевые вычисления применяются для решения таких 
задач как видеонаблюдение, кеширование видео и контроль трафика [80]. 
Консорциум OpenFog заявляет, что граничные вычисления часто ошибочно называют туманными вычислениями, и определят, что основным их отличием является то, что 
туманные вычисления являются иерархическими и обеспечивают вычисления, создание 
сетей, хранение, управление и ускорение в любом месте — от облака до конечных узлов 
интернета вещей; в то время как краевые вычисления имеют тенденцию ограничиваться 
вычислениями на узлах конечных пользователей [48]. Кроме того, в [9] о туманных и 
краевых вычислениях авторы отмечают, что «туман включает облако, ядро, транспорт, 
край, клиентов и все остальное» и «туман стремится реализовать непрерывность вычислительных услуг от облака до устройств, а не рассматривать границы сети как изолированные вычислительные платформы». 
Таким образом, термин «краевые вычисления» преимущественно используется в телекоммуникационной отрасли и обычно относится к базовым станциям 4G/5G, RAN 
(Radio Access Network) и ISP (Internet Service Provider) [9, 43]. Тем не менее, данный 
А.А. Кирсанова, Г.И. Радченко, А.Н. Черных
2020, т. 9, №3
43
термин стал с недавних пор использоваться в предметной области IoT [22, 43, 84] по отношению к локальной сети, где расположены датчики и устройства IoT. Другими словами, «краевые вычисления» расположены в пределах первого от самого IoT устройства 
транзитного участка сети, например, на точках доступа WiFi или шлюзах. 
3. Технологии, обеспечивающие поддержку туманных 
и краевых вычислений 
3.1. Виртуализация 
Облака размещаются в ЦОД, где оборудование рассчитано на сверхбольшие нагрузки. Однако не всегда серверное оборудование загружено в полной мере, что не позволяет использовать его эффективно. Ключевой технологией, которая обеспечила поддержку 
облачных, а затем и туманных вычислений стала технология виртуализации [69], которая позволяет использовать ресурсы одной физической машины несколькими логическими виртуальными машинами (ВМ) на уровне слоя аппаратных абстракций (англ. 
Hardware Abstraction Layer — HAL). Технология виртуализации использует гипервизор — программный слой, обеспечивающий работу виртуальных машин на базе аппаратных ресурсов. Машина с гипервизором называется хост-машиной. Виртуальная машина, выполняемая на хост-машине, называется гостевой машиной, на которой в свою 
очередь могу быть установлены гостевые операционные системы (ОС). Данный вид виртуализации называют виртуализацией на основе гипервизора.  
Также существует виртуализация на основе контейнеров [13], которые представляют собой упакованный, автономный, развертываемый набор прикладных компонентов, которые могут также включать промежуточное программное обеспечение и бизнес-логику в виде бинарных файлов и библиотек для запуска приложений. 
В работе [58] представлен сравнительный анализ обоих типов виртуализации, на основе которого можно выделить некоторые преимущества виртуализации на основе контейнеров. 
‒ 
Аппаратные ресурсы. Виртуализация на основе контейнеров снижает затраты на 
оборудование за счет возможности консолидации. Это позволяет параллельному 
программному обеспечению воспользоваться преимуществами истинного параллелизма, обеспечиваемого многоядерной аппаратной архитектурой. 
‒ 
Масштабируемость. Система управления контейнерами может эффективно 
управлять большим количеством контейнеров, позволяя создавать дополнительные 
контейнеры по мере необходимости. 
‒ 
Пространственная изоляция. Контейнеры поддерживают легкую пространственную изоляцию, предоставляя каждому контейнеру свои собственные ресурсы 
(например, ядро процессора, память и доступ к сети) и специфические для контейнера пространства имен. 
‒ 
Хранение. Контейнеры имеют малый вес по сравнению с виртуальными машинами. 
Приложения внутри контейнеров имеют общие двоичные файлы и библиотеки. 
‒ 
Производительность. По сравнению с виртуальными машинами, контейнеры имеют более высокую производительность (сквозную), т.к. они не эмулируют оборудование. 
‒ 
Портативность. Контейнеры поддерживают легкую переносимость из среды разработки в производственную среду, особенно для облачных приложений. 
Таким образом, существуют две основные технологии виртуализации, которые применяются в данный момент для обеспечения поддержки туманных вычислений [39]. 
Обзор технологий организации туманных вычислений
44
Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»
Причем виртуализация на основе контейнеров получает все большое распространение, 
благодаря меньшим требованиям к производительности аппаратного обеспечения, чтобы 
обеспечить развертывание вычислительных узлов на промежуточных устройствах, которые могут не обладать высокими вычислительными мощностями, что особенно актуально для краевых вычислений, т.к. они запускаются даже не на самих IoT устройствах [57], а на промежуточных точках доступа, наиболее близких к устройствам. 
3.2. Концепция оркестрации туманных вычислений 
Когда контейнеризация стала развиваться как одна из технологий поддержки туманных вычислений, встал вопрос управления вычислительной нагрузкой для обеспечения эффективного использования географически-распределенных ресурсов [38]. Реализация туманных вычислений требует решения проблемы управления вычислительными 
ресурсами на другом уровне, по сравнению, например, с облачными вычислениями [73]. 
Первая сложная задача, которая возникает при работе с туманными вычислениями 
в отличие от облачных, — это управление распределением вычислительной нагрузки 
(оркестрация) между узлами тумана [43, 44] посредством размещения на них туманных сервисов, а также оркестрация этих сервисов, т.е. обеспечение эффективной совместной работы вычислительных сервисов для решения задач, возложенных на туманную среду. 
В работе [77] сформулировано, что оркестрация в применении к туманным вычислениям обеспечивает централизованный механизм поиска и обнаружения туманных ресурсов, привязку приложений на основе их требований к физическим ресурсам (развертывание и планирование); управление выполнением рабочей нагрузки с контролем качества обслуживания (QoS).  
Рассмотрим ключевые задачи, которые должны решаться системой оркестрации туманных вычислений (Fog Orchestrator) [5, 73]. 
‒ 
Планирование заданий. Система оркестрации должна организовывать эффективную совместную работу туманных узлов для выполнения вычислительных заданий. 
Брокер вычислительных ресурсов должен учитывать специфику рабочего процесса 
выполнения каждого задания для оптимизации процессов их выполнения. 
‒ 
Вычисление и оптимизация маршрутов. Система оркестрации должна учитывать распределенный характер туманной среды при построении сетевых маршрутов. 
Она должна обеспечивать поддержку сквозного соединения узлов в случае отсутствия прямого соединения, адаптацию к динамическим сетевым топологиям, максимизацию пропускной способности сети и производительности приложений, обеспечение устойчивости сети. 
‒ 
Обнаружение. Должно обеспечиваться обнаружение физических и виртуальных 
туманных устройств, а также ресурсов, связанных с ними. 
‒ 
Функциональная совместимость. Оркестратор туманной вычислительной среды 
должен поддерживать единый набор протоколов, стандартных интерфейсов и онтологий. Это обеспечит возможность различным узлам и приложениям в системе взаимодействовать друг с другом. 
‒ 
Минимизация задержек. Одна из основных задач оркестратора туманных вычислений — уменьшение задержек передачи и обработки данных. Добиться этого позволяют интеллектуальные механизмы оптимизации потоков данных и планирования 
ресурсов. 
А.А. Кирсанова, Г.И. Радченко, А.Н. Черных
2020, т. 9, №3
45
‒ 
Отказоустойчивость. Должно обеспечиваться бесшовное взаимодействие между 
всеми участниками туманной среды, вне зависимости от возможных сбоев и проблем 
как на физическом, так и логическом уровне. 
‒ 
Прогнозирование и оптимизация. За счет сбора, хранения и анализа работы узлов системы, система оркестрации может обеспечивать оптимизацию маршрутов передачи данных и взаимодействия устройств между собой для выполнения требований по задержкам и качеству предоставляемых услуг. 
‒ 
Безопасность и приватность. Из-за усложнения топологии туманной сети существенно меняется постановка задачи обеспечения безопасности и конфиденциальности обрабатываемых данных. 
‒ 
Аутентификация и контроль доступа. Решение вопросов безопасности приводит 
к введению учетных записей, ролей и прав доступа к каждому отдельному узлу или 
группе узлов для управления, развертывания и использования. 
4. Обзор платформ туманных вычислений 
При обзоре существующих платформ для развертывания туманных вычислений были рассмотрены коммерческие платформы, а также платформы с открытым исходным 
кодом. Сложность анализа коммерческих платформ заключается в отсутствии информации об их архитектуре и применяемых технических решениях, которые составляют 
коммерческую тайну. Однако анализ коммерческих решений показал, что среди коммерческих туманных платформ встречаются как платформы с полным обеспечением 
поддержки туманных вычислений (собственно вычисления, аналитика и организация 
транспортного уровня туманной сети), так и платформы, которые обеспечивают лишь 
транспортный уровень туманной сети и не обеспечивают управления вычислительными 
узлами и собственно туманными вычислениями на них. Платформы, обеспечивающие 
лишь транспортный уровень туманных вычислений, не будут рассматриваться в данной 
работе. 
Можно выделить следующие ключевые характеристики частных и публичных коммерческих туманных платформ (см. табл. 2–3). 
1. Поддерживаемые аппаратные платформы — платформа может работать с 
любым устройством, поддерживающим виртуализацию или контейнеризацию, 
или только с ограниченным списком устройств — через драйвера или фирменные устройства. Smartiply Fog, ThingWorx и Cisco IOx работают только с собственным аппаратным обеспечением. 
2. Базовая технология разработки — на базе какой исполняемой среды создаются и запускаются приложения.  
3. Открытость коммуникационных протоколов и SDK — есть ли ограничения на приложения, которые могут использоваться в тумане: требуется ли портирование приложений, или в принципе могут исполняться только приложения, 
написанные с использованием специальных поставляемых SDK, как, например, в 
случае с ThingWorx, чьи туманные приложения должны быть написаны с использованием фирменного SDK для запуска в тумане. 
4. Технология развертывания — какая из технологий развертывания туманных 
узлов используется, если известно. 
5. Возможности интеграции — имеется ли возможность интеграции с другими 
платформами, например, корпоративными решениями или публичными облаками. 
Обзор технологий организации туманных вычислений
46
Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»
6. Подключение внешних источников данных — возможность платформы 
подключаться к сторонним базам и хранилищам данных, физически расположенных вне центрального облака для хранения и обработки данных. 
7. Доступность дополнительных сервисов (Machine Learning, Analytics и 
т.п.) — возможность подключения и использования дополнительно поставляемых сервисов, которые предоставляют дополнительный функционал по анализу 
и работе с данными в тумане. 
8. Поддержка Edge — возможность подключения и использования краевых 
устройств и краевых вычислений, и дальнейшего сбора и обработка информации 
от них. 
4.1. Частные туманные платформы 
Частные туманные платформы обеспечивают создание частных туманных решений 
на базе вычислительной инфраструктуры, развернутой непосредственно на ресурсах заказчика.  
Платформа Cisco IOx была представлена компанией Cisco в 2014 году [2] как 
развитие сетевой инфраструктуры в связи с ожидаемым ростом IoT. Основной упор в 
платформе сделан на уменьшение трудозатрат по портированию приложений на туманные узлы, что достигается за счет использования технологий контейнеризации и базирования собственной ОС на основе Linux системы. 
Cisco IOx — это среда приложений, которая объединяет в себе Cisco IOS (мини операционная система, устанавливаемая на всю аппаратуру Cisco) и Linux. Для разработки 
приложений применяются утилиты Linux с открытым исходным кодом. Используется 
единый протокол взаимодействия туманных приложений во всей сети, организованной с 
использованием технологий Cisco IoT. Туманные приложения, которые можно запускать на инфраструктуре IOx, поставляются как Cisco, так и партнерами компании. Разработку приложений можно вести на множестве языков программирования общего 
назначения. 
Для разработки и развертывания приложений используется Docker. Поддерживаются разнообразные типы приложений, включая Docker-контейнеры и виртуальные машины (если такая возможность есть у сетевого оборудования). Также возможно использовать собственную исполняемую среду IOx для написания приложений на высокоуровневых языках программирования (например, Python). 
Платформа Nebbiolo Technologies нацелена на корпоративный индустриальный 
рынок, который поддерживает концепцию Индустрии 4.0 [89]. Компания Nebbiolo Technologies тесно сотрудничает с Toshiba Digital Solutions [93] в поставке готовых вычислительных решений для промышленного и IoT сектора. 
Платформа состоит из аппаратного обеспечения fogNode, программного стека fogOS 
и системного администратора fogSM, развертываемого в облаке или локально [32]. Fog 
System Manager (fogSM) предоставляет облачную платформу централизованного управления, которая позволяет развертывать и настраивать устройства на периферии. 
Ключевой особенностью платформы является fogOS [32] — программный стек, обеспечивающий связь, управление данными и развертывание приложений на уровне тумана. Основанная на гипервизоре, fogOS предоставляет набор функций в виртуализированной форме. Поддерживается широкий спектр стандартов подключения устройств, а 
также позволяет размещать приложения и управлять ими в реальном времени. 
Платформа ClearBlade представляет собой стек технологий, обеспечивающий 
быструю разработку и развертывание корпоративных IoT решений, начиная от краевых 
А.А. Кирсанова, Г.И. Радченко, А.Н. Черных
2020, т. 9, №3
47
устройств, заканчивая облачными сервисами. Она включает в себя программные компоненты, устанавливаемые на весь стек IoT устройств, а также обеспечивает возможность 
подключения сторонних систем через предоставляемый API для интеграции с устройствами, внутренними бизнес-приложениями и облачными сервисами. Платформа 
ClearBlade обеспечивает централизованную консоль управления IoT-приложениями, с 
возможностью развертывания как локально, так и в облаке. Функции управления 
платформой делегируются краевым узлам (либо на самих конечных устройствах или 
шлюзах к ним) при помощи системы туманных и краевых вычислений ClearBlade 
Edge [35]. 
Таблица 2 
Обзор частных туманных платформ 
Характеристика ClearBlade Smartiply 
Fog 
LoopEdge ThingWo
rx 
Nebbiolo 
Technologi
es 
Cisco IOx 
Поддерживаемые аппаратные 
платформы 
Универсально 
Собственная аппаратура 
Универсально 
Собственная аппаратура 
Универсально 
Собственная аппаратура 
Базовая технология разработки 
JavaScript 
Нет  
данных 
Универсально 
(Docker) 
Java VM 
Универсально 
(Docker) 
Docker, 
Linux, 
IOx 
Открытость 
коммуникационных 
протоколов 
и SDK 
+ 
+ 
+ 
– 
+ 
+ 
Технология развертывания 
Linux KVM 
Нет  
данных 
Docker 
Нет  
данных 
Docker 
Linux 
KVM 
Возможности 
интеграции 
Oracle, SAP, 
Microsoft, 
Salesforce 
– 
– 
Microsoft 
Azure IoT 
Hub 
– 
Microsoft 
Azure IoT 
Hub 
Подключение 
внешних 
источников данных 
+ 
– 
+ 
+ 
+ 
+ 
Доступность дополнительных 
сервисов 
Нет данных 
+ 
– 
+ 
+ 
+ 
Поддержка Edge 
+ 
+ 
+ 
+ 
+ 
+ 
Платформой поддерживается бессерверный (англ. Serverless computing) подход к 
разработке сервисов на основе языка JavaScript, которые могут быть настроены на реализацию методов машинного обучения и анализа данных. Платформа обеспечивает механизмы экспорта данных и аналитики, собранной системой, в широко применяемые 
бизнес-системы, приложения и базы данных за счет интеграции с корпоративными 
платформенными решениями от Oracle, SAP, Microsoft и Salesforce. ClearBlade также 
предоставляет собственные панели управления, бизнес-приложения и системы управления базами данных для комплексного наблюдения и управления IoT экосистемой. 
ClearBlade использует модель OAuth для контроля доступа, где каждый пользователь и устройство получает токен, который должен быть авторизован для получения 
доступа в систему или ее узлу. Данные шифруются как на самих устройствах, так и пеОбзор технологий организации туманных вычислений
48
Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»
редачи по сети. Передаваемые данные шифруются с помощью библиотек OpenSSL с 
TLS-шифрованием. 
Платформа Smartiply Fog — это туманная вычислительная платформа, которая 
делает упор на оптимизацию ресурсов и поддержание работы устройств в системе даже 
без подключения к облаку. Для онлайн сред платформа обеспечивает более высокую 
надежность благодаря оптимизации ресурсов и вычислений, которые производятся на 
базе аппаратуры собственного производства [85]. Платформа обеспечивает взаимодействие между устройствами по принципу точка-точка. Таким образом, система узлов 
может продолжать автономную работу получения, анализа и хранения данных, вплоть 
до восстановления связи с внешней сетью [88]. 
Платформа LoopEdge от Litmus Automation позволяет подключать различные 
устройства в единую систему, собирать и анализировать данные от них. Также Litmus 
Automation предоставляет отдельную платформу Loop, позволяющую управлять жизненным циклом любого IoT устройства и экспортировать данные в реальном времени во 
внутренние аналитические и бизнес-приложениями.  
Разработчики платформы подчеркивают, что она способна работать с практически 
любым устройством, причем как с промышленным, так и доступным бытовому потребителю. Например, платформа поддерживает подключение устройств на базе Arduino и 
Raspberry Pi. Даже если какое-то устройство не поддерживается, подключить его к 
платформе достаточно легко, за счет устанавливаемых на само устройство исполняемых 
пакетов, которые можно расширять и создавать с нуля под конкретное устройство. Все 
инструкции находятся в открытом доступе. 
Данная платформа получила широкое распространение среди известных машиностроительных концернов: Nissan, Renault, Mitsubishi Corporation Techno. 
Платформа PTC ThingWorx — это IoT платформа, предлагающая подключение 
достаточно большого числа устройств (доступны драйвера для 150 видов устройств). 
Однако из-за того, что подключение устройств осуществляется через драйвера, которые 
требуют установки, прежде чем устройство можно использовать в тумане, эта платформа не является универсальной и имеет ограничения по используемым устройствам.   
Приложения для платформы требуется писать с используемых поставляемых SDK. 
Дальнейший анализ данных и управление бизнес-процессами идет также через предоставляемые инструменты самой платформы. Для выполнения этих задач платформа 
имеет обширный раздел для разработчиков с инструкциями и учебными пособиями, а 
также помощью специалистов от самой компании для установки, настройки и расширения платформы. Также «из коробки» имеется возможность подключения к Microsoft 
Azure IoT Hub. 
4.2. Публичные туманные платформы 
Публичные туманные платформы сегодня представляют собой решения крупных 
игроков на рынке облачных вычислений, ориентированные на решения задач обработки 
данных от IoT-систем, привязанные к возможностям соответствующей облачной платформы. Ключевые характеристики рассматриваемых публичных туманных платформа 
приведены в табл. 3. 
Платформа Azure IoT предоставляет платформу для туманных и краевых вычислений на основе стека технологий от компании Microsoft. Платформа Azure IoT состоит из нескольких крупных подсистем, таких как IoT Central, IoT Edge, которые в 
свою очередь основывают свою работу на облачной технологии Microsoft Azure. Подключение устройств от партнеров Microsoft возможно без использования драйверов или 
А.А. Кирсанова, Г.И. Радченко, А.Н. Черных
2020, т. 9, №3
49
программного кода за счет технологии IoT Plug and Play. Такой подход возможен для 
устройств под управлением любой ОС, включая Linux, Android, Azure Sphere OS, 
Windows IoT, RTOS и другие. 
Создание, установка и управление туманными приложениями осуществляется через 
портал Azure IoT Hub. IoT Hub — это управляемая служба, размещенная в облаке, которая выступает в качестве центрального обработчика сообщений для двунаправленной 
связи между IoT приложением и устройствами, которыми оно управляет. IoT Hub поддерживает передачу данных как от устройства к облаку, так и от облака к устройству. 
IoT Hub поддерживает несколько шаблонов обмена сообщениями, таких как телеметрия 
между устройствами и облаками, загрузка файлов с устройств и технологию «запросответ» для управления устройствами из облака. 
Для развертывания вычислений ближе к самим устройствам или же на самих 
устройствах использует Azure IoT Edge, которая позволяет разворачивать приложения с 
собственной бизнес логикой или уже имеющиеся в каталоге готовые приложения на конечных устройствах при помощи технологии контейнеризации. 
Платформа Amazon AWS IoT Greengrass позволяет распространить возможности AWS (Amazon Web Services) на периферийные устройства, что позволяет им локально работать с данными, используя при этом облако для управления, анализа и 
надежного хранения данных. AWS IoT Greengrass позволяет подключенным устройствам выполнять функции AWS Lambda, запускать контейнеры Docker, формировать 
прогнозы на основе моделей машинного обучения, синхронизировать данные устройств и 
безопасно взаимодействовать с другими устройствами даже без подключения к Интернету. 
AWS IoT Greengrass позволяет создавать решения IoT, которые подключают различные типы устройств к облаку и друг к другу. AWS IoT Greengrass Core можно использовать на устройствах под управлением Linux (в том числе дистрибутивов Ubuntu и 
Raspbian), которые поддерживают архитектуры Arm или x86. Сервис AWS IoT 
Greengrass Core обеспечивает локальное исполнение кода AWS Lambda, передачу сообщений, управление данными и безопасность. Устройства с AWS IoT Greengrass Core выступают в качестве порталов сервиса и могут взаимодействовать с другими устройствами, на которых работает FreeRTOS (Real-time operating system for microcontrollers) или 
установлен пакет SDK AWS IoT для устройств. Размер таких устройств может быть 
очень разным: от небольших устройств на базе микроконтроллеров до крупных бытовых 
приборов. Когда устройство с AWS IoT Greengrass Core теряет связь с облаком, устройства в группе AWS IoT Greengrass могут продолжать взаимодействовать друг с другом 
по локальной сети. 
Платформы Google, Yandex и Mail.ru предоставляют собственные облачные и 
туманные решения для сбора, хранения, обработки, анализа и визуализации данных. 
Собранные данные с устройств интегрируются в публичную облачную систему для более глубокой обработки и анализа (включая машинное обучение и искусственный интеллект) за счет высоких вычислительных мощностей облака. Данные платформы поддерживают множество протоколов подключения и взаимодействия через предоставляемое API. Имеется большое количество готовых к использованию сервисов, доступные 
для установки в каталоге самой платформы, которые можно подключать к собственному туманному решению, комбинируя между собой. 
Обзор технологий организации туманных вычислений
50
Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»
Таблица 3  
Обзор публичных туманных платформ 
Характеристика 
AWS 
Greengrass 
Azure IoT 
Google 
Yandex 
Mail.ru 
Поддерживаемые 
аппаратные 
платформы 
Универсально 
Универсально 
Универсально 
Универсально 
Универсально 
Базовая технология разработки 
Универсально 
(Docker) 
Универсально 
(Docker) 
Универсально 
Универсально 
Универсально 
Открытость коммуникационных 
протоколов 
и 
SDK 
+ 
+ 
+ 
+ 
+ 
Технология  
развертывания 
Docker 
Docker 
Docker 
Docker 
Docker 
Возможность  
интеграции 
Amazon 
Elastic 
Compute 2 
Azure, 
через API 
Сервисы 
Google и 
партнеров, 
через API 
Универсально через 
API 
Универсально через 
API 
Подключение 
внешних 
источников данных 
– 
– 
+ 
+ 
– 
Доступность  
дополнительных 
сервисов 
(Machine 
Learning, 
Analytics и т.п.) 
+ 
+ 
+ 
+ 
+ 
Поддержка Edge 
+ 
+ 
+ 
+ 
+ 
 
4.3. Туманные платформы с открытым исходным кодом 
В ходе анализа существующих решений нами был проведен обзор существующих 
туманных платформ с открытым исходным кодом. В отличие от коммерческих решений, для open source платформ представлены полные описания архитектур, требований 
к вычислительным ресурсам, а также используемым технологиям, как на аппаратном, 
так и программном уровнях (см. табл. 4).  
Таблица 4 
Обзор туманных платформ с открытым исходным кодом 
 
Цель 
Внедрение 
FogFrame2.0 
Проверить концептуальную модель 
– 
FogFlow 
Более простая и гибкая оркестрация сервисов 
+ 
FogBus 
Преодолеть неоднородность на уровне связи ОС и 
P2P различных узлов тумана 
– 
А.А. Кирсанова, Г.И. Радченко, А.Н. Черных
2020, т. 9, №3
51
Платформа FogFrame2.0 — туманная платформа с открытым исходным 
кодом [91], нацеленная на развертывание на одноплатных компьютерах (Raspberry Pi). 
Была представлена для решения следующих задач [63]:  
‒ 
определить и реализовать функции туманной инфраструктуры, т.е. создание и поддержание туманного ландшафта (множества туманных узлов и устройств) и управление приложениями [64, 65]; 
‒ 
реализовать эвристические алгоритмы для размещения услуг в тумане, а именно алгоритм первичной подгонки и генетический алгоритм; 
‒ 
ввести механизмы адаптации к динамическим изменения в тумане и для восстановления после перегрузок и сбоев. 
Чтобы оценить поведение FogFrame, применялись различные модели поступления 
запросов приложений (константы, пирамиды и случайные обходы), а также исследовались процессы размещения вычислительных сервисов. Целью исследования в итоге было 
наблюдение за поведением платформы и ее реакцией на сбои. Платформа динамически 
реагирует на события во время выполнения, то есть, когда новые устройства появляются или отключаются, когда устройства испытывают сбои или перегрузки, выполняются 
необходимые передислокации узлов.  
Платформа FogFlow — это туманная платформа с открытым исходным 
кодом [90]. Основной задачей разработчиков данной платформы было обеспечение простого и гибкого способа разработки, развертывания и оркестрации туманных сервисов 
[8]. Уникальность их подхода заключается в:  
‒ 
контекстно-зависимой оркестрации сервисов в то время, как другие сервисы оперируют просто событиями (event) или группами событий (topic), происходящими в сети; 
‒ 
сервисы и приложения FogFlow разработаны с учетом общего представления всех 
облачных узлов и пограничных узлов, а не с точки зрения каждого отдельного пограничного узла. 
FogFlow предлагает использовать программную модель потоков данных, дополненную декларативными подсказками, основанными на широко используемом стандарте 
NGSI, что дает разработчикам услуг два преимущества:  
1) быстрая и простая разработка приложений туманных вычислений, т.к. подобные 
подсказки скрывают множество настроек и сложностей развертывания от разработчиков сервисов;  
2) высокая степень открытости и функциональная совместимость для обмена информацией и интеграции источников данных. 
FogFlow является одним из компонентов большой открытой инфраструктуры 
IWARE [19], 
обеспечивающей 
разработку 
и 
внедрение 
различных 
smart 
решений [7, 10, 17]. Эта инфраструктура является одним из современных облачных 
фреймворков наряду с Amazon Web Services [25]. Для внедрения и использования FogFlow доступна широкая библиотека готовых решений от сообщества разработчиков и 
подробные инструкции по внедрению [87]. 
Платформа FogBus (при поддержке Melbourn Clouds Lab) объединяет различные 
аппаратные инструменты через программные компоненты, которые обеспечивают 
структурированное взаимодействие и независимое от платформы выполнение приложений [71]. FogBus применяет блокчейн для обеспечения целостности данных при передаче 
конфиденциальных данных. Независимая от платформы архитектура исполнения приложений и взаимодействия между узлами позволяет преодолеть неоднородность в интегрированной среде.  
Обзор технологий организации туманных вычислений
52
Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»
FogBus поддерживает реализацию различных политик управления ресурсами и планирования для выполнения приложений IoT, составленных с использованием моделей 
параллельного программирования, таких как SPMD (single program, multiple data). 
Для оценки характеристик платформы FogBus используется прототип прикладной 
системы для анализа данных Sleep Apnea. На этом примере иллюстрируется, как приложение (в области здравоохранения), составленное с использованием модели SPMD 
может быть реализовано с использованием различных настроек FogBus для обработки 
IoT-данных в интегрированной вычислительной среде.  
Данный фреймворк облегчает развертывание приложений IoT, мониторинг ресурсов 
и управление ими. Системные сервисы FogBus разработаны на кроссплатформенных 
языках программирования (PHP и Java) и используются с расширяемым протоколом 
прикладного уровня (HTTP), который помогает FogBus преодолевать неоднородность 
на уровне связи ОС и P2P различных узлов тумана. Кроме того, платформа FogBus 
функционирует как модель «Платформа как услуга» (PaaS) для интегрированной среды Fog Cloud, которая не только помогает разработчикам приложений создавать различные типы приложений IoT, но также поддерживает пользователей для настройки 
служб, и поставщики услуг для управления ресурсами в соответствии с условиями системы.  
4.4. Методы классификации туманных платформ 
Для формирования единого подхода к классификации туманных платформ, нами 
были рассмотрены ключевые туманные платформы и их ключевые характеристики. 
Так, например, AWS Greengrass способна работать без доступа к публичному облаку1, 
однако в таком режиме работы возможно лишь хранение локальных данных. Центральное управление устройствами, а также централизованный сбор и обработка данных становится невозможен. Для полноценного функционирования платформы требуется доступ до AWS IoT Core, выступающий центральным сервисом для управления и организации работы тумана и который является публичным облаком. 
Azure IoT также может работать в частных сетях2, но при условии, что внутри 
частной сети будет шлюз, который должен подключаться к центральному узлу управления и сбора данных, и данный узел также является публичным облаку. Отличие от 
обычной организации работы тумана с публичным облаком заключается в наличии единой точки выхода во внешнюю сеть, а не множества различных шлюзов, которые общаются с публичным облаком. 
Другие публичные туманные платформы имеют те же ограничения в отличие от 
частных туманных платформ и платформ с открытым исходным кодом, центральный 
узел управления которых может быть развернут на любом сервере в локальной сети или 
отсутствовать вовсе (в таком случае задачи управления и оркестрации разделяются 
между промежуточными узлами тумана, как это сделано, например, у FogFlow2.0). 
Поэтому все туманные платформы можно классифицировать по признаку открытости или закрытости развертывания хаба (Hub) — сервиса, который отвечает за 
подключение, мониторинг и управление подключенными в туман устройствами. В том 
или ином виде хаб имеют практически все коммерческие туманные платформы: 
LoopEdge и Azure IoT так и называют данный сервис — Hub. Платформы ClearBlade и 
                                         
1https://aws.amazon.com/ru/greengrass/faqs/#Local_Resource_Access 
2https://azure.microsoft.com/en-us/blog/introducing-iot-hub-device-streams-in-public-preview/ 
А.А. Кирсанова, Г.И. Радченко, А.Н. Черных
2020, т. 9, №3
53
FogHorn имеют сервис с тем же функционалом, но называется он — Device Manager. У 
AWS Greengrass этот сервис называется AWS IoT Core. 
Однако управление устройствами имеют только те системы, которые имеют Edge 
подсистемы. В остальных случаях туман оперирует не устройствами, а развернутыми 
вычислительными узлами, например, AWS IoT Greengrass Core1, которые разворачиваются на устройстве, где это возможно, но о самом устройстве ничего неизвестно, взаимодействие идет с виртуализированным узлом на этом устройстве. 
Также в предыдущих разделах рассматривались платформы, которые в некоторых 
случаях могли быть развернуты только на определенном перечне устройств, поставляемых самой платформой. Другие же туманные платформы не имели данного ограничения и могли работать с оборудованием пользователя (если оно удовлетворяет требованиям развертывания платформы). Это показатель классификации туманных платформ 
по признаку открытости или закрытости аппаратной инфраструктуры. 
Тот же принцип наблюдается и при сравнении платформ по признаку открытости или закрытости программной инфраструктуры: платформа может поддерживать открытые протоколы обмена данными между узлами тумана или же туманные 
программы поставляются исключительно разработчиками самой платформы и лицензированными партнерами. 
Таким образом, любую туманную платформу можно классифицировать по принципу открытости или закрытости ее компонентов (см. рис. 2). Также стоит отметить, что 
платформы с публичным хабом в большей степени стремятся к открытости своих аппаратных и программных инфраструктур. 
 
 
Рис. 2. Классификация туманных платформ по принципу открытости или закрытости 
ее компонентов 
                                         
1AWS IoT Core — это центральная точка управления туманом. AWS IoT Greengrass Core — это 
виртуализируемая система, разворачиваемая на устройстве, которое должно выступать вычислительным узлом. 
Обзор технологий организации туманных вычислений
54
Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»
Помимо открытости или закрытости своих компонентов некоторые платформы делали упор на доступность различных предоставляемых возможностей или сервисов, которыми обладает платформа. Azure IoT Hub, который является неотъемлемой частью 
платформы Azure IoT, явно называет себя PaaS (Platform as a Service), предоставляя 
готовые решения для реализации требуемых задач пользователя. Стоит отметить, что 
ни одна из публичных туманных платформ не позиционирует свои платформы как чисто туманные. Они предоставляют туманные вычисления как некий базовый функционал, который лежит в основе остальных предоставляемых функций и сервисов платформы. 
Таким образом, сами платформы позиционируют некоторый функционал как базовый, который должен быть в любой туманной платформе, а пользователь заинтересован 
уже не только в простом развертывании и базовом управлении туманными узлами, а в 
решении своих конкретных задач: Индустрия 4.0, медицина, умный город и т.п. Платформы должны максимально предоставить готовые решения для каждой из задач пользователя. 
Помимо прочего, некоторые платформы позволили пользователям обмениваться 
собственными готовыми решениями, созданными в рамках платформы при помощи «магазинов» — ресурсов, где пользователь может опубликовать свое готовое туманное приложение. Это привело к появлению целых туманных экосистем — EaaS (Ecosystem as a 
Service), которые позволяют создать собственное туманное решение из готовых компонентов, доступных на платформе. 
Под данное описание также попадают и Open Source решения, которые предоставляют лишь базовый уровень функционала — DaaS (Deploy as a Service): разворачивание туманных узлов на имеющихся устройствах, оркестрацию и т.п. С другой стороны, 
FogFlow имеет более широкий функционал и даже собственную экосистему, в которую 
включены готовые к установке компоненты как от разработчиков платформы1, так и от 
сообщества. 
Классификацию «as a service» можно привести как способ классификации на 
основе предоставляемого функционала платформы (см. рис. 3). 
Заключение 
Увеличение передаваемых объемов данных и повышенная нагрузка на облако для 
работы клиентских сервисов стали предпосылкой к появлению концепции туманных вычислений. В данной работе была рассмотрена концепция туманных вычислений, их 
определение и ключевые характеристики. Также были рассмотрены, классифицированы 
и обобщены некоторые туманные платформы, которые являются предметами исследования или уже используются бизнесом и частными клиентами. В конце были описаны 
общие архитектурные характеристики присущие всем рассмотренным платформам. 
Туманные вычисления являются более гибким и эффективным видом вычислений 
по сравнению с облачными за счет решения задач, требующих высокой пропускной способности вычислительной сети, возможности работы с географически рассредоточенными источниками данных, сверхнизкими задержками и обеспечением локальности обработки данных. 
                                         
1https://fogflow.readthedocs.io/en/latest/setup.html 
А.А. Кирсанова, Г.И. Радченко, А.Н. Черных
2020, т. 9, №3
55
 
Рис. 3. Классификация туманных платформ на основе предоставляемого функционала 
платформы 
Исследование выполнено при финансовой поддержке РФФИ в рамках научного проекта No 18-07-01224 a и при финансовой поддержке Министерства науки и высшего 
образования РФ (государственное задание FENU-2020-0022). 
Литература 
1. 
Al-Doghman F., Chaczko Z., Ajayan A.R., et al. A review on Fog Computing Technology // Conference Proceedings of 2016 IEEE International Conference on Systems, Man, 
and Cybernetics, SMC 2016 (Budapest, Hungary, October, 9–12, 2017). Institute 
of 
Electrical 
and 
Electronics 
Engineers 
Inc., 
2017. 
Р. 
1525–1530. 
DOI: 
10.1109/SMC.2016.7844455. 
2. 
Antonio S. Cisco Delivers Vision of Fog Computing to Accelerate Value from Billions of 
Connected Devices 2014. P. 1–4. 
3. 
Armbrust M., Fox A., Griffith R., et al. A View of Cloud Computing // Communications of the ACM. 2010. Vol. 53, no. 4. P. 50–58. DOI: 10.1145/1721654.1721672. 
4. 
Bonomi F., Milito R., Zhu J., et al. Fog Computing and Its Role in the Internet of 
Things // Proceedings of the 1st ACM Mobile Cloud Computing Workshop, MCC’12 
(Helsinki, 
Finland, 
August, 
17, 
2012). 
ACM 
Press, 
2012. 
Р. 13–15. 
DOI: 10.1145/2342509.2342513. 
5. 
Brito M.S.D., Hoque S., Magedanz T., et al. A Service Orchestration Architecture for 
Fog-enabled Infrastructures // 2017 2nd International Conference on Fog and Mobile 
Edge Computing, FMEC 2017 (Valencia, Spain, May, 8–11, 2017). Institute of Electrical 
and Electronics Engineers Inc., 2017. Р. 127–132. DOI: 10.1109/FMEC.2017.7946419. 
6. 
Brynjolfsson E., Hofmann P., Jordan J. Cloud Computing and Electricity: Beyond the 
Utility Model // Communications of the ACM. 2010. Vol. 53, no. 5. P. 32–34. DOI: 
10.1145/1735223.1735234. 
Обзор технологий организации туманных вычислений
56
Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»
7. 
Celesti A., Fazio M., Márquez F.G., et al. How to Develop IoT Cloud E-Health Systems 
Based on Fiware: A Lesson Learnt // Journal of Sensor and Actuator Networks. 2019. 
Vol. 8, no. 1. DOI: 10.3390/jsan8010007.  
8. 
Cheng B., Solmaz G., Cirillo F., et al. FogFlow: Easy Programming of IoT Services Over 
Cloud and Edges for Smart Cities // IEEE Internet of Things Journal. 2018. Vol. 5, 
no. 2. P. 696–707. DOI: 10.1109/JIOT.2017.2747214. 
9. 
Chiang M., Ha S., Chih-Lin I., et al. Clarifying Fog Computing and Networking: 10 
Questions and Answers // IEEE Communications Magazine. 2017. Vol. 5, no. 4. P. 18–
20. DOI: 10.1109/MCOM.2017.7901470. 
10. Dantas L., Cavalcante E., Batista T. A Development Environment for FIWARE-Based 
Internet of Things Applications // Proceedings of the 2019 Workshop on Middleware 
and Applications for the Internet of Things, Part of Middleware 2019 Conference, M4IoT 
2019 (Davis CA, USA, December, 7–11, 2019). Association for Computing Machinery, 
Inc, 2019. Р. 21–26. DOI: 10.1145/3366610.3368100. 
11. Dar B.K., Shah M.A., Islam S.U., et al. Delay-Aware Accident Detection and Response 
System Using Fog Computing // IEEE Access. 2019. Vol. 7. P. 70975–70985. DOI: 
10.1109/ACCESS.2019.2910862. 
12. Donno M. De, Tange K., Dragoni N. Foundations and Evolution of Modern Computing 
Paradigms: Cloud, IoT, Edge, and Fog // IEEE Access. 2019. Vol. 7. P. 150936–150948. 
DOI: 10.1109/ACCESS.2019.2947652. 
13. Eder M. Hypervisor-vs. Container-Based Virtualization // Future Internet (FI) and Innovative Internet Technologies and Mobile Communications (IITM). 2016. Vol. 1. 
14. Emeras J., Varrette S., Bouvry P. Amazon Elastic Compute Cloud (EC2) vs. In-house 
HPC Platform: A Cost Analysis // IEEE International Conference on Cloud Computing, 
CLOUD 
(Honolulu, 
USA, 
June, 
25–30, 
2017). 
IEEE, 
2017. 
Р. 284–293. 
DOI: 10.1109/CLOUD.2016.44. 
15. Eugene G. Cloud Computing Models. 2013. 
16. Evans D. The Internet of Things: How the Next Evolution of the Internet is Changing 
Every Thing // CISCO white paper. 2011. Vol. 1. P. 1–11. 
17. Fazio M., Celesti A., Marquez F.G., et al. Exploiting the FIWARE Cloud Platform to 
Develop a Remote Patient Monitoring System // Proceedings of IEEE Symposium on 
Computers and Communications, 2016. Institute of Electrical and Electronics Engineers 
Inc., 2016. Р. 264–270. DOI: 10.1109/ISCC.2015.7405526. 
18. Feeney G.J. Utility computing — A Superior Alternative? // AFIPS Conference Proceedings — 1974 National Computer Conference, AFIPS 1974 (Chicago, Illinois, USA, 
May, 6–10, 1974). ACM Press, 1974. Р. 1003–1004. DOI: 10.1145/1500175.1500370. 
19. FIWARE "About us". 2015. URL: https://www.fiware.org/about-us/ (дата обращения: 
03.03.2020). 
20. Foster I., Kesselman C. The History of the Grid // Advances in Parallel Computing. 
2011. Vol. 20. P. 3–30. DOI: 10.3233/978-1-60750-803-8-3. 
21. Garcia J., Simo E., Masip-Bruin X., et al. Do We Really Need Cloud? Estimating the 
Fog Computing Capacities in the City of Barcelona // Proceedings of the 11th 
IEEE/ACM International Conference on Utility and Cloud Computing Companion, UCC 
Companion 2018 (Zurich, Switzerland, December, 17–20, 2019). 2019. Р. 290–295. 
DOI: 10.1109/UCC-Companion.2018.00070. 
22. GE 
Digital 
What 
is 
Edge 
Computing? 
| 
GE 
Digital. 
2018. 
URL: 
https://www.ge.com/digital/blog/what-edge-computing (дата обращения: 03.03.2020). 
А.А. Кирсанова, Г.И. Радченко, А.Н. Черных
2020, т. 9, №3
57
23. Gu L., Zeng D., Guo S., et al. Cost Efficient Resource Management in Fog Computing 
Supported Medical Cyber-Physical System // IEEE Transactions on Emerging Topics in 
Computing. 2017. Vol. 5, no. 1. P. 108–119. DOI: 10.1109/TETC.2015.2508382. 
24. Guharoy R., Sur S., Rakshit S., et al. A Theoretical and Detail Approach on Grid Computing. A Review on Grid Computing Applications // 2017 8th Industrial Automation 
and Electromechanical Engineering Conference, IEMECON 2017 (Bangkok, Thailand, 
August, 16–18, 2017). Institute of Electrical and Electronics Engineers Inc., 2017. 
Р. 142–146. DOI: 10.1109/IEMECON.2017.8079578. 
25. Guth J., Breitenbucher U., Falkenthal M., et al. Comparison of IoT Platform Architectures: A Field Study Based on a Reference Architecture // 2016 Cloudification of the Internet of Things, CIoT 2016, 2017. Institute of Electrical and Electronics Engineers Inc., 
2017. DOI: 10.1109/CIOT.2016.7872918. 
26. Hagiu A., Wright J. When Data Creates Competitive Advantage...And When It Doesn’t 
// Harvard Business Review. 2020. Vol. 98, no. 1. P. 94–101. 
27. Hannabuss S. The Big Switch: Rewiring the World, from Edison to Google // Library 
Review. 2009. Vol. 58, no. 2. P. 136–137. 
28. Haouari F., Faraj R., Alja’Am J.M. Fog Computing Potentials, Applications, and Challenges // 2018 International Conference on Computer and Applications, ICCA 2018 
(Beirut, 
Lebanon, 
July, 
25–26, 
2018). 
IEEE, 
2018. 
Р. 399–406. 
DOI: 10.1109/COMAPP.2018.8460182. 
29. Hashemi S.M., Bardsiri A.K. Cloud Computing vs. Grid Computing // ARPN Journal of 
Systems and Software. 2012. Vol. 2, no. 5. 
30. Hilbrich M., Frank M. Abstract Fog in the Bottle - Trends of Computing in History and 
Future // Proceedings of the 44th Euromicro Conference on Software Engineering and 
Advanced Applications, SEAA 2018 (Prague, Czech Republic, August, 29–31, 2018). Institute 
of 
Electrical 
and 
Electronics 
Engineers 
Inc., 
2018. 
Р. 519–522. 
DOI: 10.1109/SEAA.2018.00089. 
31. Hofmann P., Woods D. Cloud Computing: The Limits of Public Clouds for Business Applications // IEEE Internet Computing. 2010. Vol. 14, no. 6. P. 90–93. DOI: 
10.1109/MIC.2010.136. 
32. Hong C.H., Varghese B. Resource Management in Fog/Edge Computing: A Survey on 
Architectures, Infrastructure, and Algorithms // ACM Computing Surveys. 2019. 
Vol. 52, no. 5. P. 1–37. DOI: 10.1145/3326066. 
33. Hong H.J. From Cloud Computing to Fog Computing: Unleash the Power of Edge and 
End Devices // Proceedings of the International Conference on Cloud Computing Technology and Science, CloudCom (Hong Kong, Hong Kong, December, 11–14, 2017). IEEE 
Computer Society, 2017. Р. 331–334. DOI: 10.1109/CloudCom.2017.53. 
34. Huang C., Lu R., Choo K.K.R. Vehicular Fog Computing: Architecture, Use Case, and 
Security and Forensic Challenges // IEEE Communications Magazine. 2017. Vol. 55, 
no. 11. P. 105–111. DOI: 10.1109/MCOM.2017.1700322. 
35. Hughes I., Immerman D., Daly P. ClearBlade Demonstrates Scalability and Edge Analytics With IoT Platform, 2017. 
36. Iorga M., Feldman L., Barton R., et al. Fog Computing Conceptual Model. 
Gaithersburg, MD, 2018. 
37. Jalali F., Hinton K., Ayre R., et al. Fog Computing May Help to Save Energy in Cloud 
Computing // IEEE Journal on Selected Areas in Communications. 2016. Vol. 34, no. 5. 
P. 1728–1739. DOI: 10.1109/JSAC.2016.2545559. 
Обзор технологий организации туманных вычислений
58
Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»
38. Jiang Y., Huang Z., Tsang D.H.K.K. Challenges and Solutions in Fog Computing Orchestration 
// 
IEEE 
Network. 
2018. 
Vol. 32, 
no. 3. 
P. 122–129. 
DOI: 
10.1109/MNET.2017.1700271. 
39. Kakakhel S.R.U., Mukkala L., Westerlund T., et al. Virtualization at the Network Edge: 
A Technology Perspective // 2018 3rd International Conference on Fog and Mobile Edge 
Computing, FMEC 2018 (Barcelona, Spain, April, 23–26, 2018). Institute of Electrical 
and Electronics Engineers Inc., 2018. Р. 87–92. DOI: 10.1109/FMEC.2018.8364049. 
40. Kokkinou A., Cranage D.A. Using Self-Service Technology to Reduce Customer Waiting 
Times // International Journal of Hospitality Management. 2013. Vol. 33, no. 1. P. 435–
445. DOI: 10.1016/j.ijhm.2012.11.003. 
41. Kumar R., Charu S. Comparison Between Cloud Computing, Grid Computing, Cluster 
Computing and Virtualization // International Journal of Modern Computer Science and 
Applications. 2015. Vol. 8, no. 31. P. 2321–2632. DOI: 10.13140/2.1.1759.7765. 
42. Lee J. A View of Cloud Computing // International Journal of Networked and Distributed Computing. 2013. Vol. 1, no. 1. P. 2–8. DOI: 10.2991/ijndc.2013.1.1.2. 
43. Li C., Xue Y., Wang J., et al. Edge-Oriented Computing Paradigms: A Survey on Architecture Design and System Management // ACM Computing Surveys. 2018. Vol. 51, 
no. 2. DOI: 10.1145/3154815. 
44. Liu L., Wang Y., Yang Y., et al. Utility-Based Computing Model for Grid // Proceedings of the 1st International Conference on Semantics, Knowledge and Grid, SKG 2005 
(Beijing, China, November, 27–29, 2005). 2005. Р. 109–109. DOI: 10.1109/SKG.2005.140. 
45. Liu Y., Fieldsend J.E., Min G. A Framework of Fog Computing: Architecture, Challenges, and Optimization // IEEE Access. 2017. Vol. 5. P. 25445–25454. DOI: 
10.1109/ACCESS.2017.2766923. 
46. Madsen H., Albeanu G., Burtschy B., et al. Reliability in the Utility Computing Era: 
Towards Reliable Fog Computing // International Conference on Systems, Signals, and 
Image Processing (Rio de Janeiro, Brazil, June, 3–5, 2013). IEEE Computer Society, 
2013. Р. 43–46. DOI: 10.1109/IWSSIP.2013.6623445. 
47. Mahmood Z., Ramachandran M. Fog Computing: Concepts, Principles and Related Paradigms // Springer International Publishing, 2018. P. 3–21. DOI: 10.1007/978-3-31994890-4_1. 
48. Mahmoudi C., Mourlin F., Battou A. Formal Definition of Edge Computing: An Emphasis on Mobile Cloud and IoT Composition // 2018 3rd International Conference on Fog 
and Mobile Edge Computing, FMEC 2018 (Barcelona, Spain, April, 23–26, 2018). Institute 
of 
Electrical 
and 
Electronics 
Engineers 
Inc., 
2018. 
Р. 34–42. 
DOI: 10.1109/FMEC.2018.8364042. 
49. McAfee A., Brynjolfsson E. Big Data: The Management Revolution // Harvard Business 
Review. 2012. Vol. 90, no. 10. P. 4. 
50. Mell P., Grance T. The NIST Definition of Cloud Computing: Recommendations of the 
National Institute of Standards and Technology. 2012. P. 97–101. 
51. Naha R.K., Garg S., Georgakopoulos D., et al. Fog Computing: Survey of Trends, Architectures, Requirements, and Research Directions // IEEE Access. 2018. Vol. 6. P. 47980–
48009. DOI: 10.1109/ACCESS.2018.2866491. 
52. Nakagawa M., Hasegawa H., Sato K., et al. Adaptive Self-Reconfigurable Network to 
Create Cost-Effective Bandwidth-on-Demand Services // Optics InfoBase Conference 
Papers 
(Rochester, 
NY, 
United 
States, 
October, 
24–28, 
2010). 
2010. 
DOI: 10.1364/nfoec.2010.nwa2. 
А.А. Кирсанова, Г.И. Радченко, А.Н. Черных
2020, т. 9, №3
59
53. Narula S., Jain A. Prachi Cloud Computing Security: Amazon Web Service // International Conference on Advanced Computing and Communication Technologies, ACCT 
(Rohtak, Haryana, India, February, 21–22, 2015). Institute of Electrical and Electronics 
Engineers Inc., 2015. Р. 501–505. DOI: 10.1109/ACCT.2015.20. 
54. OpenFog Consortium Architecture Working Group OpenFog Reference Architecture for 
Fog Computing. 2017. 
55. Pinchuk A., Sokolov N., Freinkman V. General principles of foggy computing // LastMile. 2018. no. 3. P. 38–45. DOI: 10.22184/2070-8963.2018.72.3.38.45. 
56. Proferansov D.Y., Safonova I.E. To the Question of Fog Computing and the Internet of 
Things // Educational Resources and Technology. 2017. Vol. 4, no. 21. P. 30–39. 
57. Puliafito C., Mingozzi E., Vallati C., et al. Virtualization and Migration at the Network 
Edge: An Overview // Proceedings of 2018 IEEE International Conference on Smart 
Computing, SMARTCOMP 2018 (Taormina, Sicily, Italy, June, 18–20, 2018). Institute 
of 
Electrical 
and 
Electronics 
Engineers 
Inc., 
2018. 
Р. 368–374. 
DOI: 10.1109/SMARTCOMP.2018.00031. 
58. Radchenko G.I., Alaasam A.B.A., Tchernykh A.N. Comparative Analysis of Virtualization Methods in Big Data Processing // Supercomputing Frontiers and Innovations. 
2019. Vol. 6, no. 1. P. 48–79. DOI: 10.14529/jsfi190107. 
59. Ravandi B., Papapanagiotou I. A Self-Learning Scheduling in Cloud Software Defined 
Block Storage // IEEE International Conference on Cloud Computing, CLOUD (Honolulu, Hawaii, USA, June, 25–July, 1, 2017). IEEE Computer Society, 2017. Р. 415–422. 
DOI: 10.1109/CLOUD.2017.60. 
60. Reinsel D., Gantz J. Extracting Value from Chaos // IDC Report. 2011. Vol. 1142. P. 1–
12. 
61. Sadashiv N., Kumar S.M.D. Cluster, Grid and Cloud Computing: A Detailed Comparison // Final Program and Proceedings of the 6th International Conference on Computer 
Science and Education, ICCSE 2011 (Chennai, India, December, 14–15, 2011). IEEE, 
2011. Р. 477–482. DOI: 10.1109/ICCSE.2011.6028683. 
62. Sehgal N.K., Bhatt P.C.P., Sehgal N.K., et al. Features of Private and Public Clouds 
Cham: Springer International Publishing, 2018. P. 51–60. 
63. Skarlat O., Karagiannis V., Rausch T., et al. A Framework for Optimization, Service 
Placement, and Runtime Operation in the Fog // Proceedings of the 11th IEEE/ACM 
International Conference on Utility and Cloud Computing, UCC 2018 (Zurich, Switzerland, December, 17–20, 2019). Institute of Electrical and Electronics Engineers Inc., 
2019. Р. 164–173. DOI: 10.1109/UCC.2018.00025. 
64. Skarlat O., Nardelli M., Schulte S., et al. Optimized IoT Service Placement in the Fog // 
Service Oriented Computing and Applications. 2017. Vol. 11, no. 4. P. 427–443. DOI: 
10.1007/s11761-017-0219-8. 
65. Skarlat O., Schulte S., Borkowski M., et al. Resource Provisioning for IoT Services in the 
Fog // Proceedings of 2016 IEEE 9th International Conference on Service-Oriented 
Computing and Applications, SOCA 2016 (Macau, China, November, 4–6, 2016). Institute 
of 
Electrical 
and 
Electronics 
Engineers 
Inc., 
2016. 
Р. 32–39. 
DOI: 10.1109/SOCA.2016.10. 
66. Smirnov Y. Cloud computing. The History and Impact on Libraries’ Future // Scientific 
and Technical Libraries. 2016. no. 6. P. 62–73. DOI: DOI: 10.33186/1027-3689-2016-6-6273. 
Обзор технологий организации туманных вычислений
60
Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»
67. Sotomayor B., Montero R.S., Llorente I.M., et al. Virtual Infrastructure Management in 
Private and Hybrid Clouds // IEEE Internet Computing. 2009. Vol. 13, no. 5. P. 14–22. 
DOI: 10.1109/MIC.2009.119. 
68. Taleb T., Samdanis K., Mada B., et al. On Multi-Access Edge Computing: A Survey of 
the Emerging 5G Network Edge Cloud Architecture and Orchestration // IEEE Communications Surveys and Tutorials. 2017. Vol. 19, no. 3. P. 1657–1681. DOI: 
10.1109/COMST.2017.2705720. 
69. Tseng F.H., Tsai M.S., Tseng C.W., et al. A Lightweight Autoscaling Mechanism for 
Fog Computing in Industrial Applications // IEEE Transactions on Industrial Informatics. 2018. Vol. 14, no. 10. P. 4529–4537. DOI: 10.1109/TII.2018.2799230. 
70. Tuli S., Basumatary N., Buyya R. EdgeLens: Deep Learning Based Object Detection in 
Integrated IoT, Fog and Cloud Computing Environments // Proceedings of the 4th 
IEEE International Conference on Information Systems and Computer Networks, ISCON 
2019 (Mathura, India, November 21–22, 2019). IEEE Press, USA, 2019. P. 496–502. 
DOI: 10.1109/ISCON47742.2019.9036216. 
71. Tuli S., Mahmud R., Tuli S., et al. FogBus: A Blockchain-Based Lightweight Framework 
for Edge and Fog Computing // Journal of Systems and Software. 2019. Vol. 154. P. 22–
36. DOI: 10.1016/j.jss.2019.04.050. 
72. Vandenberg A. Grid Computing for All // Charles River Media, 2005. Р. 3. 
DOI: 10.1145/1167350.1167353. 
73. Velasquez K., Abreu D.P., Assis M.R.M., et al. Fog Orchestration for the Internet of 
Everything: State-of-the-Art and Research Challenges // Journal of Internet Services 
and Applications. 2018. Vol. 9, no. 1. DOI: 10.1186/s13174-018-0086-3. 
74. Wadhwa H., Aron R. Fog Computing with the Integration of Internet of Things: Architecture, Applications and Future Directions // Proceedings of the 16th IEEE International Symposium on Parallel and Distributed Processing with Applications, 17th IEEE 
International Conference on Ubiquitous Computing and Communications, 8th IEEE International Conference on Big Data and Cloud Computing, (Melbourne, Australia, December, 11–13, 2019). IEEE, 2019. Р. 987–994. DOI: 10.1109/BDCloud.2018.00144. 
75. Webb K. Reviews. Architects of the Information Society: 35 Years of the Laboratory for 
Computer Science at MIT // Internet Research. 2000. Vol. 10, no. 1. P. 169–174. 
76. Weinhardt C., Anandasivam A., Blau B., et al. Cloud Computing — A Classification, 
Business Models, and Research Directions // Business & Information Systems Engineering. 2009. Vol. 1, no. 5. P. 391–399. DOI: 10.1007/s12599-009-0071-2. 
77. Wen Z., Yang R., Garraghan P., et al. Fog Orchestration for Internet of Things Services 
// IEEE Internet Computing. 2017. Vol. 21, no. 2. P. 16–24. DOI: 10.1109/MIC.2017.36. 
78. Wood T., Ramakrishnan K.K., Shenoy P., et al. CloudNet: Dynamic Pooling of Cloud 
Resources by Live WAN Migration of Virtual Machines // IEEE/ACM Transactions on 
Networking. 2015. Vol. 23, no. 5. P. 1568–1583. DOI: 10.1109/TNET.2014.2343945. 
79. Yang J., Pang J., Qi N., et al. On-Demand Self-Adaptivity of Service Availability for 
Cloud Multi-Tier Applications // Proceedings of 2015 IEEE/ACM 15th International 
Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2015, 2015. Institute of 
Electrical 
and 
Electronics 
Engineers 
Inc., 
2015. 
Р. 1237–1240. 
DOI: 10.1109/CCGrid.2015.146. 
80. Yousefpour A., Fung C., Nguyen T., et al. All One Needs to Know about Fog Computing and Related Edge Computing Paradigms: A Complete Survey // Journal of Systems 
Architecture. 2019. Vol. 98. P. 289–330. DOI: 10.1016/j.sysarc.2019.02.009. 
А.А. Кирсанова, Г.И. Радченко, А.Н. Черных
2020, т. 9, №3
61
81. Zhang B., Mor N., Kolb J., et al. The Cloud is Not Enough: Saving IoT from the Cloud 
// 7th USENIX Workshop on Hot Topics in Storage and File Systems, HotStorage 2015, 
2020. 
82. Zhang P., Liu J. K., Richard Yu F., et al. A Survey on Access Control in Fog Computing // IEEE Communications Magazine. 2018. Vol. 56, no. 2. P. 144–149. DOI: 
10.1109/MCOM.2018.1700333. 
83. Zlatanov N. The Data Center Evolution from Mainframe to Cloud // IEEE Computer 
Society. 2016. DOI: 10.13140/RG.2.1.4103.8489. 
84. A 
Guide 
to 
Edge 
IoT 
Analytics: 
Internet 
of 
Things 
Blog. 
URL: 
https://www.ibm.com/blogs/internet-of-things/edge-iot-analytics/ 
(дата 
обращения: 
02.03.2020). 
85. Edge 
Gateway 
| 
Smartiply. 
URL: 
https://www.smartiply.com/gateway 
(дата 
обращения: 02.03.2020). 
86. Fog computing brings new business opportunities and disruptions - IoT Agenda. URL: 
https://internetofthingsagenda.techtarget.com/blog/IoT-Agenda/Fog-computing-bringsnew-business-opportunities-and-disruptions (дата обращения: 27.02.2020). 
87. FogFlow — FogFlow v2.0 documentation. URL: https://fogflow.readthedocs.io/en/ 
latest/ (дата обращения: 03.03.2020). 
88. Mobile Platform | Smartiply. URL: https://www.smartiply.com/mobile (дата обращения: 02.03.2020). 
89. Nebbiolo Technologies — Pioneers in Fog Computing. URL: https://www.nebbiolo.tech/ 
(дата обращения: 02.03.2020). 
90. Smartfog/Fogflow: FogFlow is a Standard-Based IoT Fog Computing Framework that 
Supports Serverless Computing and Edge Computing with Advanced Programming 
Models. URL: https://github.com/smartfog/fogflow (дата обращения: 03.03.2020). 
91. Softls/FogFrame-2.0: 
FogFrame 
Framework 
(with 
Extensions). 
URL: 
https://github.com/softls/FogFrame-2.0 (дата обращения: 02.03.2020). 
92. The Industrial Internet Consortium and Openfog Consortium Join Forces | Industrial 
Internet Consortium. URL: https://www.iiconsortium.org/press-room/01-31-19.htm (дата обращения: 27.02.2020). 
93. Toshiba Digital Solutions Corporation and Nebbiolo Technologies Inc. Sign an Industrial 
IoT Strategic Partnership Agreement. URL: https://www.prnewswire.com/newsreleases/toshiba-digital-solutions-corporation-and-nebbiolo-technologies-inc-sign-anindustrial-iot-strategic-partnership-agreement-300632595.html 
(дата 
обращения: 
02.03.2020). 
 
Кирсанова Александра Александровна, научный сотрудник кафедры электронных 
вычислительных машин, Южно-Уральский государственный университет (национальный исследовательский университет) (Челябинск, Российская Федерация) 
Радченко Глеб Игоревич, к.ф.-м.н., доцент, директор Высшей школы электроники и 
компьютерных наук, заведующий кафедрой электронных вычислительных машин, Южно-Уральский государственный университет (национальный исследовательский университет) (Челябинск, Российская Федерация) 
Черных Андрей Николаевич, к.т.н., доцент, заведующий научно-исследовательской 
лабораторией проблемно-ориентированных облачных сред, Южно-Уральский государственный университет (национальный исследовательский университет) (Челябинск, Российская Федерация), профессор, научно-исследовательский центр Энсенады (Энсенада, 
Мексика) 
Обзор технологий организации туманных вычислений
62
Вестник ЮУрГУ. Серия «Вычислительная математика и информатика»
DOI: 10.14529/cmse200303 
OVERVIEW OF FOG COMPUTING ORGANIZATION 
TECHNOLOGIES 
 
© 2020 A.A. Kirsanova1, G.I. Radchenko1, A.N. Chernykh1,2 
1South Ural State University (pr. Lenina 76, Chelyabinsk, 454080 Russia) 
2Ensenada Research Center 
(Carretera Ensenada - Tijuana No. 3918, Ensenada, 22860 Mexico) 
E-mail: alexander.a.kirsanov@susu.ru, gleb.radchenko@susu.ru, chernykh@cicese.mx 
Received: 14.06.2020 
 
As the Internet of Things (IoT) becomes a part of our daily life, there is a rapid growth in the number of 
connected devices. A well-established approach based on cloud computing technologies cannot provide the necessary quality of service in such an environment, particularly in terms of reducing data latency. Today, fog computing technology is seen as a promising solution for processing large amounts of critical and time-sensitive data. 
This article reviews cloud computing technology and analyzes the prerequisites for the evolution of this approach 
and the emergence of the concept of fog computing. As part of an overview of the key features of fog computing, 
we analyze the frequent confusion with the fusion of the concepts of fog and edge computing. The paper provides 
an overview of fog computing technologies: virtualization, containerization, and orchestration, as well as a systematic analysis of the most popular platforms that support fog computing. As a result of the analysis, we offer two 
approaches to the classification of fog-computing platforms: on the principle of openness/closure of components, as 
well as three-level classification based on the provided platform functionality (Deploy-, Platform- and Ecosystem 
as a Service).  
Keywords: cloud computing, fog computing, edge computing, internet of things. 
FOR CITATION 
Kirsanova A.A., Radchenko G.I., Chernykh A.N. Overview of Fog Computing Organization Technologies. Bulletin of the South Ural State University. Series: Computational Mathematics and Software Engineering. 
2020. 
Vol. 9, 
no. 3. 
P. 35–63. 
(in 
Russian) 
DOI: 10.14529/cmse200303. 
 
This paper is distributed under the terms of the Creative Commons Attribution—NonCommercial 3.0 License which permits non-commercial use, reproduction and distribution of 
the work without further permission provided the original work is properly cites. 
А.А. Кирсанова, Г.И. Радченко, А.Н. Черных
2020, т. 9, №3
63

"Теория и практика современной науки" №5(35) 2018 
1029 
 
УДК 37.012 
Аджимуллаева Л.А. 
преподаватель 
Крымский филиал  
«Российский государственный университет правосудия»  
Россия, г. Симферополь 
ЭФФЕКТИВНОЕ ИСПОЛЬЗОВАНИЕ ОБЛАЧНЫХ ТЕХНОЛОГИЙ В 
СМЕШАННОМ ОБУЧЕНИИ 
Аннотация: 
облачные 
вычисления 
– 
новейшая 
технология, 
появившаяся в области ИТ и получившая широкое распространение в 
различных сферах человеческой деятельности. Эта технология помогает 
учебным заведениям наиболее эффективным способом перейти от 
традиционных методов обучения к более современным и результативным 
методам, в частности, к смешанному обучению. В статье приводится 
обзор различных моделей облачных технологий, таких как IaaS, CaaS, PaaS, 
FaaS и SaaS, в разрезе соответствия требованиям образовательных 
учреждений, а также рассматриваются преимущества, недостатки, риски 
и 
эффективность 
методов 
внедрения 
облачных 
вычислений 
в 
образовательной сфере. 
Ключевые 
слова: 
облачные 
вычисления, 
облачные 
технологии, 
смешанное обучение. 
 
Adzhimullaeva L.A. 
Teacher 
Crimean branch Russian State University of Justice 
Russia, Simferopol 
EFFECTIVE USE OF CLOUD TECHNOLOGIES IN BLENDED 
LEARNING 
Summary: cloud computing is the newest technology that has emerged in 
the field of IT and is widely used in various spheres of human activity. This 
technology helps educational institutions to move from traditional teaching 
methods to more up-to-date and productive ones, in particular to blended 
learning, in the most effective way. The article provides an overview of various 
cloud technology models, such as IaaS, CaaS, PaaS, FaaS and SaaS, in 
accordance with the requirements of educational institutions, and discusses the 
advantages, disadvantages, risks and effectiveness ofimplementation methods of 
cloud computing in the educational sphere. 
Keywords: cloud computing, cloud technologies, blended learning. 
 
Современное 
образование 
– 
это 
образование 
в 
области 
информационных технологий, которое отличается непрерывным и быстрым 
ростом 
требований 
к 
квалификации 
специалистов. 
Использование 
информационных технологий важно в целях интенсификации учебного 
"Теория и практика современной науки" №5(35) 2018 
1030 
 
процесса, повышения его эффективности и качества [1]. Одним из наиболее 
перспективных 
направлений 
развития 
информационных 
технологий 
являются облачные вычисления – новая компьютерная парадигма, 
основанная на глобальной сети Интернет, с внутренне присущей гибкостью 
и масштабируемостью для предоставления ИТ-услуг пользователям по 
требованию в режиме оплаты за фактическое использование [2]. Эта 
технология помогает учебным заведениям наиболее эффективным способом 
перейти от традиционных методов обучения к более современным и 
результативным, в частности к смешанному обучению.  
Облачные технологии можно разделять на 5 моделей. 
Инфраструктура как услуга (IaaS) – наиболее низкоуровневое решение 
в рамках облачных технологий. Данная модель подразумевает возможность 
использования только облачной инфраструктуры – пользователь может 
арендовать такие объекты как серверы, хранилища, сетевое оборудование и 
пр. Этот уровень позволяет управлять и настраивать облачные серверы 
аналогично физическим, что, в свою очередь, позволяет сократить нагрузку 
на обслуживание, модернизацию и расширение. 
Контейнер как услуга (CaaS) – в рамках данной модели виртуализация 
осуществляется на уровне контейнеров, что значительно снижает расходы в 
сравнении с виртуализацией на уровне операционной системы (IaaS). 
Платформа как услуга (PaaS) – позволяет клиенту арендовать готовые 
виртуальные серверы, необходимые для работы существующих приложений. 
В рамках данного подхода нет необходимости настраивать серверы, однако 
накладывается ряд жестких условий на типы поддерживаемых приложений: 
клиенты могут развертывать и контролировать приложения, но не имеют 
доступа к управлению аппаратным обеспечением, операционной системой, а 
также сетевой инфраструктурой. 
Функция как услуга (FaaS) представляет собой категорию облачных 
вычислительных сервисов, которая позволяет клиентам разрабатывать, 
запускать и управлять функциями приложений без сложности создания и 
поддержки инфраструктуры, обычно связанной с разработкой и запуском 
приложения. Создание приложения по этой модели является одним из 
способов достижения «бессерверной» архитектуры и обычно используется 
при создании микросервисов. 
Программное 
обеспечение 
как 
услуга 
(SaaS) 
– 
наиболее 
высокоуровневое 
решение, 
предоставляющее 
клиентам 
возможность 
использовать различные готовые приложения, которые работают в облачной 
инфраструктуре, 
такие 
как 
CRM/ERP-системы, 
приложения 
связи, 
электронной почты, и пр. В рамках данной модели клиенты не могут 
контролировать оборудование, сетевую инфраструктуру, операционную 
систему и программное обеспечение. Способ организации предоставляет ряд 
преимуществ, 
включая 
безопасность, 
быструю 
масштабируемость, 
совместимость программного обеспечения, глобальную доступность, а также 
"Теория и практика современной науки" №5(35) 2018 
1031 
 
надежность. 
Облачные технологии получили столь широкое распространение 
благодаря ряду ключевых преимуществ [3], в первую очередь – удобству и 
готовым решениям. К сервисам можно получить доступ из любого места и с 
любого устройства. Кроме того, пользователь не обязан загружать и 
устанавливать какое-либо программное обеспечение на своем устройстве. 
Единственное требование – устройство с доступом в Интернет и базовым 
набором приложений, таких как web-браузер, для непосредственного 
доступа к сервисам в облаке. Не менее важным преимуществом является 
экономия 
затрат 
– 
затраты 
на 
развертывание 
собственной 
ИТинфраструктуры всегда высоки. С внедрением облачных технологий 
расходы на ИТ-инфраструктуру сводятся к минимуму, в то же время 
обеспечивается легкий доступ к информации, а оплата производится по 
фактическому использованию ресурсов. Облачные технологии позволяют 
достичь 
оптимального 
использования 
программного 
и 
аппаратного 
обеспечения за счёт ресурсов провайдера, что повышает эффективность 
доступных ресурсов. 
Однако облачные технологии несут с собой дополнительные риски, 
связанные с безопасностью и доступностью [4]. Для учебных заведений 
важным аспектом является конфиденциальность персональных данных. Если 
данные находятся во внутренней сети учреждения, то принято считать, что 
они более защищены. Чтобы уменьшить риски, связанные с безопасностью, 
клиент может потребовать у провайдера предоставления дополнительных 
гарантий для обеспечения неразглашения конфиденциальных данных. Еще 
один риск использования облачных технологий связан с тем, что серверы 
провайдера могут быть недоступны по ряду причин. Для уменьшения рисков 
необходимо обговаривать требуемый уровень SLA (соглашение об уровне 
сервиса). Еще одна проблема, которая может возникнуть при переходе на 
облачные решения – недостаточная пропускная способность. Для ее 
предотвращения необходимо аппроксимировать заранее объем передаваемой 
информации и ожидаемое количество пользователей системы. 
Предварительный анализ показывает, что наиболее оптимальным 
способом внедрения облачных технологий в образовательный процесс 
является SaaS-платформа. Несомненно, этот подход обладает рядом 
достоинств и может в очень короткие сроки значительно повысить 
эффективность 
образовательного 
процесса. 
Однако 
в 
долгосрочной 
перспективе этот подход может привести к жесткой зависимости от 
конкретного поставщика приложений, что затруднит при необходимости  
переход на любую другую платформу. По мнению автора лучшим вариантом 
является подход, в рамках которого на уровне приложений разрабатываются 
свои решения и/или компонуются из уже имеющихся, с использованием 
IaaS-, CaaS- или PaaS-платформ для их развертывания. Это позволит снизить 
издержки на поддержания собственной IT-инфраструктуры и обеспечить 
"Теория и практика современной науки" №5(35) 2018 
1032 
 
простой и надежный доступ к программным сервисам, и в то же время 
позволит наиболее гибко использовать решения на уровне приложений и не 
зависеть от конкретного поставщика программных систем. 
Использованные источники: 
1. Ивашова О.Н., Яшкова Е.А. Применение облачных технологий в 
образовательном процессе // Электронный научный журнал «Наука и 
перспективы». – 2015. – №1. 
2. Mohssen M. Alabbadi. Cloud computing for education and learning: Education 
and learning as a service (ELaaS) // 14th International Conference on Interactive 
Collaborative Learning. – 2011. 
3. Стрекалова Н.Б. Облачные технологии в профессиональной подготовке 
современных специалистов // Научный диалог. – 2015. – №7 (43). 
4. Singh A. Cloud Computing for Academic Environment // International Journal 
of Information and Communication Technology Research. – 2012. – №2. 
 
УДК - 373 
Антоненко А.А. 
студент 5 курса  
направление подготовки «Педагогическое образование»  
профиль «Математика и физика» 
БГУ им. академика И.Г. Петровского 
Россия, г. Новозыбков 
ИСПОЛЬЗОВАНИЕ ПРИНЦИПА ДИФФЕРЕНЦИАЦИИ В 
ПРОЦЕССЕ ОБУЧЕНИЯ 
Аннотация: В статье рассмотрены методы, формы и схемы 
дифференцированного подхода к обучению. 
Ключевые слова: дифференциация, формы работы, обучение, схемы. 
 
Antonenko A.A. 
student 5 course 
direction of preparation "Pedagogical education» 
profile of "Mathematics and physics" 
BSU them. academician I.G. Petrovsky 
Russia, Novozybkov 
USING THE PRINCIPLE OF DIFFERENTIATION IN THE 
LEARNING PROCESS 
Annotation: The article describes the methods, forms and diagrams of a 
differentiated approach to teaching. 
Keywords: differentiation, forms of work, learning, scheme. 
 
В современном образовательном пространстве появилось большое 
множество образовательных заведений нового типа: лицеи, колледжи, 
гимназии 
и 
т.п., 
на 
основе 
которых 
реализуется 
внешний 
тип 

 
International Journal of Open Information Technologies ISSN: 2307-8162 vol. 11, no. 8, 2023 
 
 
17 
 
Управление контейнерами при построении 
распределенных систем с микросервисной 
архитектурой 
А. С. Бондаренко, К.С. Зайцев  
Аннотация. 
Целью 
настоящей 
работы 
является 
исследование возможности и способов применимости 
систем 
управления 
контейнерами 
для 
построения 
распределенных микросервисных архитектур, так как 
сегодня программисты и архитекторы всё чаще отдают 
предпочтение именно такой архитектуре. Причиной 
этого является широкое распространение облачных 
технологий, которые с каждым годом становятся все 
проще в настройке и управлении. Для достижения цели 
исследования 
в 
статье 
рассмотрены 
основные 
особенности микросервисной архитектуры, и проведена 
оценка 
применимости 
системы 
управления 
контейнерами на основе инструментов Kebernetes/ 
Openshift при построении распределенных архитектур. 
Так же был проведен эксперимент с демонстрацией 
эффективного управления топологией сети как одного 
из 
важнейших 
требований 
к 
разрабатываемым 
системам 
такого 
типа. 
Результаты 
применения 
предложенных 
инструментов 
подтвердили 
правильность использования систем управления и 
оркестрации 
контейнерами 
для 
построения 
микросервисов и продемонстрировали эффективность 
такого управления. 
 
Ключевые 
слова 
– 
микросервис, 
контейнер, 
микросервисная архитектура, облачные вычисления, 
программное обеспечение.  
I. ВВЕДЕНИЕ 
Понятие облака для современного человека уже 
давно 
ассоциируется 
с 
различными 
интернетсервисами, которые предоставляют различные услуги, 
помогая 
человеку 
закрыть 
различные 
информационные потребности. Облачные вычисления 
представляют 
собой 
различные 
вычислительные 
услуги, такие как серверы, хранилища, базы данных, 
сети, программное обеспечение, аналитику через 
Интернет-облако без активного прямого управления 
со стороны потребителя этих услуг. Вместо того, 
чтобы 
владеть 
собственной 
вычислительной 
инфраструктурой или центрами обработки данных, 
компании 
могут 
арендовать 
доступ 
ко 
всему 
вышеперечисленному у поставщика облачных услуг.  
Одним из преимуществ использования является то, 
что компании могут избежать первоначальных затрат 
и 
сложностей, 
связанных 
с 
владением 
и 
обслуживанием собственной ИТ-инфраструктуры, и 
вместо этого просто платить за то, что они 
используют, по требованию. 
Одним из преимуществ использования является то, 
что компании могут избежать первоначальных затрат 
и 
сложностей, 
связанных 
с 
владением 
и 
обслуживанием собственной ИТ-инфраструктуры, и 
вместо этого просто платить за то, что они 
используют, по требованию. 
В свою очередь, поставщики услуг облачных 
вычислений могут извлечь выгоду из значительной 
экономии за счет масштаба, предоставляя одни и те 
же услуги широкому кругу клиентов. Центральное 
место в построении облачных решений занимают 
контейнеры. 
В 
процессе 
проектирования 
современных 
информационных 
систем 
основная 
задача 
заключается в управлении сложностью системы. 
Чтобы 
система 
соответствовала 
классическим 
критериям качества ПО она должна быть хорошо 
организована, то есть, иметь хорошую архитектуру.  
Облачная 
архитектура 
является 
ключевым 
элементом построения облачных решений. Она 
соединяет 
все 
компоненты 
и 
технологии, 
необходимые 
для 
облачных 
вычислений, 
предоставляет возможность изменять и расширять 
функционал 
системы 
без 
изменения 
уже 
существующих модулей. Другими словами, новые 
функции реализуются через написание нового кода, а 
существующая кодовая база не будет изменена [1].  
Миграция в облако несет множество бизнеспреимуществ по сравнению с локальными средами, от 
повышенной 
гибкости 
и 
масштабируемости 
до 
экономической 
эффективности. 
Хотя 
многие 
организации могут начать с подхода «подъем и 
перенос», 
при 
котором 
локальные 
приложения 
перемещаются с минимальными изменениями, в 
конечном итоге потребуется создавать и развертывать 
приложения 
в 
соответствии 
с 
потребностями 
облачных сред. Облачная архитектура определяет, как 
компоненты 
интегрируются, 
чтобы 
вы 
могли 
объединять, 
совместно 
использовать 
и 
масштабировать ресурсы по сети. 
В сфере корпоративных приложений возникла 
необходимость 
интеграции 
и 
взаимодействия 
большого 
количества 
информационных 
систем 
предприятия, 
или 
нескольких 
партнерских 
предприятий, 
которая 
оказывает 
существенное 
влияние на используемые программные архитектуры.  
В 
разработке 
программного 
обеспечения 
микросервисная архитектура представляет собой 
вариант структурного стиля сервис-ориентированной 
архитектуры. Это архитектурный шаблон, в котором 
приложение 
представляет 
собой 
набор 
 
International Journal of Open Information Technologies ISSN: 2307-8162 vol. 11, no. 8, 2023 
 
 
18 
 
слабосвязанных 
мелкомодульных 
сервисов, 
взаимодействующих 
с 
помощью 
облегченных 
протоколов. Одна из его целей заключается в том, 
чтобы команды могли разрабатывать и развертывать 
свои сервисы независимо от других. Монолитное 
программное обеспечение спроектировано так, чтобы 
быть автономным, в котором компоненты или 
функции программы тесно связаны, а не слабо 
связаны, как в микросервисах. В монолитной 
архитектуре каждый компонент и связанные с ним 
компоненты должны присутствовать для выполнения 
или компиляции кода. Монолитные приложения 
являются одноуровневыми, что означает объединение 
нескольких компонентов в одно большое приложение. 
Следовательно, они, как правило, имеют большие 
кодовые базы, управлять которыми со временем 
может быть чересчур сложно. В отличии от этого, в 
микросервисах сокращаются зависимости в кодовой 
базе, что позволяет разработчикам развивать свои 
сервисы с меньшими ограничениями со стороны 
пользователей 
и 
скрывать 
от 
пользователей 
дополнительные сложности. Как следствие, компании 
могут 
разрабатывать 
программное 
обеспечение 
постоянно 
увеличивая 
скорость 
вывода 
в 
промышленную эксплуатацию и размер системы. Эти 
преимущества достигаются за счет поддержания 
независимости 
и 
разделения 
микросервисов. 
Интерфейсы 
должны 
быть 
тщательно 
спроектированы 
и 
рассматриваться 
как 
общедоступный 
API. 
Одним 
из 
используемых 
методов является наличие нескольких интерфейсов в 
одной и той же службе или нескольких версий одной 
и той же службы, чтобы не нарушать работу 
существующих пользователей [2].  
 
II. ОСОБЕННОСТИ ПРИМЕНЕНИЯ 
МИКРОСЕРВИСНОЙ АРХИТЕКТУРЫ 
 
Архитектура микросервисов (часто сокращается до 
микросервисов) 
является 
одним 
из 
видов 
сервисориентированной архитектуры и позволяют 
разделить большое приложение на более мелкие 
независимые части, каждая из которых имеет свою 
сферу ответственности. Для обслуживания одного 
запроса 
пользователя 
приложение 
на 
основе 
микросервисов 
может 
вызывать 
множество 
внутренних микросервисов для составления своего 
ответа (рис. 1) [1]. Основными характеристиками 
данной 
архитектуры 
являются 
множество 
компонентов, бизнес-подход (изоляция по бизнес 
функционалу), 
простота 
маршрутизации, 
децентрализованность 
отказоустойчивость 
и 
эволюционность [2]. 
В 
процессе 
построения 
решений 
на 
основе 
микросервисной архитектуры необходимо учитывать 
требования, 
с 
которыми 
могут 
столкнуться 
архитекторы и программные инженеры в процессе 
разработки. Основными требованиями применения 
микросервисной архитектуры являются [3]:  
1. Планирование структуры данных; 
 
Рис.1. Примеры монолитной и мироксервисной  
архитектуры 
 
2. 
Понимание 
необходимости 
использования 
именно микросервисной архитектуры; 
3. 
Планирование 
процессов 
мониторинга 
и 
логирования, тестовых сценариев и автотестов для 
проверки работоспособности сервисов; 
4. Подготовка среды быстрого развертывания; 
5. Понимание того, что разработка микросервисов 
это не быстрая задача, и над ней стоит весьма долго и 
кропотливо работать, начиная с мелких компонентов. 
Создание 
системы 
на 
основе 
микросервисов 
достаточно трудный процесс, в ходе которого надо 
учитывать различные факторы, возникающие из-за 
слабой связности модулей между сбой и возможности 
отказов в них. Если в ходе создания программного 
решения на основе микросервисов учесть все 
проблемные места, можно получить отказоустойчивое 
и надежное решение, имеющее в дальнейшем 
возможность легкой доработки или исправления 
ошибок, в случае их появления. 
 
III. КОТЕЙНЕРИЗАЦИЯ ПРИЛОЖЕНИЙ И 
АРХИТЕКТУРА МИКРОСЕРВИСОВ 
 
Применение контейнеров – один из наиболее 
динамично развивающихся сегментов ИТ-рынка, 
ориентированный 
на 
активное 
корпоративное 
использование облачных услуг. По оценкам Forrester 
Research, 
сегодня 
контейнеры 
уже 
получили 
признание у 41% корпоративного рынка. Согласно 
прогнозу компании 451 Research, мировой объем 
продаж контейнеров в ближайшее время будет 
продолжен. Если в 2016 г. он оценивался в размере 
762 миллионов долларов [1], в 2018 в размере 1,2 
миллиарда [4], к 2022 цифра выросла до 2.09 
миллиарда  и в перспективе объем должен достигнуть 
уровня 5,6 миллиардов долларов к 2027 году, что 
соответствует среднегодовому темпу роста около 
22,7% [5].  
Контейнеризация — это форма виртуализации, при 
которой приложения запускаются в изолированных 
пользовательских 
пространствах, 
называемых 
контейнерами, при использовании одной и той же 
общей операционной системы (рис.2). Одним из 
преимуществ контейнеризации является то, что 
контейнер 
представляет 
собой 
полностью 
упакованную и переносимую вычислительную среду. 
Все, что нужно приложению для запуска — его 
 
International Journal of Open Information Technologies ISSN: 2307-8162 vol. 11, no. 8, 2023 
 
 
19 
 
двоичные файлы, библиотеки, файлы конфигурации и 
зависимости — инкапсулировано и изолировано в его 
контейнере. 
Сам 
контейнер 
абстрагирован 
от 
хостовой 
операционной системы, с ограниченным доступом к 
базовым ресурсам — так же, как виртуальная машина. 
В результате 
контейнеризированное приложение 
можно запускать в различных типах инфраструктуры 
— на «голом железе», в виртуальных машинах и в 
облаке — без необходимости его рефакторинга для 
каждой среды. В Unix-подобных операционных 
системах 
контейнеризация 
реализуется 
через 
несколько механизмов:  cgroups - функции для 
изоляции 
и 
контроля 
использования 
ресурсов 
(например, сколько ЦП и ОЗУ, потоков может 
использовать 
данный 
процесс) 
в 
ядре 
Linux. 
Механизм 
namespaces 
позволяет 
изолировать 
пространства имен компонентов, таких как таблицы 
маршрутизации и файловые системы. Контейнер 
Linux 
может 
монтировать 
файловую 
систему, 
запускать команды от имени root и получать IP-адрес. 
Он выполняет эти действия в своем личном 
пользовательском пространстве.  
 
Рис.2. Место контейнера в архитектуре  
виртуализации 
 
 Docker — одна из самых популярных платформ 
контейнеризации с открытым исходным кодом. Она 
предлагает высокий уровень безопасности, простое 
управление и повышенную скорость развертывания. 
Это в значительной степени облегчает жизнь 
разработчикам, делая процесс упаковки приложений в 
контейнеры беспроблемным. Таким образом, процесс 
становится автоматизированным и быстрым. К тому 
же 
она 
позволяет 
управлять 
многочисленными 
контейнерами на одном хосте.  
Кроме того, большая часть популярности Docker 
заключается в простоте его использования. Благодаря 
множеству 
ресурсов 
и 
широкому 
комьюнити 
пользователей, Docker можно быстро освоить, изучая 
доступные материалы по теме создания и управления 
контейнерами. 
На текущий момент, технология контейнеризации 
является одним из самых мощных и современных 
методов виртуализации. [2, 3]. 
Отличия 
технологий 
контейнейризации 
от 
использования традиционной виртуальной машины 
приведены в таблице 1. 
 
Таблица 1. Сравнение инструментов виртуализации  
         Технология 
Критерий 
Контейнер 
Виртуальная 
машина 
Определение 
Изолированная 
группа 
процессов, 
управляемая 
общим ядром 
Полная ОС, 
которая 
совместно 
использует хостоборудование 
через гипервизор. 
Тип ОС 
Одно ядро ОС, 
но разные 
дистрибутивы 
Различные 
независимые ОС 
Уровень 
изоляции 
Механизмы 
namespaces и 
cgroups 
Полная изоляция 
на уровне ОС 
Размер 
Образы 
измеряются в 
мегабайтах, 
плюс размер 
пользовательско
го приложения 
Образы 
измеряются в 
гигабайтах, плюс 
размер 
пользовательского 
приложения 
Жизненный 
цикл 
Запускается 
непосредственн
о в ядре без 
процесса 
загрузки, часто 
недолговечен. 
Имеет процесс 
загрузки и, как 
правило, 
долгоживущий 
 
Компания Datadog, занимающаяся решениями по 
безопасности и мониторингу для облачных платформ, 
имеет большое количество данных об устройстве 
облачных инфраструктур в компаниях по всему миру. 
Она собрала статистику по использованию Docker и 
смежных технологий среди своих клиентов, которая 
представляет тысячи компаний и 1.5 миллиарда 
контейнеров [6, 7]. Список анализируемых компанийклиентов охватывает большинство отраслей и весь 
спектр: от стартапов до компаний из списка Fortune 
100.  
Согласно исследованиям компании ежегодно в 
период с 2019 по 2022 годы количество клиентов 
Datadog использующих Docker росло на 38-40%. 
Больше 60% компаний с количеством серверов 500 и 
выше используют Docker для решения своих задач. С 
2019 года начался рост интереса к Docker со стороны 
компаний среднего размера (количество серверов от 
100 до 500). Доля пользователей Docker среди них 
приблизилось 
к 
показателю 
как 
у 
крупных 
организаций в 2019 [8], а с 2022 года превзошло их 
[7].  
Дальнейшая 
эволюция 
контейнеризированных 
технологий привела к использованию контейнерами 
распределённых 
вычислений 
облаченных 
в 
микросервисную архитектуру [10]. Больше половины 
компаний-пользователей Docker, используют системы 
оркестрации.  
В 2019 почти 40% пользователей Docker применяли 
различные системы оркестрации [6], а в 2022 году 
 
International Journal of Open Information Technologies ISSN: 2307-8162 vol. 11, no. 8, 2023 
 
 
20 
 
именно Kubernetes стал популярен как никогда. 
Сегодня почти половина организаций использует 
Kubernetes 
для 
развертывания 
контейнеров 
и 
управления ими в растущей экосистеме. Такие 
инструменты, как Amazon Elastic Kubernetes Services 
Blueprints и Amazon EKS Anywhere упрощают для 
команд запуск кластеров в облаке и локально. Роль 
Kubernetes в качестве платформы – предоставить 
унифицированные, согласованные и удобные для 
разработчиков средства развертывания приложений в 
различных масштабах. [11, 12].  
 
 
Рис.3. Среднее количество контейнеров в компании 
удвоилось 
 
Анализируя возрастающую тенденцию внедрения 
Docker-контейнеров [9, 12] в крупных корпорациях [7, 
13], можно сделать вывод об актуальности темы 
применения контейнеров с использованием облачной 
архитектуры 
для 
построения 
информационных 
систем. 
 
IV. ПОСТРОЕНИЕ АРХИТЕКТУРЫ В СИСТЕМЕ 
УПРАВЛЕНИЯ КОНТЕЙНЕРАМИ 
 
Для 
построения 
распределенной 
системы 
с 
микросервисной 
архитектурой 
в 
качестве 
эксперимента по предмету исследования было решено 
использовать четыре приложения, написанных на 
языке Java, представляющих собой четыре отдельных 
микросервиса. 
В 
качестве 
системы 
управления 
контейнерами выбран инструмент OpenShift Online, 
позволяющий загружать пользовательские образы и 
строить микросервисную архитектуру.  
Клиентский сервис client обращается к серверу по 
сущности service openshift. За этим серверомсервисом скрываются 3 версии одного сервиса v1, v2 
и v3. Клиент инициирует запрос после обращения к 
нему (из вне) по REST протоколу, и далее вызывает 
сервер. По умолчанию в openshift маршрутизация 
происходит по алгоритму round robin, т. е. перебор 
серверов внутри одной группы по круговому циклу.  
Ожидаемое поведение в текущей конструкции 
построенной микросервисной архитектуры такое, что 
при множественных вызовах от клиента к серверу, 
нагрузка будет равномерно распределена на всех 
серверах в топологии сети, в нашем случае по трем 
сервисам. 
В рамках эксперимента было инициировано 100 
вызовов от клиента к серверу. Все вызовы имели код 
HTTP ответа 200. Это означает что 100% вызовов 
завершились успешно.  
Для 
наглядного 
представления 
топологии 
построенной микросервисной архитектуры, а также 
сетевого взаимодействия сервисов был использован 
инструмент Kiali, представленный на рисунке 4. 
 
 
Рис.4. Сетевое взаимодействие компонентов построенной архитектуры. 
 
По полученной схеме видно, что 32% трафика 
пошло на первую версию, и по 34% на вторую и 
третью. А значит алгоритм round robin работает, и мы 
имеем систему с распределенной нагрузкой по 
серверам (сервисам).  
Теперь положим, что сервер v2 перестал отвечать на 
запросы, выведен из строя. Понаблюдаем как в это 
случае поведет себя построенная архитектура. После 
эмуляции вывода одного сервера из строя, Openshift 
определяет, что сервис больше не доступен, и 
оперативно выводит его из топологии сети, чтобы 
запросы от клиента не маршрутиризировались на 
недоступный 
сервер. 
На 
этом 
этапе 
были 
инициированы дополнительные 100 запросов, после 
чего получены другие результаты (рис. 5). 
 
 
Рис.5. Сетевое взаимодействие компонентов после 
отключения сервера v2. 
 
Запросы 
распределились 
равномерно 
между 
серверами v1 и v2 и успешно ответили кодом 200.  
В этом эксперименте была продемонстрирована и 
исследована 
оркестрация 
контейнеров, 
успешно 
выполняемая системой управления OpenShift, в 
котором были развернуты созданные микросервисы.  
Как было отмечено, OpenShift с установленным 
инструментом Kiali позволяет легко наблюдать 
построенную 
микросервисную 
архитектуру 
и 
анализировать 
запросы, 
передаваемые 
между 
сервисами. 
Исследуемый пример хорошо показал, как система 
управления контейнерами может реализовать такие 
 
International Journal of Open Information Technologies ISSN: 2307-8162 vol. 11, no. 8, 2023 
 
 
21 
 
базовые принципы микросервисов как обеспечение 
обнаружения 
сервисов 
(service 
discovery) 
и 
канареечные релизы, т.е. снижение риска внедрения 
новой 
версии 
программного 
обеспечения 
в 
«промышленную 
среду». 
Проблемы 
управления 
инфраструктурой в больших масштабах с успехом 
решает специальный журнал событий, в который 
серверы и компоненты инфраструктуры сообщают о 
своих 
адресах 
и 
состоянии, 
как 
только 
они 
появляются в кластере. Любые системы, которым 
нужно знать обо всех серверах, в курсе, что всё это 
можно 
узнать 
в 
этом 
журнале 
при 
наличии 
соответствующих 
прав 
доступа. 
Такая 
система 
называется службой обнаружения (service discovery). 
В нашем примере ее исполняет OpenShift. 
С помощью этого же механизма решаются вопросы 
маршрутизации 
при 
использовании 
подхода 
канареечных 
релизов, 
когда 
путем 
плавного 
развертывания изменений для небольшой группы 
пользователей, 
значительно 
снижается 
риск 
внедрения новой версии программного обеспечения в 
промышленную среду.  
Если нам необходимо запустить трафик извне в наш 
кластер, 
Kubernetes 
предоставляет 
и 
такую 
возможность. Для этого необходимо использовать 
сущность Kubernetes / Openshift Ingress, ведь сегодня 
более 35% организаций для управления внешним 
доступом к объектам в проекте используют Ingress, 
который стал общедоступным с момента выпуска 
Kubernetes версии 1.19 в августе 2020 года. [7] 
Многие 
первые 
пользователи 
Kubernetes 
использовали облачные балансировщики нагрузки 
для маршрутизации трафика к своим сервисам. Но 
Ingress часто более экономичен, и его внедрение 
неуклонно растет с момента его выпуска. Kubernetes 
Gateway API, бета-версия которого вышла в июле 
2022 года, — это следующий шаг в эволюции 
управления сетью для контейнеров. 
 
V. ДИСКУССИЯ ПО ТЕМЕ ИССЛЕДОВАНИЙ 
 
На сегодняшний момент темы запуска крупных 
микросервисных архитектур широко обсуждается 
научным сообществом и прикладными инженерами 
по всему миру. В статье авторы широко освещают 
использование контейнеризации и подчеркивают 
важность 
использования 
подхода 
на 
основе 
микросервисов при построении систем для облачных 
вычислений.  
В настоящее время технология Docker де-факто 
стала стандартом для контейнеризации приложений, о 
чем подчеркивает авторы публикаций [8, 12]. Кроме 
того, при использовании технологий в крупных 
системах реального времени никак нельзя обойтись 
без 
применения 
программного 
обеспечения 
оркестрации для управления и синхронизацией 
систем, состоящих из множества контейнеров, о чем 
докладывают авторы в отчете [11] и статье [14].  
Большое 
распространение 
получили 
системы 
оркестрации на основе Kubernetes и Openshift, что 
подробно отражено в публикациях [14, 15]. 
На данный момент все описанные выше технологии 
и системы активно развиваются, как сообществом, так 
и вендорами, выпускающие enterprise версии, поэтому 
в ближайшее время количество исследований в 
данной 
тематике 
будет 
только 
увеличиваться, 
возможно изменяя или дополняя существующие 
факты, принятые на текущий момент. Миграция 
приложений 
в 
плоскость 
горизонтального 
масштабирования так же ускоряет процесс развития 
технологий 
и 
подходов, 
используемых 
при 
построении микросервисной архитектуры.  
 
VI. ЗАКЛЮЧЕНИЕ 
В результате изучения различных источников, мы 
пришли 
к 
выводу, 
что 
системы 
управления 
контейнерами играют важную роль при построении 
облачных 
распределённых 
систем 
в 
крупных 
корпорациях 
и 
внедрение 
микросервисной 
архитектуры 
является 
наиболее 
современным 
подходом. 
Проведенный эксперимент показал, что система 
управления контейнерами (в исследуемом случае 
Openshift) 
позволяет 
строить 
распределенные 
микросервисные 
архитектуры, 
поскольку 
инструменты, предоставляемые технологией, дают 
возможность 
реализовать 
базовые 
принципы 
микросервисной 
архитектуры. 
Нам 
удалось 
распараллелить 
классический 
клиент-серверный 
подход, 
где 
каждый 
компонент 
является 
изолированным 
от 
других 
компонентов, 
имеет 
возможность 
быть 
перемещенным 
из 
одного 
пространства в другое, использует собственное место 
для 
хранения 
данных, 
отделенное 
от 
других 
микросервисов. 
 
Механизм 
service 
discovery 
представлен по типу черного ящика (а значит 
облегчающий установку и развертывания сервисов, 
без погружения в принципы его работу) позволяет 
легко обнаруживать сервисы для дальнейшего их 
взаимодействия 
и 
поддерживать 
постоянно 
актуальную топологию сети. 
В процессе проведения эксперимента был сделан 
вывод о том, что управление доступом в кластерах 
улучшается с применением новых инструментов, но 
все же пока остается не простой задачей. В Openshift 
мы использовали управление доступом на основе 
ролей, чтобы разрешить субъектам (пользователям, 
группам или учетным записям служб) получать 
доступ 
или изменять ресурсы в кластере. В 
эксперименте мы столкнулись с тем, что не всегда 
достаточно базовых прав для полноценной работы 
построенной архитектуры. В нашем случае это не 
несет особых рисков, однако большие корпорации 
могут столкнуться с трудностями использования 
такого 
подхода, 
ведь 
в 
соответствии 
с 
рекомендациями по безопасности субъекты должны 
иметь 
только 
необходимые 
разрешения, 
и 
администраторы должны соблюдать осторожность 
при предоставлении привилегий, которые связаны с 
рисками эскалации. К ним относятся разрешения, 
которые позволяют субъектам перечислять все 
секреты 
или 
создавать 
рабочие 
нагрузки, 
сертификаты или запросы токенов, которые могут 
позволить им изменять свои собственные привилегии. 
 
International Journal of Open Information Technologies ISSN: 2307-8162 vol. 11, no. 8, 2023 
 
 
22 
 
Возможно, этот вызов могут принять алгоритмы, 
созданные с применением различных методов ИИ, 
например с использованием многоагентных моделей 
для автоматизации управления и развертывания 
контейнеризированных приложений, что является 
сегодня весьма актуальным.  
Независимо от того, что на текущий момент 
существует большое количество различных научных 
публикаций 
по 
теме 
упаковки 
приложений 
в 
контейнеры, как современного метода виртуализации, 
тема применения методов искусственного интеллекта 
для 
управления 
кластерами 
микросервисовконтейнеров, практически нет. По мнению авторов, 
такой эффект вызван новизной микросервисного 
подхода и постоянной эволюцией систем оркестрации 
контейнеров, поэтому тема является актуальной и 
будет активно развиваться в ближайшем будущем. 
 
БЛАГОДАРНОСТИ 
 
Авторы 
выражают 
благодарность 
Высшей 
инжиниринговой школе НИЯУ МИФИ за помощь в 
возможности опубликовать результаты выполненной 
работы. 
БИБЛИОГРАФИЯ 
1. Новиков И. Kubernetes, Docker Swarm и Mesos: 
лучшее 
для 
оркестрации 
контейнеров 
[Электронный 
ресурс]: 
https://www.itweek.ru/infrastructure/article/detail.ph
p?ID=196419 (Дата обращения: 13.06.2022). 
2. Бедняк С.Г., Симакова В.Е. Информационные 
технологии. Виртуализация платформ и ресурсов 
//Актуальные направления научных исследований 
XXI века: теория и практика. – 2015. – Т. 3. – No. 
7-3. – с. 346-349. 
3. Шурупов 
Д. 
Какие 
известные 
компании 
используют Docker в production и для чего? 
[Электронный 
ресурс]: 
https://habr.com/ru/company/flant/blog/326784/ 
(Дата обращения: 13.06.2022). 
4. Application 
container 
market 
[Электронный 
ресурс]: 
https://www.marketsandmarkets.com/MarketReports/application-container-market182079587.html (Дата обращения: 30.12.2022) 
5. Containers as a Service Market by Service Type 
[Электронный 
ресурс]: 
https://www.researchandmarkets.com/reports/440228
8/containers-as-a-service-market-by-servicetype?utm_source=GNOM&utm_medium=PressRelea
se&utm_code=xnrj7f&utm_campaign=1791213++Global+Containers+as+a+Service+Market+Report+
2022+to+2027%3a+Benefits+of+CostEffectiveness+and+Increased+Productivity+Drive+G
rowth&utm_exec=jamu273prd (Дата обращения: 
30.12.2022) 
6. Kumar R., Charu S. An importance of using 
virtualization technology in cloud computing //Global 
Journal of Computers & Technology. – 2015. – Т. 1. 
– №. 2. 
7. 9 Insights on Real-World Container Use | Datadog 
[Электронный 
ресурс]: 
https://www.datadoghq.com/containerreport/?utm_source=organic&utm_medium=display
&utm_campaign=dg-organic-websites-ww-corpsiteannouncement-report-container2022 
(Дата 
обращения: 30.12.2022) 
8. 8 surprising facts about real Docker adoption 
[Электронный 
ресурс]: 
https://www.datadoghq.com/docker-adoption/ (Дата 
обращения: 13.06.2022). 
9. ERN: Processing Petabytes of Data More Efficiently 
with 
Kubernetes 
[Электронный 
ресурс]: 
https://kubernetes.io/case-studies/cern/ 
(Дата 
обращения: 30.12.2022) 
10. Carter E. 2018 Docker usage report. [Электронный 
ресурс]: https://sysdig.com/blog/2018-docker-usagereport/ (Дата обращения: 13.01.2022). 
11. Carter E. Sysdig 2019 Container Usage Report: New 
Kubernetes and security insights [Электронный 
ресурс]: 
https://sysdig.com/blog/sysdig-2019container-usage-report/ 
(Дата 
обращения: 
13.06.2022). 
12. Morgan T. Inside Ebay’s shift to Kubernetes and 
containers atop Openstack [Электронный ресурс]: 
https://www.nextplatform.com/2015/11/12/insideebays-shift-to-kubernetes-and-containers-atopopenstack/(Дата обращения: 13.06.2022). 
13. Орлов Д. Как жить с Docker, или почему лучше с 
ним, чем без него? [Электронный ресурс]: 
https://habr.com/ru/post/250469/ (Дата обращения: 
13.01.2022). 
14. Oliveira C. et al. Evaluating raft in docker on 
kubernetes //International Conference on Systems 
Science. – Springer, Cham, 2016. – P. 123-130. 
15. Lossent A., Wagner A., Rodriguez Peon A. IOP: 
PaaS for web applications with OpenShift Origin // J. 
Phys.: Conf. Ser. – 2017. – Т. 898. – P. 082037.   
 
 
Статья получена 12.05.2023 
Бондаренко 
Александр 
Сергеевич, 
Национальный 
исследовательский 
ядерный 
университет 
«МИФИ» 
(НИЯУ 
МИФИ), аспирант, sasha-bond-95@mail.ru  
Зайцев Константин Сергеевич, Национальный исследовательский 
ядерный университет «МИФИ» (НИЯУ МИФИ), профессор, 
kszajtsev@mephi.ru
 
International Journal of Open Information Technologies ISSN: 2307-8162 vol. 11, no. 8, 2023 
 
 
23 
 
Using container management systems to build 
distributed cloud information systems with 
microservice architecture 
 
A.S. Bondarenko, K.S. Zaytsev  
 
Annotation – The purpose of this work is to study 
the possibility and applicability of container 
management systems for building distributed 
systems with a microservice architecture, because 
nowadays 
programmers 
and 
architects 
are 
increasingly giving their preference to such an 
architecture. The reason is the widespread use of 
cloud technologies, which every year are becoming 
easier to set up and manage. To achieve the goal of 
the study, the authors considered the main features 
of the microservice architecture and evaluated the 
applicability of a container management system 
based on Kebernetes / Openshift for building 
distributed systems with such an architecture. An 
experiment was also conducted to demonstrate the 
effective management of the network topology as 
one of the most important requirements for the 
developed systems of this type. The results of 
applying the tools proposed by the authors showed 
the importance of using container management and 
orchestration systems for building microservices 
and demonstrated the convenience and efficiency of 
this management. 
 
Keywords 
– 
distributed 
systems, 
containers, 
microservices, 
system 
architecture, 
cloud 
information system. 
 

"Мировая наука" №2(47) 2021 
science-j.com 
115 
 
 
 
УДК  001.201 
Фаррахов И.Г. 
студент магистратуры 
Набережночелнинский институт (филиал) КФУ 
Россия, г. Набережные Челны 
Якупов И.М. 
студент магистратуры 
Набережночелнинский институт (филиал) КФУ 
Россия, г. Набережные Челны 
 
АВТОМАТИЗИРОВАННЫЙ ИНСТРУМЕНТАРИЙ РАЗВЕРТЫВАНИЯ 
ОБЛАЧНЫХ СЕРВИСОВ 
 
Аннотация: В статье рассмотрены существующие системы для 
автоматизации развертывание сервисов 
Ключевые 
слова: 
облачные 
сервисы, 
развертывание 
сервисов, 
Kubernetes, 
Docker, Docker Swarm, Mesos, Rancher. 
 
Farrakhov I.G. 
gradute student 
Naberezhnye Chelny Institute (branch) of KFU 
Russia, Naberezhnye Chelny 
Yakupov I.M. 
gradute student  
Naberezhnye Chelny Institute (branch) of KFU 
Russia, Naberezhnye Chelny 
 
AUTOMATED TOOLS FOR DEPLOYING CLOUD SERVICES 
 
Abstract: The article discusses the existing systems for automating the 
deployment of services 
Keywords: cloud services, service deployment, kubernetes, Docker, Docker 
Swarm, Mesos, Rancher. 
 
С развитием интернета и технологий разработки 
подход к разработке и предоставлению услуг кардинально изменился. 
Сегодня 
в 
интернете 
хранится 
большое 
количество 
веб-сайтов 
и 
разнообразной информации. Вместе с развитием интернета развивались и вебсервисы, и способы их размещения на серверах. Изначально это были 
собственные серверы компаний, которые имели множество недостатков 
начиная от потребности хранить собственный штат сотрудников и заканчивая 
тем, что ресурсы каждого из серверов не использовались на 100%, позже 
"Мировая наука" №2(47) 2021 
science-j.com 
116 
 
 
 
появились виртуальные машины, которые тоже имели свои недостатки в виде 
потребности 
виртуализировать 
для 
каждой 
виртуальной 
машины 
операционную систему и физические компоненты компьютера. Еще позже 
появились контейнеры, используемые на данный момент в мире. 
Относительно недавно количество контейнеров, которые разворачивает 
компания выросли к сотням, а иногда и тысячам и остро встал вопрос 
автоматического развертывание таких сервисов. Именно эту проблему и 
решают такие системы как Kubernetes. Они позволяют автоматически 
разворачивать 
множество 
микросервисов, 
настроив 
их 
один 
раз 
конфигурационными файлами, а также поддерживать их работу. 
Docker – это технология виртуализации контейнеров. Она как очень 
легкая виртуальная машина – VM. Это приложение для построения 
контейнеров, работа которого на самом деле заключается в помощи людям в 
создании контейнеров и приложений внутри изолированного пространства, а 
также хранения копий этих контейнеров для того, чтобы потом делиться ими 
среди своих товарищей по команде и без проблем разворачивать такие копии 
как в облаке, так и на локальном компьютере разработчика для анализа, 
например, если возникли какие-то проблемы. 
Есть несколько проблем, которые Docker решает. Первая из них о том, 
что VM является достаточно большим вычислительным ресурсом. Средняя 
виртуальная машина – это копия операционной системы, которая работает 
поверх гипервизора – программного обеспечение, позволяющее параллельно 
управлять несколькими операционными системами, запущенными поверх 
другой 
операционной 
системы, 
работающей 
поверх 
физического 
оборудования, над которым потом находится ваша программа. Это 
представляет 
определенные 
проблемы 
в 
отношении 
скорости 
и 
производительности, а также некоторые проблемы в ловкому среде, а также 
проблемы с быстродействием под время развертывания и остановки работы. 
Kubernetes – это механизм организации контейнеров (COE) с открытым 
кодом, вдохновленный проектом Google под названием Borg. Kubernetes 
используется 
для 
организации 
групп 
контейнеров, 
представляющих 
экземпляры программ, которые отделены от машин, на которых они 
работают. Поскольку количество контейнеров в кластере увеличивается до 
сотен или тысяч экземпляров, а компоненты приложений разворачиваются 
как отдельные контейнеры, Kubernetes приходит на помощь, обеспечивая 
основу для развертывания, управления, автоматического масштабирования, 
высокой доступности и соответствующих задач. 
Контейнеры – хороший способ объединить и запустить ваши 
программы. В производственной среде вам нужно управлять контейнерами, в 
которых запущены программы, и убедиться, что нет простоев. Например, если 
контейнер опускается, нужно запускать другой контейнер. Вот так на помощь 
приходит Kubernetes. Kubernetes предоставляет фреймворк для устойчивого 
запуска распределенных систем. Он заботится о масштабировании и 
"Мировая наука" №2(47) 2021 
science-j.com 
117 
 
 
 
восстановлении после отказа для вашего приложения, предоставляет схемы 
развертывания и т. д. Например, Kubernetes может легко управлять 
развертыванием сервисов для вашей системы, или если была потеряна связь с 
одним из контейнеров, или даже компьютером, то Kubernetes знает, какие 
контейнеры и с какими параметрами были запущены на этом компьютере и за 
считанные секунды запустит все потерянные контейнеры на других 
компьютерах. Хотя данные из оперативной памяти контейнеров будут 
потеряны, но количество работающих сервисов не будет уменьшена и 
пользователю нужно лишь будет повторить свой вызов до сервиса чтобы 
получить ответ и весь этот процесс можно автоматизировать и только 
вмешиваться в него для увеличения контейнеров или для изучения 
результатов работы системы за определенное время для принятия 
определенных решений. 
Docker 
Swarm 
– 
это 
альтернативный, 
собственный механизм 
блокировки оркестрации контейнеров, который координирует размещение и 
управление контейнерами между несколькими хостами Docker Engine. Docker 
Swarm позволяет вам общаться непосредственно с множеством контейнеров, 
вместо того, чтобы общаться с каждым Docker контейнером отдельно. 
Docker compose – это простой, но мощный инструмент, который 
используется для запуска нескольких контейнеров в качестве одной службы. 
Например, предположим, что есть программа, которая требует Nginx как вебсервера и PostgreSQL как службы баз данных. В этом случае с помощью 
docker-compose можно создать один единственный файл docker-compose.yml), 
который создаст оба контейнера в качестве одной службы. У созданного 
множества сервисов будет собственная внутренняя сеть, и много других 
преимуществ. 
Apache 
Mesos, 
общекластерный 
менеджер 
ресурсов, 
широко 
применяется в нескольких облаках и центрах обработки данных. Mesos 
стремится обеспечить высокое использование кластеров с помощью 
тонкозернистого совместного планирования ресурсов и справедливости 
ресурсов среди нескольких пользователей благодаря распределению на 
основе доминирующей справедливости ресурсов. 
DRF учитывает различные типы ресурсов (ЦП, память, дисковый ввод / 
вывод), что требуется каждой программой, и определяет долю каждого 
ресурса кластера, который может быть назначен приложениям. Месос принял 
двухуровневую политику планирования: DRF для распределения ресурсов на 
конкурирующие рамки и планирование уровня задач каждой структурой для 
ресурсов, выделенных на предыдущем шаге. Мы провели эксперименты в 
локальном 
кластере 
Mesos, 
когда 
они 
использовались 
с 
такими 
фреймворками, как Apache Aurora, Marathon и нашим собственным 
фреймворком Scylla, для изучения справедливости ресурсов и использования 
кластера. 
"Мировая наука" №2(47) 2021 
science-j.com 
118 
 
 
 
Mesos объединяет все ресурсы в кластере и позволяет четко 
распределить ресурсы, позволяя и применяя несколько приложений 
(называемых Mesos framework) для совместного планирования своих задач на 
виртуальных машинах / узлах. Mesos использует DRF для распределения 
ресурсов в фреймворки, а затем фреймворки используют алгоритмы 
планирования для планирования задач в пределах выделенных ресурсов. 
Использованные источники: 
1. Diomidis S. Version Control Systems / Diomidis Spinellis, 2005. 
2. Mojtaba S. Continuous Integration, Delivery and Deployment: A Systematic 
Review on Approaches, Tools, Challenges and Practices / Mojtaba Shahin, 2017. 
3. Anderson C. Docker / Anderson Charles, 2015.  
4. Saha P. Exploring the Fairness and Resource Distribution in an Apache Mesos 
Environment / Saha Pankaj. Cloud and Big Data Laboratory, State University of, 
2019.  
5. Stolberg S. Enabling Agile Testing Through Continuous Integration / Stolberg 
Sean, 2009 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Международный научный журнал «ВЕСТНИК НАУКИ» № 8 (65) Т.2 ............................ АВГУСТ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 190 _______________________________ 
УДК 004 
Тюменцев Д.В. 
специалист 
Восточно-Сибирский государственный университет технологий и управления 
(г. Улан-Удэ, Россия) 
 
DEVOPS В ЭПОХУ ОБЛАЧНЫХ ТЕХНОЛОГИЙ: 
СОВРЕМЕННЫЕ ПРАКТИКИ И ПЕРСПЕКТИВЫ РАЗВИТИЯ 
 
Аннотация: 
статья 
освещает 
взаимосвязь 
и 
взаимоинтеграцию 
между 
методологией разработки ПО DevOps (от англ. development & operations) и современными 
облачными технологиями. В материале приводятся основные практики и методы, такие как 
"инфраструктура как код", контейнеризация и бессерверные архитектуры. Автор делает 
акцент на значимости применения данных подходов для усиления эффективности и 
безопасности разработки. Также автор статьи приводит анализ текущих трендов в DevOps 
и возможного направления развития в контексте повсеместного использования облачных 
решений. 
 
Ключевые слова: DevOps, облачные технологии, инфраструктура как код, 
контейнеризация, бессерверные архитектуры, тренды. 
 
Введение.  
Появление облачных решений значительно изменило IT-среду. По 
данным Statista, глобальные расходы на облачные сервисы достигли $312 млрд в 
2020 году и продолжили свой рост [1]. Этот факт подчеркивает растущую 
необходимость в гибких решениях для развития ИТ-сферы. Подобные решения 
должны удовлетворять быстро меняющимся бизнес-требованиям.  
Вместе с ростом популярности облачных технологий, DevOps стал 
популярным решением для компаний, которые регулярно обновляют свое 
программное обеспечение или выпускают на рынок новое.   
Международный научный журнал «ВЕСТНИК НАУКИ» № 8 (65) Т.2 ............................ АВГУСТ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 191 _______________________________ 
В настоящей статье мы рассмотрим исторический контекст развития 
DevOps технологий и их возможные направления развития. 
Исторический контекст развития DevOps технологий 
Согласно отчету Flexera за 2020 год, 93% компаний используют 
многоплатформенные облачные сервисы, что делает DevOps неотъемлемой 
частью их экосистемы [2], ведь DevOps предназначен для автоматизации 
процедур сборки, настройки и запуска программ, обеспечивает гладкую 
интеграцию сервисов в облачной среде. 
Исследуя развитие методологии DevOps в контексте его взаимодействия 
с облачными технологиями, важно вспомнить зарождение методологии в конце 
2000-х, когда специалисты ИТ-сферы начали осознавать необходимость 
улучшения коммуникации между командами разработчиков и системных 
администраторов, а также возможные преимущества, которые бы это принесло 
компаниям [3]. В то время главной задачей бизнеса было повышение 
эффективности процессов разработки и эксплуатации, что позволяло бы быстрее 
реагировать на требования рынка.  
Первоначально практики DevOps фокусировались на автоматизации, но с 
появлением облачных технологий в 2010-х, облачные платформы, такие как 
AWS, Azure и Google Cloud, начали предоставлять возможности для 
автоматизации инфраструктуры и упрощения процессов развертывания ПО. В 
настоящий момент о ситуации на рынке можно ориентироваться, например, 
благодаря отчету RightScale за 2019 год. В нем указано, что 84% компаний 
используют многоплатформенные облачные решения, комбинируя применение 
приватных и публичных серверов для достижения максимальной гибкости 
систем [4]. 
Так, с ростом популярности облачных технологий, практики DevOps 
стали необходимостью для организаций, которые хотели бы максимизировать 
преимущества применения облачной инфраструктуры. 
 
Международный научный журнал «ВЕСТНИК НАУКИ» № 8 (65) Т.2 ............................ АВГУСТ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 192 _______________________________ 
DevOps в облачной эре: ключевые тенденции и практики. 
Развитие ИТ-индустрии привело к существенным изменениям в том, как 
организации используют DevOps. Эти изменения включают в себя применение 
широкого спектра новых инструментов, методологий и подходов, направленных 
на улучшение процессов разработки и эксплуатации. Некоторые из актуальных 
практик: 
Инфраструктура как код (IaC): Один из ключевых элементов облачной 
гибкости является возможность управлять и настраивать всю инфраструктуру 
только лишь с помощью изменений кода. Такие инструменты как Terraform и 
AWS CloudFormation, позволяют разработчикам создавать, изменять и удалять 
ресурсы облачной инфраструктуры в режиме онлайн, что обеспечивает 
непрерывное развертывание и консистентность работы сервисов [5]. 
Контейнеризация и оркестрация: платформы Docker и Kubernetes стали 
стандартами сред для создания и управления контейнеризированными 
приложениями. Благодаря этим технологиям, приложения становятся более 
масштабируемыми и стабильными, что упрощает их запуск и обеспечивает 
прогнозируемость их эксплуатации [6]. 
Непрерывная интеграция и доставка (CI/CD): облачные платформы 
(такие как AWS, Azure или Google Cloud) предлагают такие инструменты и 
услуги, которые помогают автоматизировать процессы непрерывной интеграции 
(CI) и непрерывной доставки (CD) программного обеспечения. То есть, они 
помогают разработчикам автоматически тестировать, собирать и развертывать 
свои приложения в продакшн с минимальными усилиями и задержками. Это 
позволяет значительно ускорить их развертывание на конечных пользователей. 
Мониторинг и сбор обратной связи в режиме онлайн: современные 
облачные платформы предлагают инструменты для мониторинга приложений и 
инфраструктуры онлайн. Это позволяет командам DevOps быстро реагировать 
на возникающие проблемы, а также своевременно получать обратную связь от 
пользователей. 
Международный научный журнал «ВЕСТНИК НАУКИ» № 8 (65) Т.2 ............................ АВГУСТ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 193 _______________________________ 
Безопасность и соответствие стандартам: облачные провайдеры обычно 
предлагают встроенные наборы инструментов для обеспечения безопасности 
деятельности и соответствия регуляторным требованиям, что упрощает работу 
команд DevOps по созданию безопасных сервисов [5]. 
Перспективы развития DevOps в эпоху облачных технологий. 
Как и любое другое технологическое направление, DevOps продолжает 
развиваться и адаптироваться к новым вызовам и возможностям ИТ-сферы. 
Рассмотрим несколько ключевых трендов, которые, как ожидается, будут 
формировать будущее DevOps в ближайшие годы [5]. 
Применение бессерверных архитектур (Serverless): данная парадигма 
предполагает разработку приложений без прямого управления серверной 
инфраструктурой. В настоящий момент важными компонентами современной 
облачной экосистемы являются AWS Lambda, Azure Functions и Google Cloud 
Functions, ключевым преимуществом их использования является оптимизация 
затрат компании на собственную инфраструктуру. 
Применение GitOps: Это подход, который делает процесс развертывания 
и управления ПО более прозрачным, предсказуемым и автоматизированным. 
При этом Git (системы контроля версий) облегчает автоматизацию изменений, 
улучшает прозрачность разработки и предоставляет возможности для быстрого 
восстановления системы в случае критических ошибок. 
Повышение важности непрерывного обучения: С ростом сложности 
технологий и инструментов, DevOps-специалистам требуется постоянное 
обучение. Многие платформы, такие как Coursera, Udemy и edX предоставляют 
возможности пройти курсы и тренинги, помогая специалистам оставаться 
профессионалами в выбранной сфере деятельности, также необходимые знания 
можно приобретать на рабочем месте, через посещение профессиональных 
конференций, получение дополнительного высшего или профессионального 
образования. 
Международный научный журнал «ВЕСТНИК НАУКИ» № 8 (65) Т.2 ............................ АВГУСТ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 194 _______________________________ 
Повышение 
требования 
к 
безопасности 
разработки: 
С 
учетом 
значительного увеличения частоты инцидентов с утечкой данных и серьезности 
возможных последствий для компаний, обеспечение безопасности будет 
становиться все более критичной задачей при разработке и внедрения ИТпродуктов и сервисов. Будут развиваться методики интеграции принципов 
безопасности с самых начальных этапов разработки. 
Заключение.  
DevOps, 
как 
посреднический 
элемент 
между 
разработкой 
и 
эксплуатацией ПО, стоит в центре изменений современной IT-сферы и делает 
процессы 
разработки, 
тестирования 
и 
внедрения 
ПО 
более 
автоматизированными, гибкими и удобными. Автор предполагает, что в 
будущем потребность в DevOps-технологиях увеличится, равно как и требования 
к безопасности разработки и квалификации ИТ-специалистов, частота 
использования бессерверных архитектур и GitOps. Предстоящие изменения 
позволят упросить разработку и развертывание ПО, снизить стоимость 
разработки. 
 
СПИСОК ЛИТЕРАТУРЫ: 
 
1. 
Statista. (2020). "Global public IT cloud services revenue from 2016 to 2020". 
2. 
Flexera. (2020). "State of the Cloud Report 2020". 
3. 
Kim, G., Humble, J., Debois, P., & Willis, J. (2019). "The DevOps Handbook: 
How to Create World-Class Agility, Reliability, & Security in Technology 
Organizations." 
4. 
RightScale. (2019). "State of the Cloud Report 2019". 
5. 
Шайхулов Э.А. ANALYSIS OF THE IMPACT OF MANUAL TESTING ON 
THE ECONOMIC EFFICIENCY OF IT PROJECTS IN THE USA// Proceedings of 
the XXXII International Multidisciplinary Conference «Prospects and Key Tendencies 
of Science in Contemporary World». Bubok Publishing S.L., Madrid, Spain. 2023. 
Международный научный журнал «ВЕСТНИК НАУКИ» № 8 (65) Т.2 ............................ АВГУСТ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 195 _______________________________ 
6. 
Лазарева Н.Б. АВТОМАТИЗАЦИЯ РАЗВЕРТЫВАНИЯ KUBERNETESКЛАСТЕРОВ НА БАЗЕ UBUNTU ОС В RANCHER НА ИНФРАСТРУКТУРЕ 
VMWARE VSPHERE // ИВД. 2023. №4 (100). 
 
Tiumentsev D.V. 
East Siberian State University of Technology and Management 
(Ulan-Ude, Russia) 
 
DEVOPS IN THE ERA OF CLOUD TECHNOLOGIES: MODERN 
PRACTICES AND DEVELOPMENT PROSPECTS 
 
Abstract: the article highlights the relationship and mutual integration between the DevOps 
(development & operations) software development methodology and modern cloud technologies. The 
material introduces key practices and techniques such as “infrastructure as code”, containerization, 
and serverless architectures. The author focuses on the importance of using these approaches to 
enhance the efficiency and safety of development. The author of the article also provides an analysis 
of current trends in DevOps and a possible direction of development in the context of cloud solutions. 
 
Keywords: DevOps, cloud technologies, Infrastructure as Code, containerization, serverless 
architectures, trends. 
 
 

Международный научный журнал «ВСЕМИРНЫЙ УЧЕНЫЙ» Выпуск № 25, Том 1 
_____________________________________________________________________________ 
 
 
УДК 004.4 
Эркаева Н. 
Преподаватель, 
Международный университет нефти и газа им. Ягшигельды Какаева 
Туркменистан, г. Ашхабад 
 
Гаратаева Б. 
Студент, 
Международный университет нефти и газа им. Ягшигельды Какаева 
Туркменистан, г. Ашхабад 
 
Гараева Р. 
Студент, 
Международный университет нефти и газа им. Ягшигельды Какаева 
Туркменистан, г. Ашхабад 
 
Гиджиев Б. 
Студент, 
Международный университет нефти и газа им. Ягшигельды Какаева 
Туркменистан, г. Ашхабад 
 
ОБЛАЧНЫЕ ВЫЧИСЛЕНИЯ: СОВРЕМЕННЫЕ ТЕНДЕНЦИИ, 
ПРОБЛЕМЫ И ПЕРСПЕКТИВЫ 
 
Аннотация:  Облачные вычисления являются одним из ключевых 
элементов 
цифровой 
трансформации 
предприятий. 
В 
статье 
рассматриваются современные тенденции, проблемы и перспективы 
развития облачных технологий. Особое внимание уделяется гибкости, 
масштабируемости, скорости внедрения, снижению операционных затрат, 
Международный научный журнал «ВСЕМИРНЫЙ УЧЕНЫЙ» Выпуск № 25, Том 1 
_____________________________________________________________________________ 
 
 
доступу из любой точки мира, улучшенной безопасности, использованию 
аналитики и больших данных, совместной работе и коммуникациям, а 
также 
цифровой 
трансформации 
в 
области 
ИТ-инфраструктуры. 
Обсуждаются ключевые тренды, такие как кибербезопасность, гибридные 
облачные решения, крауд-компьютинг, облачные вычисления в медицине и 
биотехнологиях, а также устойчивость и экологичность облачных 
технологий. В заключении представлены перспективы развития облачных 
вычислений, связанные с интеграцией с Интернетом вещей, квантовыми 
вычислениями и искусственным интеллектом. 
Ключевые слова: облачные вычисления, цифровая трансформация, 
гибкость, масштабируемость, скорость внедрения, снижение затрат, 
доступ из любой точки мира, безопасность, аналитика, большие данные, 
совместная работа, кибербезопасность, гибридные облачные решения, 
крауд-компьютинг, медицина, биотехнологии, устойчивость, экология, 
Интернет вещей, квантовые вычисления, искусственный интеллект. 
 
В последние годы облачные вычисления стали революционной 
технологией, которая произвела революцию в способах доступа предприятий 
и частных лиц, их хранения и управления данными и приложениями. 
Благодаря 
своей 
масштабируемости, 
гибкости 
и 
экономической 
эффективности облачные вычисления становятся все более повсеместными в 
различных отраслях, стимулируя инновации и меняя цифровой ландшафт. 
Однако по мере того, как внедрение облачных технологий продолжает 
ускоряться, появились новые тенденции, проблемы и возможности, 
определяющие будущую траекторию облачных вычислений. В этой статье 
мы исследуем текущие тенденции, проблемы и перспективы облачных 
вычислений, выделяя ключевые события и предлагая понимание будущего 
этой динамичной области. 
Международный научный журнал «ВСЕМИРНЫЙ УЧЕНЫЙ» Выпуск № 25, Том 1 
_____________________________________________________________________________ 
 
 
Одной из заметных тенденций в области облачных вычислений 
является распространение мультиоблачных и гибридных облачных сред. 
Организации все чаще применяют мультиоблачную стратегию, используя 
услуги нескольких облачных провайдеров, чтобы избежать привязки к 
поставщику, повысить устойчивость и оптимизировать производительность. 
Аналогичным образом, развертывание гибридного облака, которое сочетает в 
себе локальную инфраструктуру с общедоступными и частными облачными 
ресурсами, 
набирает 
обороты, 
поскольку 
организации 
стремятся 
сбалансировать преимущества масштабируемости облака с необходимостью 
контроля и безопасности. 
Еще одна заметная тенденция — рост бессерверных вычислений, 
которые абстрагируют базовую инфраструктуру и позволяют разработчикам 
сосредоточиться на написании кода, не беспокоясь о предоставлении 
серверов или управлении ими. Бессерверные архитектуры предлагают такие 
преимущества, как снижение эксплуатационных расходов, улучшенная 
масштабируемость и оптимизация затрат, что делает их все более 
привлекательными 
для 
создания 
масштабируемых 
и 
экономичных 
приложений. 
Более того, периферийные вычисления стали ключевой тенденцией в 
сфере облачных вычислений, вызванной распространением устройств 
Интернета вещей (IoT) и необходимостью обрабатывать данные ближе к 
точке 
их 
происхождения. 
Периферийные 
вычисления 
обеспечивают 
обработку с малой задержкой, аналитику в реальном времени и снижение 
затрат на передачу данных за счет перемещения вычислительных ресурсов 
ближе к месту генерации данных. Эта тенденция особенно актуальна в таких 
приложениях, как автономные транспортные средства, умные города и 
промышленная автоматизация, где оперативность реагирования в реальном 
времени имеет решающее значение. 
Международный научный журнал «ВСЕМИРНЫЙ УЧЕНЫЙ» Выпуск № 25, Том 1 
_____________________________________________________________________________ 
 
 
Несмотря на многочисленные преимущества, облачные вычисления 
создают ряд проблем и проблем, которые необходимо решить, чтобы 
полностью реализовать свой потенциал. Одной из основных задач является 
обеспечение безопасности и конфиденциальности данных в облаке. 
Поскольку конфиденциальные данные хранятся и обрабатываются в 
удаленных центрах обработки данных, организации сталкиваются с такими 
рисками, как утечка данных, несанкционированный доступ и несоблюдение 
нормативных требований. Решение этих проблем безопасности требует 
надежного шифрования, контроля доступа и мер обеспечения соответствия 
для защиты данных на протяжении всего их жизненного цикла. 
Еще одна проблема — управление расходами на облако и оптимизация 
использования ресурсов. Хотя облачные вычисления предлагают ценовые 
преимущества за счет моделей ценообразования с оплатой по мере 
использования и масштабируемости по требованию, организации должны 
тщательно отслеживать и управлять своими расходами на облако, чтобы 
избежать перерасхода и оптимизировать распределение ресурсов. Это влечет 
за собой реализацию стратегий управления затратами, таких как оптимизация 
размеров экземпляров, использование зарезервированной мощности и 
внедрение политик автоматического масштабирования для достижения 
экономической эффективности без ущерба для производительности или 
надежности. 
Кроме того, совместимость облаков и привязка к поставщику создают 
проблемы для организаций, стремящихся перенести рабочие нагрузки между 
различными 
облачными 
платформами 
или 
вернуться 
к 
локальной 
инфраструктуре. Стандарты совместимости и технологии с открытым 
исходным кодом могут помочь смягчить эти проблемы, обеспечивая плавную 
интеграцию и переносимость в гетерогенных облачных средах. 
Заглядывая в будущее, будущее облачных вычислений обещает 
огромные перспективы, обусловленные достижениями в таких технологиях, 
Международный научный журнал «ВСЕМИРНЫЙ УЧЕНЫЙ» Выпуск № 25, Том 1 
_____________________________________________________________________________ 
 
 
как искусственный интеллект, машинное обучение и сети 5G. Эти 
технологии откроют новые возможности и варианты использования, такие 
как интеллектуальная автоматизация, прогнозная аналитика и иммерсивный 
опыт, 
которые 
будут 
использовать 
масштабируемость, 
гибкость 
и 
возможности подключения облачной инфраструктуры. 
Более того, растущая конвергенция облачных вычислений с новыми 
технологиями, такими как периферийные вычисления, блокчейн и квантовые 
вычисления, откроет новые возможности для инноваций и прорывов в 
различных отраслях. В частности, периферийные вычисления позволят 
обрабатывать и анализировать данные в режиме реального времени на 
границе сети, создавая новые приложения и услуги в таких областях, как 
беспилотные 
транспортные 
средства, 
дополненная 
реальность 
и 
интеллектуальная инфраструктура. 
Кроме того, достижения в области облачных методов разработки и 
технологий контейнеризации, таких как Kubernetes, будут способствовать 
повышению гибкости, переносимости и эффективности разработки и 
развертывания 
приложений. 
Платформы 
оркестрации контейнеров 
и 
бессерверные архитектуры позволят разработчикам создавать и развертывать 
приложения быстрее и в больших масштабах, ускоряя темпы инноваций и 
цифровой трансформации. 
В заключение отметим, что облачные вычисления продолжают быстро 
развиваться под воздействием новых тенденций, проблем и возможностей. 
Принимая стратегии мультиоблачных и гибридных облаков, внедряя 
технологии бессерверных и периферийных вычислений, а также решая 
проблемы безопасности и совместимости, организации могут использовать 
весь потенциал облачных вычислений для стимулирования инноваций, 
гибкости и конкурентоспособности в цифровой экономике. Когда мы 
смотрим в будущее, возможности облачных вычислений безграничны, 
Международный научный журнал «ВСЕМИРНЫЙ УЧЕНЫЙ» Выпуск № 25, Том 1 
_____________________________________________________________________________ 
 
 
предлагая преобразовательные возможности, которые будут определять 
будущее технологий и бизнеса на долгие годы вперед. 
 
СПИСОК ЛИТЕРАТУРЫ: 
 
 
1. "Облачные вычисления: теория и практика" - Авторы: Мельников 
А.В., Хохлова Т.В., 2023 г. 
2. "Облачные вычисления: архитектура, технологии, сервисы" - 
Авторы: Кузнецов С.Н., Федорова Н.В., 2022 г. 
3. "Облачные вычисления: для начинающих и профессионалов" - 
Авторы: Олифер В.Г., Олифер Н.В., 2021 г. 
4. "Облачные вычисления: преимущества, риски, перспективы" - 
Авторы: Соколова О.В., Соколов А.П., 2020 г. 
5. "Облачные вычисления: будущее ИТ-инфраструктуры" - Авторы: 
Иванов А.С., Петров Д.М., 2019 г. 
 
Международный научный журнал «ВСЕМИРНЫЙ УЧЕНЫЙ» Выпуск № 25, Том 1 
_____________________________________________________________________________ 
 
 
Erkayeva N. 
Lecturer, 
International Oil and Gas University 
Turkmenistan, Ashgabat 
 
Garatayeva B. 
Student, 
International Oil and Gas University 
Turkmenistan, Ashgabat 
 
Garayeva R. 
Student, 
International Oil and Gas University 
Turkmenistan, Ashgabat 
 
Gijiyev B. 
Student, 
International Oil and Gas University 
Turkmenistan, Ashgabat 
 
CLOUD COMPUTING: MODERN TRENDS, PROBLEMS AND PROSPECTS 
 
Abstract: Cloud computing is one of the key elements of digital transformation of 
enterprises. The article discusses current trends, problems and prospects for the development of 
cloud technologies. Particular attention is paid to flexibility, scalability, speed of 
implementation, lower operational costs, access from anywhere in the world, improved security, 
the use of analytics and big data, collaboration and communications, as well as digital 
transformation in the field of IT infrastructure. Key trends such as cybersecurity, hybrid cloud 
solutions, crowd computing, cloud computing in medicine and biotechnology, as well as 
sustainability and environmental friendliness of cloud technologies are discussed.  
Keywords: cloud computing, digital transformation, flexibility, scalability, speed of 
implementation, cost reduction, access from anywhere in the world, security, analytics, big data, 
collaboration, cybersecurity, hybrid cloud solutions, crowd computing, medicine, biotechnology, 
sustainability, ecology, Internet of things, quantum computing, artificial intelligence. 

УДК 004.056 
 
ИСПОЛЬЗОВАНИЕ ТЕХНОЛОГИИ КОНТЕЙНЕРИЗАЦИИ КАК КОМПОНЕНТА 
ОБЕСПЕЧЕНИЯ ИНФОРМАЦИОННОЙ БЕЗОПАСНОСТИ 
 
Н.А. Меньшов 
 
 Сибирский государственный университет науки и технологий имени академика М. Ф. Решетнева 
Российская Федерация, 660037, г. Красноярск, просп. им. газ. «Красноярский рабочий», 31 
E-mail: kolyaman_only_steam@mail.ru 
 
Основное 
внимание 
уделяется 
рассмотрению 
технологии 
контейнеризации 
и 
использованию её как компонента обеспечения информационной безопасности. 
 
Ключевые слова: информационная безопасность, контейнер, оркестратор. 
 
THE USE OF CONTEINERIZATION TECHNOLOGY AS A COMPONENT  
OF INFORMATION SECURITY 
 
N.A. Menshov 
 
Reshetnev Siberian State University of Science and Technology 
31, Krasnoyarskii rabochii prospekt, Krasnoyarsk, 660037, Russian Federation  
E-mail:  kolyaman_only_steam@mail.ru 
 
The focus is on consideration of containerization technology and its use as a component of 
information security. 
 
Keywords: information security, containerization, orchestrator. 
 
Введение. Согласно отчёту Flexera контейнеры используют 53% компаний (речь идет 
просто о Docker-контейнерах) [3]. В отчетё Red Hat The State of Enterprise Open Source 2021 
(1250 респондентов), около 50% используют контейнеры в production. Еще 37% — только 
для dev-окружения [4]. Технология контейнеризации стремительно набирает популярность в 
IT 
сфере. 
Основной 
целью 
данной 
статьи 
является 
рассмотрение 
технологии 
контейнеризации и оценка предоставляемых ею механизмов для реализации мер по 
обеспечению информационной безопасности. 
Технология контейнеризации. Контейнеризация – способ упаковки, совместного 
использования и развертывания приложения. Контейнер включает в себя все зависимости 
(пакеты, библиотеки, код приложения, файловая система, сетевой стек и т.д.), которые 
необходимы приложению для функционирования. При развертывании контейнера все его 
ресурсы помещаются в фактически изолированную среду, к которой другие контейнеры не 
могут получить доступ. Контейнеры реализованы на двух ключевых технологиях: 
пространстве имен Linux (namespace) и контрольных группах Linux (cgroups). Пространство 
имен создает практически изолированное пользовательское пространство и предоставляет 
приложению выделенные системные ресурсы. Cgroups обеспечивают ограничение 
аппаратных ресурсов, расстановку приоритетов, мониторинг и контроль приложения. 
Контейнеры – виртуализация на уровне операционной системы. В отличии от виртуальных 
машин контейнеры используют как оборудование, так и ядро операционной системы, что 
уменьшает затраты ресурсов (оперативной памяти, процессорного времени, дискового 
пространства), но обеспечивает меньший уровень изолированности системы [1]. 
_________________________________________________________
Секция «Информационная безопасность»
__________________________________________________________
281
Система оркестрации. При упаковке приложения в контейнер, создаётся образ 
контейнера. Образ создается из конфигурационного файла, в котором описаны все 
необходимые параметры конфигурации (переменные окружения, порты, пользователей, 
установку и обновление пакетов, копирование файлов и т.д.). Для того, чтобы выполнить 
централизованное 
хранение 
образов 
контейнеров, 
используются 
реестры. 
Реестр 
представляет собой дерево папок. Кроме самих образов в реестре хранятся и их более старые 
версии. Рано или поздно встаёт вопрос о том, как эффективно управлять образами. 
Микросервисные архитектуры, постоянное наращивание функционала, запуск новых 
продуктов – всё это ведёт к увеличению числа контейнеров, а значит, и к усложнению 
управления ими. Для преодоления трудностей масштабирования были созданы среды 
контейнерной оркестрации, которые еще называются оркестраторами [2]. 
Системы оркестрации обладают следующим функционалом: запуск контейнеризованных 
приложений в кластере; организация сетевого взаимодействия; поддержка актуального 
состояния приложения в соответствии с желаемой конфигурацией; отладка приложений; 
мониторинг ресурсов; мониторинг систем хранения данных; контроль потока данных между 
контейнерами и оркестратором; проверка работоспособности приложений; репликация узлов 
с приложениями; применение горизонтального автомасштабирования подов (под – группа 
контейнеров с общими разделами, запускаемых как единое целое); именование и 
обнаружение; распределение ресурсов и балансировка нагрузки между контейнерами; 
обкатка обновлений; доступ к журнальным файлам и их обработка; предоставление 
аутентификации и авторизации. 
Преимущества и недостатки технологии. Контейнеры обладают следующими 
преимцществами: быстрое создание и развертывание приложений; непрерывные разработка, 
интеграция 
и 
развертывание; 
разграничение 
ответственности 
разработчиков 
и 
администраторов; 
однородность 
сред 
разработки, 
тестирования 
и 
промышленного 
использования; переносимость между разными облачными провайдерами и операционными 
системами; сосредоточение управления непосредственно на приложении; слабо связанные, 
распределенные, эластичные, независимые микросервисы; изоляция, утилизация ресурсов; 
Недостатки: 
высокая 
сложность, 
рост 
количества 
контейнеров, 
работающих 
с 
приложением, влияет на сложность управления ими; разрастание, нередко в контейнеры 
упаковывается гораздо больше ресурсов, чем реально требуется, из-за этого образ 
разрастается, занимая больше места на диске; запуск контейнеров в Windows-среде не всегда 
удобен; недостаточная зрелость, технологии контейнеризации приложений появились на 
рынке сравнительно недавно, не всегда удаётся сразу решить возникшую проблемы. 
Использование 
технологии 
контейнеризации 
как 
компонента 
обеспечения 
информационной безопасности. Изоляция контейнера усложняет возможность проведения 
атак из приложения внутри контейнера на ресурсы информационной системы. Система 
оркестрации 
предоставляет 
контроль 
работоспособности, 
параметров 
настройки 
и 
правильности функционирования программного обеспечения внутри контейнеров и 
контроль 
целостности 
программного 
обеспечения. 
Обеспечивает 
возможность 
восстановления стабильной версии программного обеспечения при тестировании новой 
версии 
в 
процессе 
эксплуатации 
или 
при 
возникновении 
нештатных 
ситуаций. 
Предоставляет 
дополнительную 
идентификацию 
и 
аутентификацию 
пользователей. 
Предоставляет средство мониторинга ресурсов, систем хранения данных и потока данных 
между контейнерами. Предоставляет возможность масштабирования и распределения 
ресурсов, а также обеспечивает балансировку нагрузки между контейнерами, что 
обеспечивает доступность циркулирующей информации.  
Однако у всего этого есть и обратная сторона. Контейнеры имеют достаточное количество 
уязвимостей. Например, CVE-2014-3519, CVE-2016-5195, CVE-2016-9962, CVE-2017-5123 и 
CVE-2019-5736, которые могут привести к получению злоумышленником доступа к данным 
_________________________________________________________
Актуальные проблемы авиации и космонавтики – 2022. Том 2
__________________________________________________________
282
за пределами контейнера. Также необходимо обеспечить безопасность системы оркестрации. 
Практически все компоненты оркестратора, включая запущенные на нем приложения, могут 
быть использованы злоумышленником для начала/развития атаки. К примеру, Microsoft 
адаптировала структуру MITRE ATT&CK и создала матрицу атак на систему оркестрации 
Kubernetes «Threat Matrix for Kubernetes» с описанием 10 техник от Initial Access до Impact и 
45 тактик, реализующих указанные техники. Кроме того, можно обратиться непосредственно 
к материалам MITRE, выпустившей собственную матрицу «Containers Matrix» несколько 
позже, чем это сделала Microsoft. 
Выводы. Технология контейнеризации может быть использована как компонент 
обеспечения 
информационной 
безопасности. 
Она 
предоставляет 
инструменты 
для 
реализации мер обеспечения информационной безопасности, что при правильном 
менеджменте и использовании механизмов может повысить общий уровень безопасности 
информационной системы. Однако технология ещё довольно незрела и имеет большой 
уровень вхождения. 
 
Библиографические ссылки 
 
1. Как сделать контейнеры еще более изолированными: обзор контейнерных sandboxтехнологий [Электронный ресурс]. URL: https://habr.com/ru/company/itsumma/blog/457760/  
2. Осваиваем Kubernetes. Оркестрация контейнерных архитектур [Электронный ресурс]. 
URL: https://itsecforu.ru/wp-content/uploads/2019/11/Dzhidzhi_Saifan_Osvaivaem_Kubernetes._ 
Orkestraci.pdf 
3. Flexera. State of the Cloud Report. [Электронный ресурс] URL: https://info. 
flexera.com/CM-REPORT-State-of-the-Cloud 
4. Red Hat. The State of Enterprise Open Source. [Электронный ресурс] URL: https://www. 
redhat.com/rhdc/managed-files/rh-enterprise-open-source-report-f27565-202101-en.pdf 
 
© Меньшов Н. А., 2022 
_________________________________________________________
Секция «Информационная безопасность»
__________________________________________________________
283

Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 233 _______________________________ 
УДК 004 
Филисов Д.А. 
руководитель команды разработки Grid Dynamics 
(г. Белград, Сербия) 
 
СТРАТЕГИИ ОПТИМИЗАЦИИ  
ДЛЯ ВЫСОКОНАГРУЖЕННЫХ ПРИЛОЖЕНИЙ:  
ПОВЫШЕНИЕ ОБЩЕЙ ПРОИЗВОДИТЕЛЬНОСТИ 
 
Аннотация: в данной статье подробно рассматриваются стратегии оптимизации 
для высоконагруженных приложений, имеющие решающее значение для повышения 
эффективности общей производительности. В условиях быстро растущей базы цифровых 
пользователей высоконагруженные приложения - те, которые могут обрабатывать от 
тысяч до миллионов запросов в минуту, - требуют точной настройки для удовлетворения 
растущего спроса. Также рассматриваются различные методы оптимизации и стратегии, 
необходимые для повышения производительности этих высоконагруженных приложений. 
Вначале анализируется важность оптимизации кода, от написания эффективных 
алгоритмов до использования правильных структур данных, подчеркивая ее влияние на 
снижение вычислительной сложности приложений. Далее обсуждается актуальность 
оптимизации базы данных и исследуются такие методы, как индексация, кэширование и 
оптимизация запросов, которые играют жизненно важную роль в увеличении времени 
отклика базы данных, а, следовательно, и производительности приложения. Также 
рассматривается 
влияние 
эффективной 
балансировки 
нагрузки 
на 
эффективное 
распределение сетевого трафика и, следовательно, предотвращение перегрузок системы. 
Подробно обсуждаются современные инструменты, такие как Prometheus, New Relic и 
Datadog, проливающие свет на их преобразующую роль в мониторинге и оптимизации 
производительности приложений. Благодаря этим аспектам оптимизации приложений в 
статье представлено подробное руководство по совершенствованию высоконагруженных 
приложений, направленное на обеспечение того, чтобы они удовлетворяли потребности 
пользователей эффективно, действенно и надежно. 
 
Ключевые 
слова: 
высоконагруженные 
приложения, 
оптимизация 
производительности, оптимизация кода, оптимизация базы данных, балансировка нагрузки, 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 234 _______________________________ 
оптимизация аппаратного уровня, эффективность алгоритмов, структуры данных, 
индексация, кэширование, оптимизация запросов, сетевой трафик, использование ресурсов, 
конфигурация сервера. 
 
Введение 
Высоконагруженные 
приложения, 
в 
просторечии 
называемые 
приложениями "больших данных", становятся все более распространенными в 
современном цифровом ландшафте. Отличаясь способностью обрабатывать 
огромное количество запросов одновременно, эти приложения представляют 
собой уникальный набор задач и возможностей для оптимизации. 
Термин "высоконагруженный" обычно относится к программным 
приложениям, способным обслуживать тысячи, если не миллионы запросов в 
секунду. Они могут охватывать широкий спектр областей, от платформ 
социальных сетей, сайтов электронной коммерции и многопользовательских 
онлайн-игр до сложных финансовых торговых систем и не только. 
Масштабируемость таких приложений имеет первостепенное значение для их 
производительности, требуя эффективного управления и распределения 
вычислительных ресурсов для обеспечения непрерывности обслуживания, а 
также бесперебойного взаимодействия с пользователем [1]. 
Оптимизация высоконагруженных приложений - это многомерная 
проблема, требующая тонкого понимания не только основных функциональных 
возможностей приложения, но и внешних систем, с которыми оно 
взаимодействует. Включает в себя глубокие знания различных вычислительных 
парадигм, структур данных, алгоритмов, систем баз данных и сетевых 
протоколов. Более того, понимание взаимосвязи между этими элементами 
является ключом к разработке эффективных стратегий оптимизации [2]. 
Цель статьи - предоставить всесторонний обзор различных методов, 
используемых при оптимизации высоконагруженных приложений. Эти методы 
варьируются от низкоуровневой оптимизации систем баз данных посредством 
индексации и денормализации до стратегий более высокого уровня, таких как 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 235 _______________________________ 
кэширование, оптимизация на уровне приложений, управление параллелизмом и 
балансировка нагрузки. Данный методы представлены в рамках, которые 
подчеркивают их практическое применение, с различными примерами 
кодирования для облегчения более четкого понимания. 
 
Оптимизация базы данных 
В основе многих высоконагруженных приложений лежит надежная и 
высокоэффективная система баз данных. Задача таких систем - хранить, 
извлекать и манипулировать огромными объемами данных в режиме реального 
времени. Следовательно, оптимизация производительности базы данных может 
привести к значительному повышению общей производительности приложения. 
В этом разделе будут рассмотрены две ключевые стратегии оптимизации базы 
данных, а именно индексация и денормализация, и представлены примеры 
кодирования, иллюстрирующие их реализацию [3]. 
1. Индексация 
Индексация - это метод оптимизации базы данных, который повышает 
скорость поиска данных. Подобно указателю в книге, который помогает быстро 
найти конкретную информацию, не читая всю книгу целиком, индекс базы 
данных позволяет системе управления базами данных (СУБД) быстро находить 
запрашиваемые данные, не просматривая каждую строку в таблице. 
Приведенный ниже пример кода демонстрирует создание индекса в 
Django ORM: 
 
 
 
В этом фрагменте кода определяется пользовательская модель в системе 
объектно-реляционного отображения (ORM) Django. Атрибут db_index=True в 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 236 _______________________________ 
поле электронной почты создает индекс для этого поля в основной базе данных. 
Следовательно, любые запросы, которые фильтруют или упорядочивают по 
электронной почте, будут выполняться значительно быстрее, поскольку СУБД 
может использовать индекс для эффективного поиска совпадающих записей. 
2. Денормализация 
Хотя нормализация базы данных имеет решающее значение для 
минимизации избыточности данных и улучшения целостности данных, это 
может привести к сложным запросам с несколькими таблицами, которые могут 
снизить производительность. Денормализация - это стратегия, используемая для 
противодействия этому путем добавления избыточных данных или группировки 
в базе данных. Сохраняя дополнительные копии данных или группируя их, 
чтобы избежать объединения таблиц, база данных может быстрее выполнять 
операции чтения. 
Однако денормализация сопряжена с компромиссами. Хоть это ускоряет 
операции чтения, но также может замедлить операции записи, поскольку для 
обновления может потребоваться изменить данные в нескольких местах. Что 
может увеличить сложность проектирования базы данных и риск возникновения 
аномалий. Поэтому следует тщательно продумать, когда и где использовать 
денормализацию. 
Приведенная ниже инструкция SQL иллюстрирует денормализованную 
таблицу: 
 
 
 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 237 _______________________________ 
В этом примере таблица «Заказов» включает поле «Общая сумма заказа», 
которое является избыточным, поскольку общая сумма заказа может быть 
рассчитана по отдельным позициям заказа в отдельной таблице. Однако, 
сохраняя общую сумму заказа, база данных может быстро получить общую 
сумму без необходимости каждый раз суммировать отдельные позиции, что 
повышает производительность. 
В заключение, хотя методы оптимизации базы данных, такие как 
индексация и денормализация, могут значительно повысить производительность 
высоконагруженного 
приложения, 
их 
следует 
использовать 
разумно. 
Разработчики должны сбалансировать преимущества более быстрого извлечения 
данных с потенциальными недостатками, такими как повышенная сложность и 
риск аномалий данных. 
 
Кэширование 
Кэширование - это стратегия, которая повышает эффективность в 
высоконагруженных 
приложениях 
за 
счет 
минимизации 
затрат 
на 
дорогостоящие операции. Сохраняя результат таких операций или часто 
запрашиваемые данные в кэше, будущие запросы на те же данные могут 
выполняться быстрее, что снижает нагрузку на базу данных или вычислительные 
ресурсы. 
1. Кэш Redis 
Redis (сервер удаленных словарей) является популярным выбором для 
кэширования в высоконагруженных приложениях. Redis - это хранилище данных 
в памяти. Оно хранит данные непосредственно в памяти, что приводит к 
быстрым операциям чтения и записи. Хотя Redis в первую очередь известен как 
хранилище ключей и значений, он также поддерживает сложные типы данных, 
такие как списки, наборы и хэш-карты, что делает его универсальным 
инструментом в арсенале разработчика. 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 238 _______________________________ 
Ниже представлен пример того, как Redis можно использовать для 
кэширования в приложении на Python: 
 
 
В приведенном коде сначала устанавливается соединение с сервером 
Redis с помощью redis.Конструктор Redis(). Затем метод set() используется для 
сохранения пары ключ-значение в хранилище Redis. Последующие запросы на 
эти данные могут быть быстро выполнены с помощью метода get(), который 
извлекает данные, связанные с предоставленным ключом. 
Используя Redis, можно значительно сократить время, затрачиваемое на 
извлечение данных, особенно для операций, которые в противном случае 
потребовали бы дорогостоящих вычислений или запросов к базе данных. 
Однако, хотя кэширование повышает производительность, важно учитывать его 
ограничения и потенциальные подводные камни, например, необходимость 
надлежащего управления аннулированием кэша. 
2. Аннулирование кэша 
Аннулирование кэша относится к процессу удаления устаревших или 
нежелательных данных из кэша. Это становится решающим, когда данные в 
кэше обновляются или удаляются из базы данных. В таких случаях 
невозможность аннулировать кэш может привести к тому, что приложение будет 
обслуживать устаревшие данные. 
Хотя сам Redis по своей сути не предоставляет функций аннулирования 
кэша, его можно запрограммировать для обработки аннулирования с 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 239 _______________________________ 
использованием таких методов, как истечение срока действия, основанное на 
времени.  
Ниже представлен пример кода аннулирования кэша: 
 
 
В этом коде вместо set() используется метод settext(). Метод settext() 
принимает дополнительный параметр, который определяет время истечения 
срока действия ключа. В этом случае ключ будет автоматически удален из Redis 
через 300 секунд (5 минут). 
Данный метод может гарантировать, что кэш не будет обслуживать 
устаревшие данные, но он не всегда идеален. Если данные в базе изменятся до 
истечения срока действия ключа кэша, кэш все равно может обслуживать 
устаревшие данные. Следовательно, необходимы тщательное рассмотрение и 
планирование для выбора наилучшей стратегии аннулирования кэша каждого 
варианта использования. 
 
Оптимизация на уровне приложений 
Помимо внешних систем и сервисов, можно получить значительные 
выгоды от оптимизации внутренней работы приложения. В данном разделе 
описана возможность, как повысить производительность приложения на уровне 
кода. Это включает в себя оптимизацию выбранных структур данных и 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 240 _______________________________ 
алгоритмов, 
сокращение 
объема 
ненужных 
вычислений, 
эффективное 
управление памятью и многое другое. 
1. Использование эффективных структур данных и алгоритмов 
Выбор правильной структуры данных или алгоритма для задачи может 
существенно повлиять на производительность приложения. Например, если 
возникает необходимость проверить, существует ли элемент в коллекции, 
структура данных set становится лучшим выбором, чем список, поскольку set 
может выполнять эту операцию быстрее. 
Ниже представлен пример на Python, который демонстрирует разницу: 
 
 
 
В этом коде происходит сравнение использования списка и набора для 
проверки наличия элемента. В то время как список должен перебирать свои 
элементы, пока не найдет нужный (что занимает O(n) времени), набор может 
выполнить ту же операцию гораздо быстрее (приблизительно O (1) раз), 
поскольку он основан на хэш-таблице. 
2. Сокращение ненужных вычислений 
Другой важный аспект оптимизации на уровне приложения связан с 
минимизацией ненужных вычислений. Данная функция включает в себя 
определение вычислений, которые можно использовать повторно, и сохранение 
их результатов, вместо того чтобы пересчитывать их каждый раз. 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 241 _______________________________ 
Ниже представлен пример, в котором производится оптимизация 
вычисления последовательности Фибоначчи, используя метод, известный как 
запоминание: 
 
 
В данной функции используется словарь под названием memo для 
сохранения чисел Фибоначчи по мере их вычисления. Прежде чем вычислять 
число Фибоначчи для заданного n, сначала проверяется, сохранено ли оно уже в 
memo. Если это так, сохраненный результат возвращается, экономя время и 
вычислительные ресурсы, которые потребовались бы для его повторного 
вычисления. Этот метод значительно повышает эффективность функции, 
особенно при больших входных данных. 
Стоит отметить, что оптимизация кода часто предполагает компромисс 
между эффективностью во времени и пространстве. Поэтому разработчикам 
необходимо досконально разбираться в этих компромиссах, чтобы принимать 
обоснованные решения. 
 
Управление параллелизмом 
Управление параллелизмом - важнейшая стратегия оптимизации 
высоконагруженных 
приложений. 
Включает 
в 
себя 
управление 
одновременными 
операциями 
таким 
образом, 
чтобы 
обеспечить 
согласованность и корректность данных, а также повысить производительность 
приложения [4]. 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 242 _______________________________ 
1. Многопоточность 
Одним из распространенных методов достижения параллелизма является 
многопоточность. Многопоточность позволяет одному процессу выполнять 
несколько 
потоков 
параллельно, 
увеличивая 
использование 
ресурсов 
центрального процессора и повышая производительность приложения. 
Приведенный ниже код на Python демонстрирует создание и выполнение 
двух потоков: 
 
 
 
В этом примере происходит определение двух функций: print_numbers() 
и print_letters(). Затем каждая функция назначается отдельному потоку. Метод 
start() инициирует каждый поток, который начинает выполнять свои 
соответствующие функции одновременно. Метод join() гарантирует, что 
основная программа дождется завершения обоих потоков, прежде чем 
продолжить. 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 243 _______________________________ 
Хотя многопоточность может значительно повысить производительность 
приложения, она также создает потенциальные проблемы, такие как условия 
гонки, взаимоблокировки и другие проблемы с синхронизацией. Поэтому при 
внедрении многопоточности требуется тщательное рассмотрение и надлежащее 
управление. 
2. Блокировки 
Блокировки - это механизм, используемый для управления доступом к 
общим ресурсам в многопоточной среде. Они гарантируют, что, когда один 
поток обращается к определенному ресурсу, другие потоки не могут получить к 
нему доступ до тех пор, пока первый не снимет блокировку. 
Ниже представлен пример использования блокировок в Python: 
 
 
В данном коде уже есть общий ресурс (shared_resource), который будет 
пытаться увеличить несколько потоков. Используя блокировку, есть гарантия, 
что только один поток одновременно может увеличивать общий ресурс, 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 244 _______________________________ 
предотвращая 
условия 
гонки. 
Оператор 
with 
lock: 
используется 
для 
автоматического получения и снятия блокировки, гарантируя, что блокировка 
будет снята, даже если внутри блока возникнет ошибка. 
Хотя блокировки могут помочь справиться с проблемами параллелизма, 
они также могут привести к другим проблемам, таким как взаимоблокировка или 
конфликты. Поэтому важно разумно использовать блокировки и рассмотреть 
другие методы синхронизации, такие как семафоры или переменные условия, в 
зависимости от требований приложения. 
 
Балансировка нагрузки 
Балансировка нагрузки - это метод, используемый для равномерного 
распределения нагрузки между несколькими серверами или процессами. Что 
помогает 
повысить 
производительность 
и 
надежность 
приложений, 
предотвращая превращение какого-либо отдельного сервера в узкое место, тем 
самым делая систему более устойчивой к условиям высокой нагрузки. 
1. Обратный прокси-сервер 
Одним из простейших способов реализации балансировки нагрузки 
является использование обратного прокси-сервера. Данный сервер распределяет 
входящие запросы по нескольким внутренним серверам, распределяя нагрузку 
между ними. Одним из популярных вариантов использования обратного проксисервера является Nginx. 
Ниже представлен пример конфигурации, демонстрирующий, как Nginx 
можно настроить для распределения входящего трафика между двумя 
внутренними серверами: 
 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 245 _______________________________ 
 
 
В этой конфигурации директива upstream используется для определения 
группы внутренних серверов. Директива proxy_pass внутри блока location 
перенаправляет входящие запросы на внутренние серверы. Nginx по умолчанию 
использует алгоритм циклического планирования, поэтому он чередует 
backend1.example.com и backend2.example.com для каждого входящего запроса. 
2. Сохранение сеанса 
Хотя равномерное распределение запросов важно, в определенных 
приложениях крайне важно, чтобы все запросы от одного клиента отправлялись 
на один и тот же внутренний сервер. Данный метод необходим, когда 
приложение поддерживает информацию о состоянии по нескольким запросам от 
одного и того же пользователя. 
В примере ниже представлено, как можно изменить конфигурацию Nginx, 
чтобы обеспечить сохранение сеанса: 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 246 _______________________________ 
 
 
Директива ip_hash предписывает Nginx хэшировать IP-адрес клиента и 
использовать хэш для определения того, на какой внутренний сервер отправлять 
запросы клиента. Это гарантирует, что все запросы от одного и того же клиента 
будут перенаправлены на один и тот же сервер. 
В заключение, балансировка нагрузки может значительно повысить 
производительность и надежность высоконагруженных приложений. Однако 
используемые методы и инструменты зависят от конкретных потребностей 
приложения. Кроме того, разработчики должны тщательно следить за нагрузкой 
на каждый сервер, чтобы убедиться, что нагрузка распределяется равномерно, и 
корректировать свою стратегию по мере необходимости. 
 
Мониторинг и анализ высоконагруженных приложений 
В заключительном разделе статьи было бы полезно обсудить важность 
мониторинга и анализа производительности высоконагруженных приложений. 
Мониторинг производительности позволяет разработчикам выявлять узкие 
места и проверять, дают ли их усилия по оптимизации желаемый эффект. Более 
того, визуальное представление данных о производительности может помочь в 
понимании поведения системы и передаче информации о нем. 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 247 _______________________________ 
 
Инструменты мониторинга 
Существуют различные доступные инструменты, которые могут 
отслеживать производительность приложения, собирать данные и предоставлять 
графические интерфейсы для анализа этих данных: 
1. 
Prometheus. Решение для мониторинга с открытым исходным 
кодом, которое может собирать широкий спектр показателей и предоставляет 
мощный язык запросов для анализа этих показателей. Prometheus также хорошо 
интегрируется с Grafana, инструментом для создания информационных панелей 
на основе данных Prometheus [5].  
К основным компонентам системы Prometheus относятся: 
● 
Сервер Prometheus: основной компонент, который обрабатывает и 
хранит данные временных рядов. 
● 
Клиентские 
библиотеки: 
Они 
используются 
для 
инструментирования кода приложения и предлагают четыре основных типа 
показателей: счетчик, датчик, гистограмма и сводка. 
● 
Pushgateway: Для поддержки кратковременных рабочих мест. 
Позволяет эфемерным и пакетным заданиям предоставлять свои показатели 
Prometheus. Метрики передаются на шлюз, а затем сервер Prometheus извлекает 
метрики из этого шлюза. 
● 
Экспортеры: Они используются для предоставления метрик из 
сторонних систем в качестве метрик Prometheus. Включает в себя такие системы, 
как HAProxy, StatsD, Graphite и т.д. 
● 
Alertmanager: Он обрабатывает оповещения, отправляемые сервером 
Prometheus, и заботится о дедупликации, группировке и маршрутизации их к 
правильным интеграциям получателей, таким как электронная почта, PagerDuty 
или OpsGenie. Также заботится о заглушении и подавлении предупреждений. 
● 
Обнаружение 
служб: 
Prometheus 
поддерживает 
несколько 
механизмов обнаружения служб для динамического обнаружения целевых 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 248 _______________________________ 
объектов очистки. Включает в себя такие механизмы, как файловый, Kubernetes, 
DNS-based, Consul и т.д. 
● 
Веб-интерфейс и API: Встроенный браузер выражений используется 
для визуализации данных и тестирования выражений для языка запросов 
Prometheus (PromQL). Пользователи также могут использовать эти данные 
программно через HTTP API. 
● 
Хранилище: Prometheus включает в себя локальную базу данных 
временных рядов на диске, но также опционально интегрируется с удаленными 
системами хранения. 
● 
PromQL: Это родной язык запросов Prometheus, используемый для 
выбора и агрегирования данных временных рядов в режиме реального времени 
или на основе записанных данных. 
Объединяя эти компоненты, Prometheus предоставляет гибкую и мощную 
платформу для сбора и анализа показателей для контролируемых систем и 
приложений. 
Prometheus получает показатели из инструментированных заданий либо 
напрямую, либо через промежуточный push-шлюз для выполнения эфемерных 
задач. Он поддерживает все очищенные выборки локально и выполняет правила 
над этими данными либо для агрегирования и записи новых временных рядов, 
на основе существующих данных, либо для генерации предупреждений. 
Визуализации скомпилированных данных могут быть созданы с помощью 
Grafana или других пользователей API. 
 
 
 
 
 
 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 249 _______________________________ 
Ниже представлена схема, которая иллюстрирует архитектуру Prometheus 
и некоторые компоненты его экосистемы: 
 
 
Рисунок 1. Компоненты архитектуры Prometheus 
Figure 1. Prometheus architecture components 
 
2. 
New Relic. Комплексная облачная платформа для наблюдения, 
используемая для отслеживания и анализа производительности приложений [6]. 
Он предоставляет широкий спектр услуг и функциональных возможностей, 
помогающих в мониторинге, устранении неполадок и оптимизации приложений, 
включая: 
● 
Мониторинг 
производительности 
приложений 
(APM): 
APM 
обеспечивает мониторинг и отслеживание производительности приложений в 
режиме реального времени. Он может измерять такие показатели, как время 
отклика, пропускная способность и частота ошибок. APM также предоставляет 
информацию о производительности внешних служб, вызываемых вашим 
приложением. 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 250 _______________________________ 
● 
Мониторинг пользователей в режиме реального времени (RUM): Эта 
функция помогает понять пользовательский опыт, фиксируя и анализируя 
каждую транзакцию каждого пользователя в режиме реального времени. 
● 
Мониторинг инфраструктуры: New Relic предоставляет показатели 
работоспособности и данные о производительности ваших хостов, облачных 
сервисов и центров обработки данных в режиме реального времени. 
● 
Бессерверный мониторинг: Эта функция предоставляет информацию 
о бессерверных архитектурах, таких как функции AWS Lambda, включая 
шаблоны вызовов, холодные запуски и ошибки функций. 
● 
Синтетический мониторинг: Синтетический мониторинг позволяет 
вам проактивно тестировать производительность и функциональность вашего 
приложения путем создания имитируемых пользовательских потоков и 
сценариев. 
● 
Распределенная трассировка: New Relic предлагает распределенную 
трассировку, которая поможет вам понять, как проходит запрос через вашу 
сложную архитектуру микросервисов. 
● 
Оповещения и AIOps: New Relic Alerts - это гибкая централизованная 
система оповещения, которая раскрывает оперативный потенциал New Relic. 
Искусственный интеллект New Relic, известный как New Relic AI, обеспечивает 
обнаружение аномалий и анализ инцидентов для уменьшения шума оповещения 
и выявления критических проблем. 
● 
Информационные панели и визуализация данных: New Relic 
предоставляет настраиваемые информационные панели для визуализации, 
анализа и совместного использования данных на платформе. Он предлагает 
множество типов диаграмм, параметров фильтрации и функций настройки. 
● 
Мониторинг журналов: New Relic предлагает интегрированный 
мониторинг журналов, позволяющий вам связывать журналы с другими 
данными телеметрии, такими как метрики, трассировки и события, для более 
целостного наблюдения. 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 251 _______________________________ 
● 
Доступ к API: New Relic также предоставляет API-интерфейсы, 
которые позволяют пользователям программно взаимодействовать со своими 
данными, 
интегрироваться 
с 
другими 
сервисами 
и 
настраивать 
функциональность платформы New Relic. 
На рисунке ниже представлена панель управления New Relic: 
 
 
Рисунок 2. Панель управления New Relic (Источник: веб-сайт New Relic) 
Figure 2. New Relic Control Panel (Source: New Relic website) 
 
3. 
Datadog. 
Полнофункциональный 
сервис 
мониторинга 
производительности 
приложений 
(APM), 
который 
предоставляет 
ряд 
функциональных 
возможностей 
для 
отслеживания 
и 
оптимизации 
производительности вашего приложения [7]. Вот некоторые из ключевых 
особенностей и функциональных возможностей Datadog: 
● 
Визуализация данных в реальном времени: Информационные панели 
Datadog предоставляют показатели в реальном времени с высоким разрешением 
для приложений, инфраструктуры и бизнеса. Эти информационные панели 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 252 _______________________________ 
настраиваемы, что упрощает отслеживание показателей, наиболее важных для 
команды [8]. 
На рисунке ниже проиллюстрирована визуализация всех трассировок 
стека в одном месте с помощью сервиса Datadog: 
 
 
Рисунок 3. Визуализация трассировок стека с помощью сервиса Datadog 
Figure 3. Visualization of stack traces using the Datadog service 
 
● 
Мониторинг производительности приложений (APM): Datadog APM 
обеспечивает сквозную видимость приложений. Он отслеживает запросы от 
начала до конца в распределенных системах и предоставляет подробные графики 
flame, которые помогают понять, как работает приложение и где возникают 
узкие места. 
● 
Мониторинг инфраструктуры: Datadog предоставляет более 450 
интеграций, которые собирают показатели с серверов, баз данных и других 
систем. Возможно визуализировать производительность инфраструктуры и 
оповещать о ней, а также соотносить это с данными о производительности 
приложения. 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 253 _______________________________ 
● 
Управление журналами: Datadog позволяет собирать, обрабатывать 
и анализировать данные журналов из приложений и инфраструктуры. Включает 
в себя функции для ведения журнала, аналитики, архивирования и регидратации. 
● 
Мониторинг производительности сети (NPM): NPM Datadog 
обеспечивает видимость сетевых данных, позволяя изучать исходящий и 
входящий трафик между хостами, контейнерами и службами для быстрого 
выявления узких мест или проблемных сетевых схем. 
● 
Мониторинг безопасности: Мониторинг безопасности Datadog 
позволяет в режиме реального времени обнаруживать угрозы в приложениях, 
сети и журналах. Есть возможность настроить запросы и оповещения, связанные 
с безопасностью, для обнаружения угроз и несанкционированного поведения. 
● 
Синтетический мониторинг: Эта функция позволяет проактивно 
тестировать пользовательские пути и API-интерфейсы с имитацией трафика, 
чтобы понять пользовательский интерфейс и выявить проблемы раньше, чем это 
сделают другие пользователи. 
● 
Управление инцидентами: Datadog предоставляет унифицированное 
представление показателей, трассировок и журналов, чтобы помочь быстрее 
обнаруживать, сортировать и разрешать инциденты. 
● 
Мониторинг пользовательского опыта: Отслеживая и анализируя 
реальные сеансы, Datadog может помочь понять, как производительность 
приложения влияет на работу конечного пользователя (рис. 4) [8]. 
 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 254 _______________________________ 
 
Рисунок 4. Панель аналитики долгосрочных тенденций производительности 
Figure 4. Analytics panel for long-term performance trends 
 
● 
Доступ к API: Datadog предоставляет API для настройки и 
автоматизации взаимодействий с платформой Datadog. 
В совокупности эти функции превращают Datadog в надежную 
комплексную платформу для мониторинга и оптимизации приложений и 
инфраструктуры. 
В заключение, мониторинг и анализ производительности является 
важнейшим аспектом оптимизации высоконагруженных приложений. Собирая 
данные и создавая визуализации, появляется возможность лучшего понимания, 
как приложение ведет себя под нагрузкой, и определить области, где 
оптимизация может оказать наибольшее влияние. 
 
Заключение 
Оптимизация высоконагруженных приложений - это многогранная 
область, которая охватывает несколько областей, включая оптимизацию кода и 
базы данных, балансировку нагрузки и оптимизацию на аппаратном уровне. В 
каждой из этих областей могут быть использованы различные стратегии и 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 255 _______________________________ 
методы для значительного повышения производительности приложений, 
сокращения задержек и улучшения пользовательского опыта. 
Например, улучшения алгоритмов и структуры данных имеют решающее 
значение для повышения эффективности приложения на уровне кода. 
Оптимизация базы данных, такая как индексация, оптимизация запросов и 
кэширование, имеет решающее значение для снижения ненужной нагрузки на 
базу данных и повышения скорости поиска данных. Эффективная балансировка 
нагрузки позволяет максимизировать пропускную способность, сократить время 
отклика и избежать перегрузки какого-либо отдельного ресурса. Наконец, 
оптимизация на аппаратном уровне, такая как выбор подходящих конфигураций 
серверов, процессоров и памяти, также является неотъемлемой частью 
повышения производительности. 
Более того, современные инструменты, такие как Prometheus, New Relic и 
Datadog, 
привнесли 
новое 
измерение 
в 
мониторинг 
и 
оптимизацию 
производительности приложений. Они не только дают представление о 
производительности приложений, но и помогают выявить узкие места, позволяя 
разработчикам 
активно 
устранять 
проблемы 
и 
оптимизировать 
производительность приложений. 
Тщательное согласование этих стратегий и методов, подкрепленное 
мониторингом и обратной связью с помощью надежных инструментов, может 
значительно повысить производительность высоконагруженных приложений, 
гарантируя, что они эффективно удовлетворяют растущим требованиям 
современного цифрового ландшафта.  
 
 
 
 
 
 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 256 _______________________________ 
СПИСОК ЛИТЕРАТУРЫ: 
 
1. 
Лащевски 
Т. 
Облачные 
архитектуры: 
разработка 
устойчивых 
и 
экономичных облачных приложений / Т. Лащевски, К. Арора, Э. Фарр, П. Зонуз. 
– СПб: ИД Питер, 2022. – 320 с. 
2. 
Гниденко И. Г. Технологии и методы программирования: учебное пособие 
для вузов / И. Г. Гниденко, Ф. Ф. Павлов, Д. Ю. Федоров. – М.: Издательство 
Юрайт, 2022. - 235 с. 
3. 
Зильберман, Н., Аудзевич, Ю., Ковингтон, Г. А., и Мур, А. Итог NetFPGA: 
Приближение к 100 Гбит/с в качестве исследовательского продукта / IEEE Micro, 
2014, 35 (5), стр. 32-41.  
4. 
Сассман 
А., 
Смелянский 
М., 
Дубей 
П. 
Жемчужины 
высокопроизводительного параллелизма, том 2: Многоядерные подходы к 
программированию / Морган Кауфман, 2015. - 592 с. 
5. 
Prometheus. Official website. [Электронный ресурс], URL: Prometheus - 
Monitoring system & time series database 
6. 
New Relic. (n.d.). Official website. [Электронный ресурс], URL: New Relic | 
Monitor, Debug and Improve Your Entire Stack 
7. 
Datadog. (n.d.). Official website. [Электронный ресурс], URL: Cloud 
Monitoring as a Service | Datadog (datadoghq.com) 
8. 
Кай Синь Тай. Анализ производительности кода в рабочей среде с 
помощью Datadog Continuous Profiler. Блог Datadog, 2020, [Электронный ресурс], 
URL: https://www.datadoghq.com/blog/datadog-continuous-profiler/ 
 
 
 
 
 
 
Международный научный журнал «ВЕСТНИК НАУКИ» № 7 (64) Т.3 .............................. ИЮЛЬ 2023 г. 
_______________________________________________________________________________________ 
_________________________________ 257 _______________________________ 
Filisov D.A. 
team leader of Grid Dynamics  
(Belgrade, Serbia) 
 
OPTIMIZATION STRATEGIES FOR HIGH-LOAD APPLICATIONS: 
ENHANCING OVERALL PERFORMANCE EFFICIENCY 
 
Abstract: this article provides a detailed examination of optimization strategies for highload applications, critical for enhancing overall performance efficiency. In the face of a rapidly 
growing digital user base, high-load applications - those capable of handling from thousands to 
millions of requests per minute - require fine-tuning to meet escalating demand. Various optimization 
methods and strategies necessary for improving the performance of these high-load applications are 
also discussed. Initially, the importance of code optimization is analyzed, from crafting efficient 
algorithms to employing correct data structures, highlighting its impact on reducing the 
computational complexity of applications. The relevance of database optimization is further 
discussed, and methods such as indexing, caching, and query optimization are explored, playing a 
vital role in enhancing database response time and hence application performance. The influence of 
effective load balancing on efficient network traffic distribution, thus preventing system overloads, is 
also examined. Modern tools such as Prometheus, New Relic, and Datadog are extensively discussed, 
shedding light on their transformative role in application performance monitoring and optimization. 
With these aspects of application optimization, the article provides a comprehensive guide to refining 
high-load applications, ensuring they cater to user needs efficiently, effectively, and reliably. 
 
Keywords: high-load applications, performance optimization, code optimization, database 
optimization, load balancing, hardware-level optimization, algorithm efficiency, data structures, 
indexing, caching, query optimization, network traffic, resource utilization, server configuration. 
 
 

Труды ИСП РАН, том 34, вып. 5, 2022 г. // Trudy ISP RAN/Proc. ISP RAS, vol. 34, issue 5, 2022 
143 
DOI: 10.15514/ISPRAS-2022-34(5)-9 
Исследование методов построения облачных 
платформенных сервисов и реализаций 
стандарта TOSCA  
1,2 А.А. Борисова, ORCID: 0000-0003-4558-7872 <aaborisova_4@edu.hse.ru> 
2 О.Д. Борисенко, ORCID: 0000-0001-8297-5861 <al@somestuff.ru> 
1 Национальный исследовательский университет «Высшая школа экономики»,  
101000, Россия, г. Москва, ул. Мясницкая, д. 20 
  2 Институт системного программирования им. В.П. Иванникова РАН, 
109004, Россия, г. Москва, ул. А. Солженицына, д. 25 
Аннотация. В статье рассматриваются и сравниваются различные инструменты автоматизации 
управления ресурсами в облаке. Изменения в архитектуре программного обеспечения и подходов к 
разработке 
требуют 
автоматизации 
процессов 
управления 
развертывания 
и 
дальнейшего 
сопровождения по в разных средах. В разд. 2 представлен подробный обзор инструментов с примерами 
конфигураций, а также разбор релевантных статей, рассматривающих различные инструменты 
автоматизации и эффективность их внедрения. В разд. 3 представлен проект решения по объединению 
оркестраторов, разработанных в ИСП РАН, для получения инструмента с функционалом, которого нет 
у конкурентов. 
Ключевые слова: оркестрация; развертывание ПО; TOSCA 
Для цитирования: Борисова А.А., Борисенко О.Д. Исследование методов построения облачных 
платформенных сервисов и реализаций стандарта TOSCA. Труды ИСП РАН, том 34, вып. 5, 2022 г., 
стр. 143-162. DOI: 10.15514/ISPRAS-2022-34(5)-9 
Благодарности: Работа выполнена при финансовой поддержке Министерства науки и высшего 
образования Российской Федерации (соглашение №075-15-2022-294 от 15 апреля 2022) 
Research of Construction Methods for Cloud Services and Overview 
of the Implementations TOSCA Standard 
1,2 A.A. Borisova, ORCID: 0000-0003-4558-7872 <aaborisova_4@edu.hse.ru> 
2 O.D. Borisenko, ORCID: 0000-0001-8297-5861 <al@somestuff.ru> 
1 HSE University,  
20, Myasnitskaya st., Moscow, 101000 Russia 
2 Ivannikov Institute for System Programming of the Russian Academy of Sciences, 
25, Alexander Solzhenitsyn st., Moscow, 109004, Russia 
Abstract. This paper overview and compares various tools for automating resource management in the cloud. 
Changes in software architecture and development approaches require automation of deployment management 
processes and further maintenance of software in different environments. Chapter 2 provides a detailed 
overview of the tools with sample configurations, as well as a breakdown of relevant articles that look at various 
automation tools and the effectiveness of their implementation. Chapter 3 presents a draft solution for 
combining orchestrators developed at ISP RAS to obtain a tool with functionality that competitors do not have. 
Keywords: Orchestration; Software Deployment; TOSCA  
Borisova A.A., Borisenko O.D. Research of Construction Methods for Cloud Services and Overview of the Implementations TOSCA 
Standard. Trudy ISP RAN/Proc. ISP RAS, vol. 34, issue 5, 2022, pp. 143-162 
144 
For citation: Borisova A.A., Borisenko O.D. Research of Construction Methods for Cloud Services and 
Overview of the Implementations TOSCA Standard. Trudy ISP RAN/Proc. ISP RAS, vol. 34, issue 5, 2022. 
pp. 143-162 (in Russian). DOI: 10.15514/ISPRAS-2022-34(5)-9 
Acknowledgements. This work was supported by the Ministry of Science and Higher Education of the Russian 
Federation, agreement No. 075-15-2022-294 dated 15 April 2022. 
1. Введение  
В индустриальной разработке программного обеспечения наблюдается переход от 
монолитных архитектур к микросервисным. Микросервисом называют сервис, у которого 
определена роль и набор процессов, которые он обрабатывает. Преимущество по сравнению 
с монолитными архитектурами состоит в том, что управление и масштабирование 
микросервиса может производиться независимо от других микросервисов системы. При этом 
сервисы можно объединить в группы по специфике использования (например: базы данных, 
служебные сервисы, сервисы приложений и пр.). Для каждой группы микросервисов 
жизненный цикл управления – разный. Микросервисы общаются между собой по протоколам 
передачи данных через программные интерфейсы приложений или передачу сообщений. 
Архитектура разрабатываемого ПО влияет на то, как будет организованы разработка, 
поставка и сопровождение ПО, обслуживание инфраструктуры, и организация команды. 
Переход от монолитной архитектуры к микросервисной влечет за собой переход подходов: 
обслуживания баз данных; обслуживания инфраструктуры; мониторинга; организации 
процесса разработки. 
Внедрение техник автоматизации процессов разработки (DevOps – development and 
operations) дает значительный прирост скорости поставки ПО, делает итерации разработки 
короче и увеличивает частоту поставки. К тому же уменьшается количество отказов системы. 
Это достигается за счет того, что каждый микросервис поставляется в виде контейнера. 
Контейнер — это единица поставляемого ПО, содержащая в себе все необходимые для его 
работы библиотеки и зависимости.  
В стандартный жизненный цикл монолитного приложения входит ручное: 
 
развертывание; 
 
обновление; 
 
удаление; 
 
размещение; 
 
создание резервной копии (англ. backup); 
 
восстановление из сохраненной копии. 
Для управления микросервисным системами "ручной" подход (и даже автоматизация 
отдельных задач) неприменим, т.к. количество сервисов для одной системы может 
измеряться тысячами. Поэтому ЖЦ был расширен и автоматизирован. Для микросервисов 
продвинутый ЖЦ позволяет: 
 
автоматическое управление стандартным ЖЦ ПО; 
 
автоматическое масштабирование;  
 
автоматический возврат ПО к предыдущей версии (или "откат изменений" - англ. 
rollback); 
 
развертывание изменений для части сервисов, а не на все сразу (canary и blue-green 
развертывания). 
К тому же, были разработаны подходы автоматического управления, которые способны 
реагировать на изменения в микросервисах и по необходимости менять шаг жизненного 
цикла (напр. автоматическое восстановление при возникновении ошибок во время 
развертывания). Для эффективного управления сложными системами необходимо повышать 
прозрачность и просматриваемость (observability) системы. Необходимо отслеживать очаги 
Борисова А.А., Борисенко О.Д. Исследование методов построения облачных платформенных сервисов и реализаций стандарта 
TOSCA. Труды ИСП РАН, том 34, вып. 5, 2022 г., стр. 143-162 
145 
аномальной работы, падений (иногда отказ одного сервиса тянет за собой отказ связанных 
сервисов). Все ошибки записываются, отслеживаются и измеряются метриками - этот 
процесс называется мониторингом системы. Также вводится понятие состояние сервиса и 
предоставляются хранилища состояний, для отслеживания состояния сервиса и системы в 
целом в конкретный период времени. Примеры состояний: запущен, в ожидании, прерван. 
Не всегда отображается действительное состояние сервиса, он может быть запущен, но 
может не отвечать на определенного рода запросы. Для более детальной информации 
используются обработчики событий, которые посылают запросы на проверку "здоровья" 
(работоспособности) сервиса. Все это дает четкое представление о работе системы. Как итог, 
видны критические сервисы, и сервисы, на которые идет большая/меньшая нагрузка. Все это 
позволяет сделать систему в целом более управляемой, позволяет быстрее реагировать на 
изменение потребления ресурсов системы (напр. автоматическое масштабирование при 
высоком потреблении ресурсов) [3]. 
Стоит отметить, что для работы распределенной системы с микросервисной архитектурой, 
необходимо поддерживать рабочее состояние сети и обеспечивать связывание с другими 
микросервисами и устаревшими системами по сети (используя различные протоколы и 
сервисы обмена сообщениями). 
Рассмотрим все вышеописанное на примере. Предположим, что для продукта была выбрана 
микросервисная архитектура. При этом происходят следующие изменения: команда 
продукта делится на множество мелких подкоманд, которые отвечают каждая за свой сервис. 
Развертывание микросервисов может происходить независимо друг от друга и с частотой 
менее одного дня. Само развертывание микросервиса длится не долго. Управление 
микросервисом происходит автоматически. Микросервис можно развернуть на тестовой 
среде и протестировать изменения перед развертыванием в промышленную среду. При этом 
для микросервисов доступно продвинутое управление ЖЦ ПО. Устройство системы 
позволяет быстро реагировать на изменения рынка (разрабатывать и поставлять обновления 
ПО как можно быстрее), и получать на выходе успешный и конкурентоспособный продукт. 
Однако реализация перехода с монолитных на микросервисные архитектуры оказалась 
проблематичной в направлениях создания инфраструктуры, а также развертывания и 
управления сразу множеством сервисов. Благодаря появлению контейнеров и инструментов 
оркестрации проблемы были решены.  
Для конфигурации и управления монолитным ПО используются инструменты автоматизации 
стандартных задач ЖЦ ПО. Но для микросервисов этого недостаточно, поэтому 
используются инструменты оркестрации, которые позволяют автоматизировать управление 
набором отдельных автоматизированных задач. Термин «оркестрация» не имеет 
зафиксированного определения и границы сильно размыты. Однако можно описать набор 
задач, которые должен решать оркестратор. А именно: автоматизировать набор задач по 
развертыванию, координации и управлению инфраструктурой, ПО и сервисами. Процесс 
оркестрации зависит от того, как вообще организована поставка ПО для конкретного 
продукта. Включение оркестрации в процессы позволяет ощутимо для компании и не 
ощутимо для пользователя поставлять обновления, менять схему взаимодействия сервисов, 
исправлять перебои и пр. Гибкая разработка ПО напрямую связана с гибким управлением и 
поставкой. Это позволяет наиболее эффективно использовать ресурсы. С появлением 
оркестрации появилась возможность делать безопасные Canary, и Blue-Green релизы. 
Производить частичное тестирование новых функций ПО до того, как масштабировать 
изменения на всех пользователей. Платформы оркестрации позволяют осуществлять более 
надежную и стабильную работу системы в целом, а также осуществлять поиск возможных 
ошибок на ранних этапах и быстрое обслуживание системы, без увеличения количества 
сотрудников. 
Контейнеры более легковесные и управляемые, ресурсы контейнера изолированы от других 
частей 
системы. 
Контейнерам 
не 
нужно 
настраивать 
и 
развертывать 
уровень 
Borisova A.A., Borisenko O.D. Research of Construction Methods for Cloud Services and Overview of the Implementations TOSCA 
Standard. Trudy ISP RAN/Proc. ISP RAS, vol. 34, issue 5, 2022, pp. 143-162 
146 
инфраструктуры, в нем уже содержится инфраструктура в виде слоя базового образа. 
Инфраструктурой называют часть сервисного ПО, которая необходима для работы 
разрабатываемого ПО. Процесс оркестрации контейнеров и виртуальных машин отличается 
по этапам и построению. Для контейнеров используется платформы оркестрации Docker 
Swarm и Kubernetes. На начальном этапе пользователь уже имеет «собранный» вручную 
контейнер, обслуживаемый контейнерным движком. 
Настройки сборки прописываются при запуске сборки или конфигурируются на следующем 
этапе. Далее пользователь пишет конфигурацию сервиса (или группы сервисов). В Kubernetes 
есть API конфигурации, который абстрактно описывает элементы инфраструктуры. По API 
для контейнеров можно понять, какую роль выполняет контейнер (напр. рабочая нагрузка, 
БД, балансировщик и т.д.). Также в конфигурации описано поведение контейнера при 
отказах, заданы связи с другими контейнерами. Для более сложных функций работы 
контейнеров друг с другом используются специальные настраиваемые элементы - 
«операторы».  
В отличие от контейнеров, ПО не имеет под собой инфраструктуры. Создание 
инфраструктуры выполняется не линейно. Перед запуском сценария развертывания может 
выполниться подготовительный шаг, а после - недостающие параметры подставляются в 
конфигурацию и запускается скрипт донастройки. Это объясняется тем, что виртуализация 
на базе гипервизора требует развертывания на аппаратной части, в то время как 
виртуализация контейнеров выполняет развертывание на уровне операционной системы. 
Таким образом, при развертывании виртуальных машин на базе гипервизора необходимо 
развернуть уровень инфраструктуры и настроить его (этот этап называют Infrastructure 
Provisioning), затем собрать и развернуть на инфраструктуре сервисы (этот этап называют 
Configuration management).  Для каждого уровня свои сценарии развертывания. Связи и 
отношения между сервисами также заданы в сценарии развертывания (поэтому это не так 
прозрачно, как у контейнеров). Инструменты конфигурации очень гибкие и работают на 
высоком уровне абстракции, поэтому никак не ограничивают роль, которую может 
выполнять сервис, что делает процесс настройки намного более сложным, нежели настройка 
контейнеров. При наличии таких сценариев можно начинать использовать Оркестратор, 
который позволит создавать сценарии управления группой сервисов. Далее подключаются 
сервисы мониторинга и переход на этап обслуживания и мониторинга. 
На практике распределенные приложения требуют решений и для обслуживания 
виртуальных машин, и для обслуживания контейнеров. Для контейнеров стандартом стал 
Kubernetes – как наиболее мощная и развивающаяся платформа. А для виртуальных машин 
оркестрация остается сложно реализуемой, потому что уровень виртуализации ближе к 
аппаратной части. Поэтому оркестраторы поставляются для каждого провайдера по 
отдельности, либо процесс оркестрации реализуется инструментами конфигурации в 
полуавтоматическом режиме. Задача построения мультиоблачного оркестратора с 
поддержкой нескольких инструментов конфигурации остается актуальной.  
На пути реализации такого оркестратора был создан стандарт TOSCA [2], позволяющий 
выйти на необходимый уровень абстракции. Стандартом описаны сущности облачных 
вычислений без привязки к инструменту или провайдеру. На базе стандарта стало возможно 
реализовать мультиоблачный оркестратор. 
Еще одной актуальной проблемой является измерение эффективности внедрения 
оркестратора.  
2. Обзор решений 
В обзоре технологий рассмотрены различные инструменты оркестрации. Сравнение 
инструментов возможно, при учете следующих факторов: уровень виртуализации, уровень 
доступа к ресурсам, поддержка одного/нескольких облачных провайдеров, возможность 
Борисова А.А., Борисенко О.Д. Исследование методов построения облачных платформенных сервисов и реализаций стандарта 
TOSCA. Труды ИСП РАН, том 34, вып. 5, 2022 г., стр. 143-162 
147 
взаимодействия с инструментами конфигурации, графический интерфейс построения 
топологий и пр. Эти факторы необходимо учитывать, т.к. некорректно сравнивать 
оркестраторы различных уровней доступа к ресурсам или же сравнивать производительность 
инструмента конфигурации и оркестратора контейнеров. Поэтому технологии разбиты на 4 
категории, в каждой из которых описан обзор и предоставлено сравнение инструментов.  
2.1 Инструменты, позволяющие осуществлять оркестрацию 
2.1.1 
Terraform 
Terraform – инструмент, осуществляющий оркестрацию для различных облачных 
провайдеров (более 30) уровня IaaS на этапе настройки и развертывания [4]. Для некоторых 
провайдеров доступен PaaS уровень. Поддерживается создание, удаление и обновление 
единиц развертывания. Единицей развертывания является модуль, поддерживаемый для 
данного провайдера. Конфигурация задается через Terraform configuration – шаблон описания 
конфигурации для каждого провайдера на языке Terraform. Не поддерживаются функции 
масштабирования, мониторинга, обслуживания ЖЦ ПО (например, автоматическое 
восстановление в случае отказа).  
Terraform предоставляет часть функций бесплатно. Функции Terraform: удаленное 
хранилище состояний, возможность удаленных запусков, соединение с инструментами 
контроля версий (Github, Gitlab, Bitbucket, Azure Devops). В индустриальной версии 
Terraform есть возможность поддержки собственного облачного сервиса Terraform. При этом 
облако можно разделить между организациями, у которых будет отдельный счет для оплаты 
ресурсов и собственное рабочее пространство. Внутри каждой организации ресурсы можно 
разделить и ограничить между командами разработки и пользователями. Terraform можно 
самостоятельно расширять, добавляя поддержку необходимого провайдера. Однако 
официальные модули крупных облачных провайдеров обновляются со значительной 
задержкой. Инструмент позволяет описывать зависимости между модулями. Но при 
развертывании и удалении проверяется только порядок, а не совместимость и наличие 
циклов.  
Terraform подходит для простых задач по подготовке инфраструктуры для различных 
облачных провайдеров. На листинге 1 представлен пример сценария развертывания с 
использованием Terraform. Оркестрация сервисов PaaS уровня менее полезна, потому что 
инструмент не поддерживает продвинутых функций оркестрации. Инструмент не 
предоставляет доступ к своим собственным PaaS сервисам, а лишь управляет сервисами 
провайдеров. К тому же язык Terraform требует изучения и не позволяет использовать внутри 
себя вставки с вызовом кода. Terraform позволяет описать топологию, однако шаблоны 
описания не универсальны и для каждого провайдера необходимо создавать сценарий. 
Terraform не предоставляет ресурсы по запросу, но позволяет управлять этими ресурсами. 
#### INSTANCE DB #### 
 
# Create instance 
# 
resource "openstack_compute_instance_v2" "db" { 
  name        = "front01" 
  image_name  = var.image 
  flavor_name = var.flavor_db 
  key_pair    = openstack_compute_keypair_v2.user_key.name 
  user_data   = file("scripts/first-boot.sh") 
  network { 
    port = openstack_networking_port_v2.db.id 
Borisova A.A., Borisenko O.D. Research of Construction Methods for Cloud Services and Overview of the Implementations TOSCA 
Standard. Trudy ISP RAN/Proc. ISP RAS, vol. 34, issue 5, 2022, pp. 143-162 
148 
  } 
} 
 
# Create network port 
resource "openstack_networking_port_v2" "db" { 
  name           = "port-instance-db" 
  network_id     = openstack_networking_network_v2.generic.id 
  admin_state_up = true 
  security_group_ids = [ 
    openstack_compute_secgroup_v2.ssh.id, 
    openstack_compute_secgroup_v2.db.id, 
  ] 
  fixed_ip { 
    subnet_id = openstack_networking_subnet_v2.http.id 
  } 
} 
 
# Create floating ip 
resource "openstack_networking_floatingip_v2" "db" { 
  pool = var.external_network 
} 
 
# Attach floating ip to instance 
resource "openstack_compute_floatingip_associate_v2" "db" { 
  floating_ip = openstack_networking_floatingip_v2.db.address 
  instance_id = openstack_compute_instance_v2.db.id 
} 
 
#### VOLUME MANAGEMENT #### 
 
# Create volume 
resource "openstack_blockstorage_volume_v2" "db" { 
  name = "volume-db" 
  size = var.volume_db 
} 
 
# Attach volume to instance instance db 
resource "openstack_compute_volume_attach_v2" "db" { 
  instance_id = openstack_compute_instance_v2.db.id 
  volume_id   = openstack_blockstorage_volume_v2.db.id 
} 
Листинг 1. Пример сценария развертывания ресурсов Openstack в Terraform 
Listing 1. Example of a Terraform deployment scenario for OpenStack 
2.1.2 
Configuration management software 
Инструменты, такие как Ansible, Chef, Puppet, имеют схожий функционал [5, 6, 7].   
Все они поддерживают несколько облачных провайдеров, для которых описаны модули, с 
которыми они работают. В сценариях можно описывать не только развертывание, но и 
любую конфигурацию. В сценарии можно описать развертывания различных модулей 
различных провайдеров. Сценарии не являются шаблонами, не предназначены для пере 
Борисова А.А., Борисенко О.Д. Исследование методов построения облачных платформенных сервисов и реализаций стандарта 
TOSCA. Труды ИСП РАН, том 34, вып. 5, 2022 г., стр. 143-162 
149 
использования между провайдерами. Инструментами конфигурации поддерживаются все 
уровни доступа к ресурсам. Инструменты конфигурации позволяют осуществлять процессы 
оркестрации, однако написание вручную шаблонов и вызов функций не дают им 
преимущество по сравнению с оркестраторами.  
Рассмотрим один из инструментов конфигурации. 
Ansible 
Ansible – мощный инструмент настройки для автоматизации управления вычислительными 
ресурсами в облаке. Благодаря декларативному описанию на предметно-ориентированном 
языке в формате YAML специалисты могут легко настраивать, развертывать и 
контролировать инфраструктуры, приложения, сети и множество других сущностей. Файлы 
описания конфигурации называются Playbook, и они состоят из списка задач для исполнения, 
будем называть их «сценариями Ansible». На листинге 2 представлен пример сценария 
развертывания с использованием Ansible.  
Однако использование этого инструмента требует понимания правил построения Ansible 
сценариев, а также знания точного API провайдера конкретного облачного провайдера. Это 
вызывает проблемы при использовании, когда конфигурация приложения уже выполнена, и 
требуется развернуть приложение с использованием услуг другого облачного провайдера. 
Это то, что отличает его от шаблонов унифицированного абстрактного описания, которые не 
зависят от конкретной технологии или поставщика услуг.  
- name: Create OpenStack component openstack cluster 
  hosts: localhost 
  tasks: 
  - name: Create OpenStack component server 
    os_server: 
      config_drive: false 
      name: server_master 
      flavor: '{{ id_3387 }}' 
      image: '{{ name_8282 }}' 
    register: server_master_server 
  - set_fact: 
      server_master_server_list: '{{ server_master_server_list  
      | default([]) 
        }} + [ "{{ item.id }}" ]' 
    loop: '{{ server_master_server.results | flatten(levels=1)  }}' 
    when: item.id  is defined 
  - set_fact: 
      server_master_server_list: 
        server_master_server_ids: '{{ server_master_server_list }}' 
    when: server_master_server_list is defined 
  - lineinfile: 
      path: '{{ playbook_dir }}/id_vars_example.yaml' 
      line: 'server_master_server_delete: {{ server_master_server.id }}' 
    when: server_master_server.id is defined 
  - lineinfile: 
      path: '{{ playbook_dir }}/id_vars_example.yaml' 
      line: '{{ server_master_server_list | to_nice_yaml }}' 
    when: server_master_server_list is defined 
  - fail: 
Borisova A.A., Borisenko O.D. Research of Construction Methods for Cloud Services and Overview of the Implementations TOSCA 
Standard. Trudy ISP RAN/Proc. ISP RAS, vol. 34, issue 5, 2022, pp. 143-162 
150 
      msg: Variable server_master_server is undefined! So it will not be 
deleted 
    when: 
server_master_server_list 
is 
undefined 
and 
server_master_server.id 
      is undefined 
    ignore_errors: true 
Листинг 2. Пример сценария развертывания ресурсов Openstack в Ansible 
Listing 2. Example of a Ansible deployment scenario for OpenStack 
Ansible Galaxy представляет собой цифровую библиотеку открытых исходных кодов, которая 
содержит более 27 тысяч сценариев конфигурации сервисов, оформленных в виде ролей 
Ansible [8]. Далеко не все они работают, и большая часть из них требует дополнительной 
подготовки перед использованием. 
Ansible используется множеством компаний для сопровождения их сервисов, а Galaxy стал 
местом для обмена лучшими практиками. Число людей, входящих в это сообщество, 
превосходит 299 тысяч человек. 
2.2 Оркестраторы с поддержкой одного облачного провайдера 
2.2.1 
Cloud Formation 
Cloud Formation – оркестратор облачного провайдера Amazon AWS уровня IaaS [9]. 
Поддерживает создание, удаление и обновление единиц развертывания. Единицей 
развертывания является Stack – набор ресурсов (напр. балансировщик, виртуальные машины, 
хранилища данных). Конфигурация задается через AWS templates в формате YAML/JSON на 
языке Cloud Formation template language. Шаблоны можно проверять на корректность. Можно 
задавать связи типа «Depends on» внутри шаблона. Поддерживается функция управления ЖЦ 
ПО сине-зеленое развертывание, сохранение резервной копии и восстановление из нее, 
автоматическое масштабирование. Пример сценария развертывания приведен на листинге 3. 
Также, по шаблону можно создать "безсервисную" конфигурацию и получить развертывание 
уровня FaaS (функция как сервис). Шаблоны покрывают все ресурсов провайдера, в том 
числе сервисы мониторинга и сопровождения ЖЦ ПО. Провайдер Amazon имеет встроенный 
инструмент графического задания топологий формата CloudFormation. CloudFormation это 
полноценный оркестратор, покрывающий все функции для одного облачного провайдера 
Amazon AWS. 
{ 
  "AWSTemplateFormatVersion" : "2010-09-09", 
 
  "Description" 
: 
"AWS 
CloudFormation 
Sample 
Template 
EC2InstanceWithSecurityGroupSample: Create an Amazon EC2 instance 
running the Amazon Linux AMI. The AMI is chosen based on the region in 
which the stack is run. This example creates an EC2 security group for 
the instance to give you SSH access. **WARNING** This template creates 
an Amazon EC2 instance. You will be billed for the AWS resources used if 
you create a stack from this template.", 
 
  "Parameters" : { 
    "KeyName": { 
      "Description" : "Name of an existing EC2 KeyPair to enable  
       SSH access to the instance", 
      "Type": "AWS::EC2::KeyPair::KeyName", 
      "ConstraintDescription" : "must be the name of an existing  
       EC2 KeyPair." 
Борисова А.А., Борисенко О.Д. Исследование методов построения облачных платформенных сервисов и реализаций стандарта 
TOSCA. Труды ИСП РАН, том 34, вып. 5, 2022 г., стр. 143-162 
151 
    }, 
 
    "InstanceType" : { 
      "Description" : "WebServer EC2 instance type", 
      "Type" : "String", 
      "Default" : "t2.small", 
      "AllowedValues" : [ "t1.micro", "t2.nano", "t2.micro", "t2.small", 
"t2.medium", 
"t2.large", 
"m1.small", 
"m1.medium", 
"m1.large", 
"m1.xlarge", 
"m2.xlarge", 
"m2.2xlarge", 
"m2.4xlarge", 
"m3.medium", 
"m3.large", 
"m3.xlarge", 
"m3.2xlarge", 
"m4.large", 
"m4.xlarge", 
"m4.2xlarge", "m4.4xlarge", "m4.10xlarge", "c1.medium", "c1.xlarge", 
"c3.large", 
"c3.xlarge", 
"c3.2xlarge", 
"c3.4xlarge", 
"c3.8xlarge", 
"c4.large", 
"c4.xlarge", 
"c4.2xlarge", 
"c4.4xlarge", 
"c4.8xlarge", 
"g2.2xlarge", 
"g2.8xlarge", 
"r3.large", 
"r3.xlarge", 
"r3.2xlarge", 
"r3.4xlarge", "r3.8xlarge", "i2.xlarge", "i2.2xlarge", "i2.4xlarge", 
"i2.8xlarge", "d2.xlarge", "d2.2xlarge", "d2.4xlarge", "d2.8xlarge", 
"hi1.4xlarge", 
"hs1.8xlarge", 
"cr1.8xlarge", 
"cc2.8xlarge", 
"cg1.4xlarge"] 
, 
      "ConstraintDescription" : "must be a valid EC2 instance type." 
    }, 
 
    "SSHLocation" : { 
      "Description" : "The IP address range that can be used to SSH to 
the EC2 instances", 
      "Type": "String", 
      "MinLength": "9", 
      "MaxLength": "18", 
      "Default": "0.0.0.0/0", 
      "AllowedPattern": 
"(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})", 
      "ConstraintDescription": "must be a valid IP CIDR range of the form 
x.x.x.x/x." 
   } 
 } 
Листинг  3. Пример сценария развертывания ресурсов Amazon в CloudFormation 
Listing 3. Example of a CloudFormation deployment scenario for Amazon 
2.2.2 
Heat Orchestrator 
Heat – инструмент, осуществляющий оркестрацию для облачного провайдера OpenStack 
уровня IaaS [10]. Поддерживает создание, удаление и обновление единиц развертывания. 
Единицей развертывания является Stack – набор ресурсов (напр. балансировщик нагрузки, 
виртуальные машины, хранилища данных). Конфигурация задается через Heat Orchastration 
Templates – шаблоны TOSCA ненормативных типов Openstack. Пример сценария 
развертывания показан на листинге 4. 
Продвинутые функции управления жизненным циклом ПО не поддерживаются. Имеется 
Amazon AWS Query API для поддержки создания сопоставимых ресурсов Cloud Formation. 
Провайдер OpenStack имеет встроенный инструмент графического задания топологий 
формата Heat. 
heat_template_version: 2015-04-30 
 
description: Simple template to deploy a single compute instance 
Borisova A.A., Borisenko O.D. Research of Construction Methods for Cloud Services and Overview of the Implementations TOSCA 
Standard. Trudy ISP RAN/Proc. ISP RAS, vol. 34, issue 5, 2022, pp. 143-162 
152 
 
parameters: 
  key_name: 
    type: string 
    label: Key Name 
    description: Name of key-pair to be used for compute instance 
  image_id: 
    type: string 
    label: Image ID 
    description: Image to be used for compute instance 
  instance_type: 
    type: string 
    label: Instance Type 
    description: Type of instance (flavor) to be used 
 
resources: 
  my_instance: 
    type: OS::Nova::Server 
    properties: 
      key_name: { get_param: key_name } 
      image: { get_param: image_id } 
      flavor: { get_param: instance_type } 
 
 
Рис. 4. Пример сценария развертывания ресурсов OpenStack в Heat Orchestrator 
Fig. 4. Example of a Heat deployment scenario for Openstack 
2.2.3 
Michman 
Michman (разработка ИСП РАН) - это оркестратор уровня PaaS провайдера OpenStack [11, 
12]. Отличие оркестратора в том, что сервисы предоставляются по запросу и уровень 
инфраструктуры настраивается автоматически. Сейчас реализована поддержка следующих 
сервисов: Apache Spark, Apache Hadoop, Apache Ignite, Apache Cassandra, ClickHouse, 
CouchDB, СVAT, ElasticSearch with OpenDistro tools, Jupyter, Jupyterhub, Kubernetes, 
Nextcloud, NFS-Server, Slurm, PostgreSQL, Redis. Есть возможность добавления поддержки 
собственного сервиса. Michman поддерживает создание и удаление единиц развертывания, а 
также масштабирование сервиса. Единицей развертывания является сервис, который 
пользователь выбирает и настраивает через графический или программный интерфейс. 
Michman использует Ansible, как инструмент конфигурации. Для развёртывания некоторого 
сервиса пользователю необходимо задать выбрать сервис, а затем выполнить одну команду 
для развертывания. Michman автоматически подберет параметры IaaS и обработает 
зависимости между сервисами. Результатом программы является сервис, развернутый в 
облаке Openstack. Не поддерживаются функции, мониторинга и продвинутого обслуживания 
ЖЦ ПО (например, автоматическое восстановление в случае отказа). Нет графического 
редактора топологий. 
2.3 Оркестраторы нескольких облачных провайдеров 
2.3.1 
Cloudify 
Cloudify – мультиоблачный оркестратор уровня IaaS и контейнеров .[13]. Расширяем при 
помощи добавления плагинов. На данный момент плагины реализованы для интеграции с 
Борисова А.А., Борисенко О.Д. Исследование методов построения облачных платформенных сервисов и реализаций стандарта 
TOSCA. Труды ИСП РАН, том 34, вып. 5, 2022 г., стр. 143-162 
153 
инструментами конфигураций, развертывания IaaS ресурсов и осуществления оркестрации. 
Cloudify поддерживает создание, удаление и обновление единиц развертывания а также 
любые операции, которые поддерживают инструменты Terraform, Ansible, Helm (Kubernetes), 
Docker. Единицей развертывания является Blueprint template – топология, описанная на 
похожем на стандарт TOSCA предметно-ориентированном языке в формате YAML файла. 
Пример сценария развертывания приведен на листинге 5.  
Шаблоны покрывают ресурсы и операции ЖЦ ПО, реализованные в соответствующих 
плагинах. Поддерживает интеграцию с CI/CD инструментами Jenkins, Github Actions, 
CircleCI. Cloudify имеет графический инструмент создания топологий, что делает порог 
вхождения в Cloudify dsl ниже и позволяет визуально отобразить топологию и связи между 
элементами. 
tosca_definitions_version: cloudify_dsl_1_3 
 
imports: 
  - https://cloudify.co/spec/cloudify/6.3.0/types.yaml 
  - plugin:cloudify-openstack-plugin?version= >=3.2.2 
  - plugin:cloudify-ansible-plugin 
  - plugin:cloudify-utilities-plugin?version= >=1.22.1 
  - includes/hello-world-ansible.yaml 
 
inputs: 
  <parameters_here> 
 
dsl_definitions: 
 
  openstack_config: &openstack_config 
    auth_url: { get_secret: openstack_auth_url } 
    region_name: { get_input: region } 
    project_name: { get_secret: openstack_tenant_name } 
    username: { get_secret: openstack_username } 
    password: { get_secret: openstack_password } 
    user_domain_name: { get_input: user_domain_name } 
    project_domain_name: { get_input: project_domain_name } 
 
node_templates: 
 
  vm: 
    type: cloudify.nodes.openstack.Server 
    properties: 
      client_config: *openstack_config 
      agent_config: 
        install_method: none 
        key: { get_attribute: [agent_key, private_key_export] } 
        user: { get_input: agent_user } 
      resource_config: 
        name: vm 
        image_id: { get_input: image } 
        flavor_id: { get_input: flavor } 
        user_data: { get_attribute: [ cloud_init, cloud_config ] } 
      use_public_ip: true 
Borisova A.A., Borisenko O.D. Research of Construction Methods for Cloud Services and Overview of the Implementations TOSCA 
Standard. Trudy ISP RAN/Proc. ISP RAS, vol. 34, issue 5, 2022, pp. 143-162 
154 
    relationships: 
      - type: cloudify.relationships.openstack.server_connected_to_port 
        target: port 
      - type: cloudify.relationships.depends_on 
        target: cloud_init 
Рис. 5. Пример сценария развертывания ресурсов Openstack в Cloudify 
Fig. 5. Example of a Cloudify deployment scenario for Openstack 
2.3.2 
xOpera Orchestrator 
xOpera – мультиоблачный оркестратор уровня IaaS провайдера Openstack, FaaS провайдеров 
Amazon, Azure, Google Cloud Platform и SaaS [14]. В xOpera реализована поддержка 
соединения между облаками (запрос можно последовательно обрабатывать на ресурсах 
разных провайдеров). xOpera поддерживает создание, удаление единиц развертывания 
(обновление происходит как удаление и развертывание новых). Единицей развертывания 
является TOSCA template – топология, описанная по стандарту TOSCA в формате YAML 
файла. Шаблоны содержат описания в ненормативных типах, зависящих от провайдера. На 
каждый ресурс необходимо предоставить артефакты со сценариями создания и удаления на 
Ansible. xOpera поддерживает интеграцию только с инструментом конфигурации Ansible. 
Параметры из шаблона TOSCA подставляются в сценарий при запуске оркестратором 
сценария развертывания Ansible. И шаблоны, и сценарии остаются провайдер-зависимыми. 
Нельзя добавлять поддержку нового инструмента конфигурации, но можно описать свои 
ненормативные типы ля добавления нового провайдера. 
2.3.3 
Clouni 
Clouni (разработка ИСП РАН) - это TOSCA мультиоблачный оркестратор уровня IaaS 
провайдеров OpenStack, Amazon, Kubernetes [15]. Основная особенность этого инструмента - 
использование нормативных типов TOSCA для их автоматического преобразования в 
сценарии для поддерживаемых облачных провайдеров. Clouni поддерживает создание и 
удаление единиц развертывания. Единицей развертывания является TOSCA template – 
топология, описанная по стандарту TOSCA в формате YAML файла. Инструменты 
конфигурации для разных провайдеров могут отличаться. Например, для Kubernetes 
провайдеров используются Kubernetes манифесты, для OpenStack провайдеров - сценарии 
Ansible. Для развёртывания некоторой инфраструктуры пользователю необходимо описать 
эту инфраструктуру в TOSCA шаблоне с использованием нормативных типов (параметры, 
описанные в стандарте TOSCA, которые интуитивно понятны, например, ЦП, размер памяти 
и т. д.). А затем выполнить одну команду для создания сценария развёртывания. Результатом 
программы является готовый к развертыванию сценарий или манифест. Еще одна 
особенность заключается в том, что любой может добавлять поддержку облачных 
провайдеров, создав определение ненормативных TOSCA типов ресурсов облачного 
провайдера и схему сопоставления с нормативными типами. 
В настоящее время Clouni реализовано несколько нормативных типов TOSCA. А именно 
преобразование Compute, Network и Port в Ansible Playbooks для провайдеров OpenStack и 
только Compute в Amazon и в Kubernetes Manifest для контейнерной платформы Kubernetes. 
Сценарии Ansible генерируются для создания и удаления инфраструктуры. Манифесты 
Kubernetes следует использовать с командами kubectl apply/delete. 
2.3.4 
Alien4Cloud 
Alien4Cloud - это инструмент моделирования TOSCA с открытым исходным кодом [16].  Его 
основное отличие состоит в том, что он позволяет разворачивать сервисные кластеры из вебинтерфейса за счёт подключаемых плагинов. Каждый плагин трансформирует запросы и 
Борисова А.А., Борисенко О.Д. Исследование методов построения облачных платформенных сервисов и реализаций стандарта 
TOSCA. Труды ИСП РАН, том 34, вып. 5, 2022 г., стр. 143-162 
155 
ответы Alien4Cloud и некоторого TOSCA сервиса, что позволяет использовать внешний 
TOSCA сервис из веб-интерфейса Alien4Cloud. Кроме того, инструмент предоставляет 
возможность добавлять собственные расширяющие плагины с определенной логикой 
(например, дополнительный оркестратор TOSCA). Alien4cloud не меняет вводимые данные 
и позволяет использовать собственный нормативный тип TOSCA. Реализована функция 
мониторинга. 
Yorc оркестратор - официальный мультиоблачный оркестратор TOSCA для Alien4cloud 
уровня IaaS провайдеров Amazon, OpenStack, Google Cloud Platform, Kubernetes [17].  Также, 
Yorc предоставляет Slurm как сервис (PaaS уровень). Yorc использует Alien4cloud типы 
TOSCA на основе TOSCA Simple YAML Profile v.1.2. Надлежащая документация по обоим 
инструментам четко описывает, как использовать инструменты вместе. Единицей 
развертывания является архив, содержащий TOSCA template. Продвинутые функции 
управления жизненным циклом ПО не поддерживаются. К сожалению, примеры процессов 
оркестрации из документации нельзя оценить, потому что образцы не обновлялись долгое 
время и не работали. Однако же удалось подключить Alien4cloud к Openstack и запустить 
развертывание виртуальной машины, которое оказалось не успешным. По анализу логов, 
можно убедиться, что Alien4cloud использует Terraform как инструмент развертывание и 
подставляет готовые сценарии развертывание для заданных ненормативных типов. 
2.4 Оркестраторы контейнеров 
2.4.1 
Docker Swarm 
Swarm это специальный режим работы сервиса Docker. Swarm не предоставляет ресурсы по 
запросу, он только управляет контейнерами. Поддерживает развертывание, удаление, 
обновление и масштабирование ресурсов. Единицей развертывания является стек - набор 
сервисов-контейнеров. Конфигурация развертывания задается через конфигурационный 
файл docker compose. Swarm использует декларативный подход и одновременно выступает в 
роли инструмента оркестрации контейнеров. Мониторинг сервисов не производится, но 
можно получить состояние сервиса по запросу. При обновлении стека можно указать порядок 
обновления сервисов, количество контейнеров, которые можно обновлять одновременно, 
указать стратегию в случае сбоя (есть функция отката обновлений в случае сбоя). Функции 
Swarm можно расширять при помощи подключения плагинов. Графический интерфейс не 
поддерживается. 
2.4.2 
Kubernetes 
Наиболее функциональной платформой оркестрации контейнеров на данный момент 
является Kubernetes. Kubernetes выступает как провайдер, позволяя предоставлять ресурсы 
на всех уровнях IaaS, PaaS, SaaS [18]. А также как платформа оркестрации, которая управляет 
ресурсами. Поддерживает все функции обслуживания ЖЦ ПО, в том числе масштабирование 
(как горизонтальное, так и вертикальное), а также мониторинг ресурсов и сервисов. 
Конфигурация развертывания задается через Kubernetes manifest. Kubernetes использует 
декларативный подход и одновременно выступает в роли инструмента конфигурации и 
оркестрации. Шаг со сборкой сервиса не требуется (т.к. оркестратор оперирует 
контейнерами, в которых уже содержаться собранный библиотеки и зависимости). Поэтому 
Kubernetes позволяет настроить конфигурацию работы между сервисами и полностью 
описать работу сервиса в различных случаях, например в различных состояниях. В каждый 
момент времени Kubernetes отслеживает состояние сервиса и принимает действие, например, 
в случае отказа. Состояния отслеживаются 
при помощи механизма "проверки 
работоспособности" сервиса (англ. Healthcheck). Kubernetes Можно интегрировать с 
Borisova A.A., Borisenko O.D. Research of Construction Methods for Cloud Services and Overview of the Implementations TOSCA 
Standard. Trudy ISP RAN/Proc. ISP RAS, vol. 34, issue 5, 2022, pp. 143-162 
156 
различными инструментами конфигурации, интеграции и поставки ПО. Минусом является 
отсутствие программ для графического редактирования топологий. 
2.5 Сравнение Cloudify и Terraform 
В статье [19] сравнивают производительность развертывания двух инструментов – Terraform 
(инструмент автоматизации) и Cloudify (оркестратор на базе стандарта TOSCA), а также 
рассматривают функции оркестраторов: OpenStack Heat, CloudFormation, и Cloud Assembly. 
Многие 
инструменты 
описаны 
поверхностно, 
авторы 
дают 
представление 
о 
функциональности, но не позволяют сделать сравнительный анализ и выявить сильные и 
слабые стороны инструментов. 
Производительность сравнивают только для Cloudify и Terraform. Для эксперимента авторы 
предложили развернуть инфраструктуру (IaaS) и приложение Wordpress на ней (PaaS), т. к. 
Wordpress популярный вариант приложения. Было решено развернуть облачное приложение 
у 3-х различных провайдеров (Aws, Google cloud platform and Azure)  под это требование из 
рассмотренных в статье оркестраторов подходят только Cloudify и Terraform.  
На хост с оркестраторами были установлены сервисы мониторинга, которые следили за 
состоянием во время развертывания и удаления инфраструктуры. Авторы статьи 
использовали похожие конфигурации провайдеров. Однако, стоит заметить, что Terraform не 
оркестратор, и в данном случае выступает, как инструмент конфигурации. В то время как 
Cloudify - оркестратор, главным преимуществом которого является не работа "на скорость", 
а работа с несколькими облачными провайдерами и инструментами конфигурации. То есть 
само по себе сопоставление не является благоразумным. Авторы выбрали Ansible, как 
инструмент конфигурации для эксперимента, но не упомянули об этом. Стоит заметить, что 
Cloudify может работать и с Terraform и причины выбора Ansible авторы не называют. То 
есть в случае с Cloudify, неявно используется инструмент конфигурации, а Terraform работает 
напрямую с провайдером и очевидно, что результаты будут в худшую сторону для Cloudify. 
Эти результаты подтверждены графиками, но не отражают действительных преимуществ 
оркестратора. В заключении авторы делают упор на высокую производительность 
инструмента конфигурации Terraform и значительные преимущества в производительности. 
Однако эксперимент был проведен только в части развертывания и удаления сервисов. У 
Cloudify есть значительные преимущества - унифицированные шаблоны топологий, 
графический интерфейс для отображения топологии, возможность полномасштабной 
оркестрации (Terraform осуществляет оркестрацию только на IaaS уровне).  
Авторы дают ссылку на шаблоны развертывания, которые они используют. По ним видно, 
какую проблему пытается решить стандарт TOSCA.  
1) Оба инструмента развертывают одну и ту же инфраструктуру.  
2) Для использования Terraform необходимо выучить синтаксис языка инструмента, а для 
использования Cloudify с несколькими провайдерами и инструментами конфигурации - 
достаточно научиться использовать только TOSCA.  
С другой стороны, Cloudify использует свою интерпретацию стандарта TOSCA - TOSCA 
DSL. И поддерживает только ненормативные типы, которые добавляются в виде плагинов, 
что лишает преимуществ инструмент по сравнению с другими TOSCA Оркестраторами. 
2.6 ToolKit Deployment Manager 
В статье [20] рассматривается инструмент автоматизации развертывания (оркестратор) для 
различных дистрибутивов облачного провайдера Openstack на уровне IaaS. 
Реализованный оркестратор должен отвечать требованиям по координации, автоматизации, 
подготовке и мониторингу ресурсов. Инструмент называется "ToolKit Deployment Manager" 
и 
состоит 
из 
набора 
блоков-контроллеров, 
которые 
отвечают 
за 
управлением 
(развертыванием и удалением): Сетью, Платформой, ПО, Сервисами. 
Борисова А.А., Борисенко О.Д. Исследование методов построения облачных платформенных сервисов и реализаций стандарта 
TOSCA. Труды ИСП РАН, том 34, вып. 5, 2022 г., стр. 143-162 
157 
Инструмент был реализован и проведено исследование эффективности оркестратора, 
которое наглядно показывает пользу. Процесс оркестрации проводился при помощи 
сценариев Развертывания Ansible. В качестве метрик для измерения возможности 
развертывания взяты: 
 
"среднее время развертывания" – время от вызова скрипта развертывания до перевода 
состояния как "запущено"; 
 
"надежность развертывания" – отношение успешных развертываний к общему 
количеству запусков; 
 
"количество шагов развертывания" – общее количество шагов, требующееся для 
выполнения развертывания; 
 
"параметры, введенные во время развертывания" – столько раз пользователю 
необходимо вручную ввести параметры во время выполнения развертывания; 
 
"сумма шагов развертывания" – сумма количества шагов развертывания и количества 
вмешательств пользователя. 
 
"возможность развертывания" – итоговое количество усилий для успешного 
развертывания ( (1-"Надежность развертывания") + 1 ) * "Сумма шагов развертывания". 
Эти метрики действительно показывают эффективность внедрения инструментов 
оркестрации, т.к. они учитывают время, которое пользователь бы затратил без использования 
оркестратора. Также, необходимо добавить метрики, которые показывают количество 
артефактов развертывания, с которыми приходится работать. Т.к. версионирование, учет и 
написание новых сценариев занимает большое количество времени, а оркестратор помогает 
автоматизировать эти задачи и уменьшить количество сценариев. 
Результаты, полученные в статье, демонстрируют сокращение количества ошибок 
развертывания и потраченного времени, что говорит об эффективном внедрении 
оркестратора. 
3 Исследование и построение решения задачи 
3.1 Сравнительный анализ методов построения облачных 
платформенных сервисов 
Построение сервисов для виртуализации на базе гипервизора и на базе ОС (контейнеры) 
отличается. Для развертывания контейнеров не требуется развертывание уровня 
инфраструктуры. Kubernetes фактически стал стандартом в оркестрации и поставке сервисов 
уровня PaaS для контейнеров. Вопрос построения сервисов, развернутых на виртуальных 
машинах, остается открытым. Это связано с тем, что только провайдеры решают, 
предоставлять открытые интерфейсы для управления ресурсами в облаке или оставлять 
оркестрацию за собой. В виду этого, модули, которые поддерживаются инструментами 
конфигурации ограничены в возможностях. Например, ни Terraform, ни Ansible не могут 
управлять ресурсами в облаке Amazon также гибко, как и Cloud Formation. Именно поэтому, 
новейшие оркестраторы используют унифицированное описание топологий по стандарту 
TOSCA, которое помогает абстрактно описывать ресурсы и ЖЦ ПО, не привязываясь к 
реализациям провайдера. Кроме того, использование стандарта помогает настроить 
сообщение и работу между облачными провайдерами, а это безусловное преимущество 
TOSCA-оркестраторов. 
Для развертывания виртуальных машин в облаке активно используются инструменты 
конфигурации, а также Terraform, однако это инструменты конфигурации можно считать 
устаревшими, ведь они не отвечают уровню, который необходим для оркестрации и 
масштабирования сложных микросервисных систем. 
Borisova A.A., Borisenko O.D. Research of Construction Methods for Cloud Services and Overview of the Implementations TOSCA 
Standard. Trudy ISP RAN/Proc. ISP RAS, vol. 34, issue 5, 2022, pp. 143-162 
158 
В табл. 1 видно, что оркестраторы в основном не предоставляют ресурсы, а лишь управляют 
ими (ведь это задача провайдера). Однако, на самом деле возможно построение такого 
оркестратора, который бы предоставлял платформенные сервисы по запросу. Примером 
реализации является Michman. Сложность построения кроется в том, что Michman "под 
капотом" развертывает и настраивает уровень инфраструктуры, который необходим для 
работы сервиса. Подобные возможности есть только у оркестраторов-провайдеров, таких как 
Kubernetes. Слабой стороной Michman является то, что нельзя запросить любую топологию, 
необходимую пользователю. Эту проблему можно решить, если представить уровень 
инфраструктуры в Michman через TOSCA. Такое представление уже реализовано в 
оркестраторе уровня инфраструктуры - Clouni.  
Еще из данных табл. 1 можно понять, что гибкое и продвинутое управление ЖЦ ПО является 
основным преимуществом систем оркестрации. Эти функции также необходимо реализовать 
в эффективном оркестраторе. 
Табл. 1. Таблица сравнения систем Оркестрации 
Tab. 1. Competitive analysis of orchestration systems 
Инструмент 
Совместимость 
с провайдером 
Возможность 
управления 
уровнем 
предоставления 
ресурсов 
Уровень 
предоставления 
ресурсов 
Имеется 
встроенный 
редактор 
топологий 
Совместимость 
с 
инструментами 
конфигурации 
Инструменты, позволяющие осуществлять оркестрацию 
Terraform 
OpenStack, 
AWS, Azure, 
GCP, 
Kubernetes, etc 
IaaS, PaaS 
- 
Нет 
- 
Ansible 
OpenStack, 
AWS, Azure, 
GCP, 
Kubernetes, etc 
IaaS, PaaS, 
SaaS 
- 
Нет 
- 
Оркестраторы с поддержкой одного облачного провайдера 
Cloud 
Formation 
AWS 
IaaS, PaaS, 
SaaS, FaaS 
IaaS, PaaS, 
SaaS, FaaS 
да 
  
Heat 
Orchestrator 
OpenStack, 
AWS 
IaaS 
- 
Нет 
  
Michman 
OpenStack 
PaaS 
PaaS 
Нет 
Ansible 
Оркестраторы нескольких облачных провайдеров 
xOpera 
OpenStack, 
AWS, Azure, 
GCP, 
Kubernetes 
IaaS, SaaS, 
FaaS 
- 
Нет 
Ansible 
Cloudify 
OpenStack, 
AWS, Azure, 
GCP, 
Kubernetes, etc 
IaaS, PaaS 
- 
Да 
Ansible, 
Terraform, etc 
Alien4Cloud 
+ 
 Yorc 
OpenStack, 
AWS, GCP, 
Kubernetes, etc 
IaaS 
- 
Да 
Terraform 
Clouni 
OpenStack, 
AWS, 
Kubernetes 
IaaS 
- 
Нет 
Ansible 
Борисова А.А., Борисенко О.Д. Исследование методов построения облачных платформенных сервисов и реализаций стандарта 
TOSCA. Труды ИСП РАН, том 34, вып. 5, 2022 г., стр. 143-162 
159 
Оркестраторы контейнеров 
Swarm 
Есть 
возможность 
поставки при 
помощи 
других 
провайдеров 
PaaS 
- 
Нет 
Ansible, 
Terraform, etc 
Kubernetes 
Выступает как 
провайдер + 
есть 
возможность 
поставки при 
помощи 
других 
провайдеров 
IaaS, PaaS 
IaaS, PaaS 
Нет 
Ansible, 
Terraform, etc 
Табл. 1. Продолжение 1 
Table 1. Continuation 1 
Инструмент 
Автоматический 
мониторинг 
Валидация 
зависимостей 
Использует 
ли TOSCA? 
Автоматическое 
отслеживание 
состояния 
единиц 
развертывания 
Возможность 
автоматического 
создания резервной 
копии и 
восстановления из 
нее 
Инструменты, позволяющие осуществлять оркестрацию 
Terraform 
Нет 
Нет 
Нет 
Нет 
Да 
Ansible 
Нет 
Нет 
Нет 
Нет 
Да 
 (не авт.-е) 
Оркестраторы с поддержкой одного облачного провайдера 
Cloud 
Formation 
Да 
Нет 
Нет 
Да 
Да 
Heat 
Orchestrator 
Нет 
Нет 
Да 
Нет 
Нет 
Michman 
Нет 
Да 
Планируется 
 переход 
Да 
Нет 
Оркестраторы нескольких облачных провайдеров 
xOpera 
Нет 
Нет 
Да 
Нет 
 
Cloudify 
Да 
Нет 
Да 
Да 
 
Alien4Cloud 
+ 
 Yorc 
Да 
Нет 
Да 
Да 
 
Clouni 
Нет 
Да 
Да 
Нет 
 
Оркестраторы контейнеров 
Swarm 
Нет 
Нет 
Нет 
Да 
Нет 
Kubernetes 
Да 
Нет 
Нет 
Да 
Да 
Borisova A.A., Borisenko O.D. Research of Construction Methods for Cloud Services and Overview of the Implementations TOSCA 
Standard. Trudy ISP RAN/Proc. ISP RAS, vol. 34, issue 5, 2022, pp. 143-162 
160 
Табл. 1. Продолжение 2 
Table 1. Continuation 2 
Инструмент 
Возможность 
автоматического 
возврата ПО к 
предыдущей 
версии 
Возможность 
автоматического 
масштабирования 
Возможность 
развертывание 
изменений для 
части сервисов 
Инструменты, позволяющие осуществлять оркестрацию 
Terraform 
Нет 
Нет 
Нет 
Ansible 
Да 
 (не авт.-е) 
Да 
 (не авт.-е) 
Да 
 (не авт.-е) 
Оркестраторы с поддержкой одного облачного провайдера 
Cloud 
Formation 
Да 
Да 
Да 
Heat 
Orchestrator 
Нет 
Нет 
Нет 
Michman 
Нет 
Да 
Нет 
Оркестраторы нескольких облачных провайдеров 
xOpera 
Нет 
Нет 
Нет 
Cloudify 
Да 
Да 
Да 
Alien4Cloud + 
 Yorc 
Нет 
Нет 
Нет 
Clouni 
Нет 
Нет 
Нет 
Оркестраторы контейнеров 
Swarm 
Да 
Да 
Нет 
Kubernetes 
Да 
Да 
Да 
3.2 Проект решения 
Далее будет описано решение по созданию единого оркестратора и будет описан требуемый 
функционал, но реализация не будет осуществлена в рамках работы. 
Сравнительный анализ позволил выявить сильные и слабые стороны инструментов и 
функции, которые необходимо реализовать в оркестраторах, разработанных в ИСП РАН. 
К тому же становится понятно, что не существует полнофункционального TOSCA 
оркестратора, в котором были бы полностью реализованы функции автоматизации. 
Michman предоставляет сервисы PaaS по запросу как услугу. Однако для развертывания 
сервиса 
Michman 
использует 
заранее 
подготовленные 
скрипты 
развертывания 
инфраструктуры, которую нельзя дополнить и гибко настроить. Michman может развернуть 
один мастер узел и неограниченное количество узлов с рабочей нагрузкой в кластере. Во 
время удаления сервиса, Michman удаляет кластер целиком по причине того, что 
развертывание инфраструктуры и сервисов сильно связаны в инструменте через 
конфигурационные переменные. Этап создания инфраструктуры и этап развертывания 
Борисова А.А., Борисенко О.Д. Исследование методов построения облачных платформенных сервисов и реализаций стандарта 
TOSCA. Труды ИСП РАН, том 34, вып. 5, 2022 г., стр. 143-162 
161 
сервисов необходимо разделить. Тогда можно будет масштабировать инфраструктуру и 
сервисы не зависимо. Также необходимо реализовать поэтапное и раздельное развертывание 
и удаление.  
Clouni позволяет унифицировано описывать единожды TOSCA template и развертывать 
инфраструктуру для нескольких облачных провайдеров. Однако в Clouni не реализована 
возможность предоставлять сервисы по запросу. 
Выгодное решение – соединить оба оркестратора для совместной и эффективной работы.  
Проектируемый TOSCA оркестратор должен принимать на вход унифицированный шаблон 
описания топологии инфраструктуры и сервисов. На выходе пользователь должен получить 
уже развернутую инфраструктуру или сервис в выбранном облачном провайдере с 
возможностью дальнейшей оркестрации и отслеживания статуса.  
Функции оркестратора: 
 
графическое моделирование топологии; 
 
создание, удаление, обновление топологии; 
 
 создание, удаление, обновление инфраструктуры для заданной топологии; 
 
отслеживание состояния единицы развертывания для заданной топологии; 
 
добавление нового провайдера; 
 
добавление добавление поддержки нового сервиса; 
 
валидация топологии и зависимостей в ней; 
 
автоматическое создание резервной копии единицы развертывания и восстановление из 
копии; 
 
автоматический возврат единицы развертывания к предыдущей версии; 
 
автоматическое масштабирование единицы развертывания; 
 
развертывание изменений на часть единиц развертывания. 
Новый оркестратор будет ориентирован прежде всего на OpenStack и Ansible, однако в 
процессе 
проектирования 
необходимо 
закладывать 
возможность 
добавления 
поддерживаемых провайдеров и инструментов конфигурации. 
В конечной реализации должны использоваться OpenStak-специфичные типы. Состояние 
сервисов должно быть реализовано в соответствии с дополнением к стандарту TOSCA 
Instance model [21]. Механизм подстановки параметров, необходимых для получения 
сценария для заданного инструмента конфигурации должен быть реализован через TOSCA 
substitute. После реализации этих требований можно будет реализовать механизм 
мониторинга 
4 Заключение 
В ходе описанной работы были решены следующие задачи: 
 
исследованы отличия оркестрации виртуальных машин с сервисами от оркестрации 
контейнеров; 
 
исследованы инструменты оркестрации контейнеров; 
 
исследованы инструменты конфигурации; 
 
исследованы механизмы оркестрации крупных облачных провайдеров; 
 
исследованы оркестраторы на базе унифицированного стандарта TOSCA; 
 
исследованы оркестраторы, разработанные в ИСП РАН; 
 
проведен сравнительный анализ методов построения облачных платформенных 
сервисов; 
 
предложен проект решения по объединению работы оркестраторов ИСП РАН. 
Borisova A.A., Borisenko O.D. Research of Construction Methods for Cloud Services and Overview of the Implementations TOSCA 
Standard. Trudy ISP RAN/Proc. ISP RAS, vol. 34, issue 5, 2022, pp. 143-162 
162 
Исходя из результатов работы были выявлены функциональные требования, необходимые 
для реализации оркестратора. В работе описан проект решения по объединению работы 
оркестраторов, разрабатываемых в ИСП РАН. Проект одобрен к реализации.  
Реализация оркестратора, расширяемого путем добавления поддержки новых провайдеров и 
инструментов конфигурации, возможна при помощи стандарта TOSCA. Также, были 
проанализированы метрики эффективности внедрения инструментов оркестрации, которые 
можно будет использовать в дальнейшем. 

Труды ИСП РАН, том 31, вып. 6, 2019 г. // Trudy ISP RAN/Proc. ISP RAS, vol. 31, issue 6, 2019 
125 
DOI: 10.15514/ISPRAS-2019-31(6)-7 
Кэширование данных в мультиконтейнерных 
системах 
1 Д.А. Грушин, ORCID: 0000-0002-6789-5473 <grushin@ispras.ru> 
1,2 Д.O. Лазарев, ORCID: 0000-0002-6253-6447 <denis.lazarev@phystech.edu> 
1,2С.А. Фомин, ORCID: 0000-0002-1151-2189  <fomin@ispras.ru> 
1 Институт системного программирования им. В.П. Иванникова РАН, 
109004, Россия, г. Москва, ул. А. Солженицына, д. 25 
2 Московский физико-технический институт, 
141700, Россия, Московская область, г. Долгопрудный, Институтский пер., 9   
Аннотация. Сегодня виртуализация – это ключевая технология облачных вычислений и 
современных центров обработки данных, обеспечивающая масштабируемость и безопасность, 
управление глобальной ИТ-инфраструктурой и снижение затрат. Среди методов виртуализации, 
наиболее популярной стала контейнеризация – изоляция связанных групп процессов, 
разделяющих общее ядро операционной системы. Эффективность контейнеризации в сравнении с 
классической аппаратной виртуализацией, проявляется в компактности контейнеров и меньших 
накладных затратах вычислительных ресурсов – памяти, диска, ЦПУ.  Однако в сравнении с 
классическими архитектурами без изоляции процессов контейнеры могут обходиться дороже, и в 
любом случае, индустрия ждет дополнительной оптимизации – скорости запуска, экономии 
памяти и дискового пространства и других ресурсов. В этом может помочь различное 
кеширование – старейший механизм повышения производительности программ без радикальной 
модификации алгоритма и оборудования. Однако при этом возникают различные архитектурноинженерные дилеммы вида «безопасность или эффективность» и здесь мы рассмотрим 
современные научно-технические подходы к их решению в разных аспектах – ускорение запуска, 
оптимизация совместного использования, ускорение сборки образов, а также некоторые проблемы 
безопасности, возникшие из-за агрессивного кэширования в процессорных архитектурах. А в 
некоторых сценариях использования мультиконтейнерных систем наоборот, скорость и задержки 
не важны, важно обеспечить максимальную загрузку физических серверов – в этом случае 
актуальны алгоритмы планирования и размещения контейнеров, и нами приведен обзор 
теоретических работ на эту тему. 
Ключевые слова: контейнеры; кеширование; облачные вычисления 
Для цитирования: Грушин Д.А., Лазарев Д.О., Фомин С.А. Кэширование данных в 
мультиконтейнерных системах. Труды ИСП РАН, том 31, вып. 6, 2019 г., стр. 125–144. DOI: 
10.15514/ISPRAS–2019–31(6)–7 
Благодарности: Исследования, результаты которых представлены в этой статье, выполнены при 
поддержке Российского фонда фундаментальных исследований (проект 17-07-01006). 
 
 
Grushin D.A., Lazarev D.O., Fomin S.A. Data caching in multi-container systems. Trudy ISP RAN/Proc. ISP RAS, vol. 31, issue 6, 
2019. pp. 125-144 
126 
Data caching in multi-container systems 
1 D.A. Grushin, ORCID: 0000-0002-6789-5473 <grushin@ispras.ru> 
1,2 D.O. Lazarev, ORCID: 0000-0002-6253-6447 <denis.lazarev@phystech.edu> 
1,2S.A. Fomin, ORCID: 0000-0002-1151-2189  <fomin@ispras.ru> 
1 Ivannikov Institute for System Programming of the Russian Academy of Sciences, 
25, Alexander Solzhenitsyn st., Moscow, 109004, Russia 
2 Moscow Institute of Physics and Technology (State University), 
9 Institutskiy per., Dolgoprudny, Moscow Region, 141700, Russia 
Abstract. Today, virtualization is a key technology for cloud computing and modern data centers, 
providing scalability and security, managing the global IT infrastructure and reducing costs. Among the 
methods of virtualization, the most popular was containerization, that is the isolation of related groups of 
linux processes that share a common Linux kernel. Containerization is more profitable that classical 
hardware virtualization because of compactness of containers and lower overhead costs of memory, disk, 
CPU.  However, in comparison with classical architectures without process isolation containers can cost 
more, and in any case, the industry is waiting for additional optimization – the speed of launch, saving 
memory and disk space and other resources. Different caching techniques can help in this, because 
Caching is the oldest mechanism of increasing software productivity without radical modification of 
algorithms and hardware. However, there are a lot of architectural and engineering tradeoffs. Here we will 
consider modern scientific and technical approaches to their solution in different aspects – acceleration of 
launch, optimization of shared usage, acceleration of building container images, as well as some security 
problems caused by aggressive caching in modern processor architectures. And in some use cases for 
multi-container systems performance and latency are not important, but we have to ensure the maximum 
load of physical servers. In these cases, the algorithms of planning and placement of containers are 
relevant, and we give an overview of theoretical work on this topic. 
Keywords: containers; caching; cloud computing 
For citation: Grushin D.A., Lazarev D.O., Fomin S.A. Data caching in multi-container systems. Trudy 
ISP RAN/Proc. ISP RAS, vol. 31, issue 6, 2019. pp. 125-144 (in Russian). DOI: 10.15514/ISPRAS-201931(6)-7 
Acknowledgements: The studies, the results of which are presented in this article, were supported by the 
Russian Foundation for Basic Research (project 17-07-01006). 
1. Введение 
Кеширование – один из важнейших механизмов повышения производительности, без 
радикальной модификации алгоритма и оборудования. К сожалению, в реальности не 
существует компьютеров, реализующих теоретическую модель RAM – Random Access 
Machine, машину с произвольным доступом к памяти, способную за один условных такт 
дотянутся до любой ячейки однородной памяти. В реальных вычислениях существует 
очень строгая иерархия доступа к памяти, от наносекунд к кэшам первого уровня 
процессора, до секунд доступа к удаленным сетевых хранилищам на шпиндельных 
жестких дисках.  
Как пример, можно привести ориентировочные времена доступа к различным 
устройствам (иерархия – кэши процессора, память, ssd-диски, сеть, … [1], табл. 1). 
Видно, что каждый уровень «попадания в кеш» (не важно, речь идет о диске при 
кэшировании сети, или про кэш первого уровня при кэшировании  второго уровня), 
может увеличить производительность на порядки. Кроме того, кеширование бывает и 
вычислительным, т.е. обеспечивается быстрый доступ не только к элементам, 
хранящимся на более «низком» уровне систем хранения, но и сохраненным результатам 
вычисления, требующим серьезных временных затрат.  Кеширование – старая 
Грушин Д.А., Лазарев Д.О., Фомин С.А. Кэширование данных в мультиконтейнерных системах. Труды ИСП РАН, том 31, вып. 
6, 2019 г., стр. 125–144 
127 
технология, термин «cache» появился в 1967 году, но практически этот подход – 
ровесник «электронно-вычислительных машин». 
Табл. 1. Иерархия типов памяти и задержки доступа 
Table 1. Hierarchy of memory types and access delays 
Операция 
Задержка 
(в наносек) 
Доступ к кэшу первого уровня 
0.5  
Доступ к кэшу второго уровня 
7  
Доступ к DRAM 
100  
3D XPoint на основе чтения NVMe SSD 
10000 
NAND NVMe SSD R/W 
20000  
NAND SATA SSD R/W 
50000 
Случайное чтение блоков 4K с SSD 
150000  
Задержка P2P TCP/IP (физика на физику) 
150000  
Задержка P2P TCP/IP (ВМ на ВМ) 
250000  
Последовательное чтение 1MB из памяти 
250000  
Сетевая задержка внутри ЦОД 
500000  
Последовательное чтение 1 MB с SSD 
1000000  
Поиск диска 
10000000  
Последовательное чтение 1 MB с диска 
20000000 
Отправка пакетов США → Европа → США 
150000000 
 С другой стороны, контейнеры – недавно появившаяся технология обеспечения 
дешевой изоляции групп процессов, давшая огромный развитие IT-индустрии, родившая 
такие практики как DevOps и микросервисная архитектура. Конечно, сама идея 
некоторой изоляции процессов на уровне операционной системы не нова – «chroot» был 
введен в 1982 году, но реальная изоляция процессов появилась только в начале нулевых 
с FreeBSD Jail, эволюционировала к концу нулевых уже к промышленным технологиям 
LXC, Solaris Containers и OpenVZ/Virtuozzo, но только с появлением Docker в середине 
десятых годов нашего века стала поистине массовой, настолько, что большинство 
ITшников при слове «контейнеры» вспоминают только технологии от «докер», что 
безусловно обидно для первопроходцев контейнеризации.  
Контейнеры практически окончили войну между разработчиками и системными 
администраторами, дав возможность первым практически никак не зависеть от 
тонкостей целевых платформ, и их  особенностей – будь то операционная система и ее 
дистрибутив, или оборудование с его глюками.  Теперь, для каждой выкатки новой 
версии не требуется многомесячное согласование «отдела разработки» и «отдела 
администрирования», появилась возможность выкатывать в «production» новые версии 
ежечасно, более того, появились новые архитектуры оркестрации контейнеров, 
позволяющие экономить на долгом цикле тестирования и поддерживать в работе 
несколько версий сервиса, автоматически переключаясь на проверенную версию, в 
случае проблем с новой. Все это, а также тенденция к глобализации вычислительных 
ресурсов, обозначаемых сильно заезженным маркетинговым термином «облака», сильно 
уронило престиж и ценность «бородатых сисадминов», хранителей знаний о 
взаимодействии 
зоопарка 
различного 
оборудования 
и 
операционных 
систем. 
Большинству из них пришлось переквалифицироваться в DevOps инженеров, 
Grushin D.A., Lazarev D.O., Fomin S.A. Data caching in multi-container systems. Trudy ISP RAN/Proc. ISP RAS, vol. 31, issue 6, 
2019. pp. 125-144 
128 
специалистов по сборке контейнеров, развертывании вычислительной инфраструктуры, 
и они стали близки с разработчиками как по навыкам – «программирование 
инфраструктуры как кода» вместо беготни с оборудованием, так и по взаимодействию – 
DevOps специалистов принято размещать вместе с командой разработки.   
 
Рис. 1. Архитектурная выгода контейнеров в сравнении с классической виртуализацией 
Fig. 1. The architectural benefits of containers versus classic virtualization 
Даже если не используются микросервисные архитектуры и используется классический 
подход «виртуальная машина с сервисами», зачастую предоставляемая провайдерами 
виртуальная машина – это тоже контейнер (LXC или OpenVZ), ведь таким образом 
достигается серьезная экономии памяти (все контейнеры используют только одно ядро 
ОС), и достигается их большая плотность размещения на физических серверах (рис. 1). 
Но разумеется, весь этот тренд на изоляцию и дублирование процессов, обеспеченный 
контейнеризацией находится в некоторой оппозиции к эффективности использования 
памяти и ресурсов, примерно в такой же, в которой «конфликтуют» архитектурные 
аспекты «эффективности» и «безопасности». Соответственно, возникают инженерные и 
научные проблемы, как решить эти технологические дилеммы, как сохранить 
эффективность при использовании контейнеров, какого рода кеширование, и в каком 
случае, может тут помочь. 
Далее мы приведем несколько разделов с обзором различных аспектов темы 
«контейнеры и кеши», рассмотрев современные научно-практические подходы к их 
решению в разных аспектах – ускорение запуска, оптимизация совместного 
использования, ускорение сборки образов, а также некоторые проблемам безопасности, 
возникшим из-за агрессивного кэширования в процессорных архитектурах.  
В некоторых сценариях использования мультиконтейнерных систем наоборот, скорость 
и задержки не важны, важно обеспечить максимальную загрузку физических серверов – 
в этом случае актуальны алгоритмы планирования и размещения контейнеров – и мы 
привели обзор некоторых теоретических алгоритмов, еще не реализованных в системах 
виртуализации или контейнерной оркестрации. 
Грушин Д.А., Лазарев Д.О., Фомин С.А. Кэширование данных в мультиконтейнерных системах. Труды ИСП РАН, том 31, вып. 
6, 2019 г., стр. 125–144 
129 
2. Межконтейнерное совместное использование файлов 
Несмотря на то, что контейнеры сильно выигрывают в плотности размещения по 
сравнению с виртуальными машинами, есть множество областей, где сложились 
практики обеспечивающие еще большую плотность сервисов. Одна из таких областей – 
так называемый shared-хостинг, т.е. хостинг, в котором все сайты живут внутри одной 
операционной системы (Linux), пользуясь общим сервисом СУБД (обычно MySQL) под 
разными аккаунтами, общим веб-сервером (обычно связка Apache+PHP), так 
называемым классическим веб-стеком LAMP, под которым до сих пор работают 
большинство популярных CMS, Content Management Systems, систем управления 
контентом – таких как Wordpress, Drupal, Joomla, Bitrix, Mediawiki …, т.е. все те же 
фреймворки, на которых до сих пор держится большинство сайтов – от блогов и сайтов 
визиток, до порталов сообществ и интернет магазинов (рис. 2). 
 
Рис. 2. Архитектура LAMP – Linux, Apache, MySQL, PHP 
Fig. 2. LAMP architecture – Linux, Apache, MySQL, PHP 
Плотность размещения сайтов внутри при разделяемом хостинге (шаред хостинге, до 
сих пор нет устоявшегося русского перевода для термина shared hosting) может быть на 
порядок больше, чем при хостинге отдельных сайтов внутри виртуальных машин и даже 
контейнеров, и по сути, ограничена только совестью провайдеров. Обычно, провайдеры 
пользуются тем, что такие дешевые услуги покупают владельцы слабо посещаемых 
сайтов – непопулярные блоги, сайты-визитки малоизвестных компаний, хобби-проекты, 
и часто им удается размещать до десятков тысяч сайтов внутри одного физического 
сервера. 
Однако эта дешевизна имеет и оборотную сторону. В виду отсутствия настоящей 
изоляции процессов, один сайт получивший внезапную нагрузку, «выедает» все 
вычислительные ресурсы физического сервера (процессор, память, дисковые IOPSы, 
сеть) и «кладет», т.е. вызывает отказ в обслуживании у всех остальных сайтов на том же 
сервере.  Причем эта нагрузка может быть не обязательно в результате DDOSа, 
возможно просто внезапный всплеск посещаемости, когда ссылка на сайт попадает на 
популярных ресурс («reddit-эффект» или «хабра-эффект», по названию известных 
коллективных блогов, по ссылке с которых могут пойти одновременно десятки тысяч 
пользователей). Контейнеризация, т.е. изоляция в отдельных контейнерах хотя бы 
процессов вебсервера могла бы решить эту проблему, т.к. для отдельного контейнера 
можно поставить ограничения по потреблению CPU и памяти.  
 
 
Grushin D.A., Lazarev D.O., Fomin S.A. Data caching in multi-container systems. Trudy ISP RAN/Proc. ISP RAS, vol. 31, issue 6, 
2019. pp. 125-144 
130 
 
 
 
 
 
 
 
Рис. 3 Преимущество разделяемого хостинга – в плотности 
Fig. 3 The advantage of shared hosting is density 
Да, контейнеры были бы лучше, если бы их удалось сделать более экономичными, не 
дублирующими ни общие файлы библиотек и системных процессов (glibc, apache-nginxphp-python-mysql-postgres), ни развесистые файлы прикладных фреймворков на PHP и 
других языках, которые могут занимать сотни мегабайтов дискового пространства, и, 
что более вредно, одни и те же файлы в разных контейнерах, занимают свое собственное 
место в страничном кэше файловой системы в оперативной памяти (рис. 3)! 
Аналогичная проблема возникает и вне шаредхостинга, в микросервисной архитектуре, 
когда приложение разбито на десятки микросервисов, каждый из которых тянет с собой 
как минимум ограниченную версию дистрибутива и легкий веб-сервер (обычно nginx), а 
максимум… не ограничен, и многогигабайтные образы контейнеров – вполне не 
редкость. 
 
Рис. 4. Слои overlayfs в docker-контейнерах 
Fig. 4. Layers of overlayfs in docker containers 
 
Рис. 5. Иерархия и наследование docker-образов 
Fig. 5. Hierarchy and inheritance of docker images 
Грушин Д.А., Лазарев Д.О., Фомин С.А. Кэширование данных в мультиконтейнерных системах. Труды ИСП РАН, том 31, вып. 
6, 2019 г., стр. 125–144 
131 
C проблемой экономии дискового пространства удалось справиться технологии Dockerобразов, использующих с одной стороны, технологию «слоистой» файловой системы 
overlay-fs (рис. 4), когда можно добавлять свои «дельта-слои», добавляющие (или 
удаляющие) какие-то файлы к уже известным и неизменным образам, с другой – 
выстраивая эти образы в иерархию зависимостей (рис. 5), плюс инфраструктура 
построения и распространения таких образов (docker-реестр). 
Однако эта технология, overlayfs, не подходит для классического шаредхостинга, где не 
очень продвинутый пользователь может только «положить и поправить» PHP-файлы 
(несмотря на то, что в сотнях соседних контейнеров они уже есть, и есть общий докеробраз, включающих нужный PHP-фреймворк), либо где такие общие файлы образуются 
множеством других образов в обход технологии докер-образов. Или докер вовсе не 
используется, а используется OpenVZ, LVX или Virtuozzo. Ну и в целом, нельзя заранее 
выстроить целостную иерархию файловых слоев, каждый контейнер управляется 
непредсказуемо и независимо, но в целом, очень много одинаковых файлов и очень 
хотелось бы что-то с этим сделать. 
В таком случае, можно использовать технологию PFCache, VZFs, или аналогичную, 
позволяющую дедуплицировать операции над файлами.  Идея этого подхода состоит в 
следующем. Файловые системы для процессов внутри контейнера создавать не 
напрямую на блочных устройствах хоста, а на специальном промежуточном блочном 
устройстве (ПБУ). 
Это ПБУ можно создать (рис. 6) 
 
на обычных дисках хоста с помощью Union FS; 
 
в файлах хостовой системы, с помощью Loop device или Ploop, ZFS ZVol или 
подтомов BTRFS; 
 
LVM-разделов на блочном устройстве хоста; 
 
на сетевом кластере, с помощью, например, Ceph RBD. 
В любом случае, у нас возникает потенциальная возможность контроля за системными 
файловыми вызовами внутри контейнера.   
 
Рис. 6. Промежуточное блочное устройство для перехвата обращений к файлам 
Fig. 6. Intermediate block device for intercepting file access 
Grushin D.A., Lazarev D.O., Fomin S.A. Data caching in multi-container systems. Trudy ISP RAN/Proc. ISP RAS, vol. 31, issue 6, 
2019. pp. 125-144 
132 
Далее заводится дисковый кеш на отдельном ПБУ, выделенном под эту задачу (на 
быстром устройстве, например, SSD NMVE). Этим ПБУ будут пользоваться все 
контейнеры/VPS внутри узла – физического или виртуального сервера.  Затем, для 
процессов в контейнерах используется файловая система Ext4, где каждый файл, кроме 
имени, может нести дополнительные атрибуты в метаинформации (xattr) (рис. 7).  
 
Рис. 7. Кэширование обращений к файлам с использованием файловых атрибутов ext4 
Fig. 7. Caching file access using ext4 file attributes 
В процессе эксплуатации происходит следующее. 
1. На уровне контейнера, специальный пользовательский сервис прописывает SHA1 
хеши файлов в их мета-атрибут xattr. 
2. На уровне ядра:  
a. нужно периодически собирать статистику чтения файлов из ядра, анализирует 
ее, и добавлять файлы в кэш, если их использование частое; 
b. при чтении файла проверяется, содержит ли он указанный хеш в расширенных 
атрибутах xattr; если содержит – открывается «общий» файл, вместо файла 
контейнера;  
c. при записи в файл хеш инвалидируется; таким образом, при последующем 
открытии будет открываться уже непосредственно файл контейнера, а не его 
кеш. 
В результате, держа в страничном кэше в основном «общие файлы» для всех 
контейнеров, получается значительная экономия как самого кэша и, следовательно, 
оперативной памяти, а также существенной экономии на дисковых операциях, ведь 
вместо чтения десяти файлов с диска, читается один, который сразу идет в страничный 
кэш. 
Более детально, для технологии Pfcache с конкретными командами администрирования, 
этот процесс рассмотрен в статье [2], для технологии VZFS в [3], но аналогичный подход 
вполне может быть реализован и самостоятельно. 
3. Кеширование старта контейнера 
Контейнеры, автоматически запускаемые системой оркестрации, такой как Kubernetes, 
дали «путевку в жизнь» не только микросервисной архитектуре, но и такому модному 
Грушин Д.А., Лазарев Д.О., Фомин С.А. Кэширование данных в мультиконтейнерных системах. Труды ИСП РАН, том 31, вып. 
6, 2019 г., стр. 125–144 
133 
тренду как «serverless», когда разработчики практически ничего не знают о целевой 
платформе, и реализуют только отдельные, максимально атомарные функции на 
удобных для них языках программирования. Всю остальную работу – запуск этих 
функций 
в 
изолированных 
контейнерах, 
балансировка, 
и 
самое 
главное 
– 
автоматическое масштабирование – делает за них система оркестрации. Т.е. больше нет 
необходимости в редких специалистах по «highload»-архитектуре, снобах-архитекторах, 
умеющих рисовать диаграммы балансировки, или слишком дорогих разработчиках, 
изобретающих свои велосипеды по динамическому масштабированию числа сервисов 
при росте нагрузке. Все это может взять на себя система оркестрации, и скорее всего, это 
будет самый популярный за последние годы, проект Kubernetes. 
Но есть еще один момент на переднем крае «технологических проблем» – проблема 
задержки при холодном старте. Действительно, при внезапном росте нагрузки (приход 
новых пользователей, неожиданный всплеск), система оркестрации автоматически 
поднимет дополнительные контейнеры для обслуживания… но сам старт этих 
контейнеров – дело не быстрое. Тут и старт самого контейнера (что, например, в docker, 
далеко не мгновенно), и самое главное – старт собственно сервисов внутри контейнера. 
Не важно, написаны они на «медленном» python, или «быстрых» NodeJS или Java, старт 
их может занимать сотни миллисекунд, а то и секунды/минуты. Если для обычного 
сайта, и даже интернет-магазина это наверное не является критической проблемой, то 
есть множество сервисов, обычно имеющих в своих названиях «real time», требующих 
реакцию в считанные миллисекунды (обычно порядка сотни мс). Это могут быть разные 
разновидности роботизированных торговых агентов, причем не обязательно для 
понятных «человеческих» бирж, где торгуют валютами и ценными бумагами, а 
например, real time bidding (RTB) для рекламных систем. В тот момент, когда 
пользователь открывает какую-нибудь страницу в интернете, пока еще идет загрузка, во 
множестве рекламных систем RTB-систем происходит соревновательная торговля 
ботов-агентов, за то, какую рекламу показать этому пользователю. Ну и разумеется, 
быстрая реакция будет нужна и для растущего рынка IoT устройств, особенно 
медицинских. 
Тут как раз очень востребовано упомянутое в «введении» вычислительное кеширование. 
Как бы сделать так, чтобы запускаемые контейнеры уже оживали в рабочем состоянии, а 
не мучительно читали конфигурационные файлы и собирали в памяти нужные 
библиотеки и кеши? 
Хорошие новости в том, что для этой задачи сейчас удается применить интересный 
проект CRIU – Checkpoint/Restore In Userspace. Рожденный русскими разработчиками 
Virtuozzo в 2014 (см. [4]), сейчас он стал уже стабильным и надежным международным 
продуктом с открытыми исходными кодами, которыми пользуются в своих решениях 
такие гиганты индустрии, как IBM/RedHat и Google. Изначально этот проект был создан 
для решения проблемы «живой» миграции контейнера, без остановки, с одного 
физического узла на другой. Подобное решение для виртуальных машин уже было, и 
несмотря на сложность живой миграции виртуальных машин, стоит признать, что оно 
алгоритмически и системно проще – работает на уровне гипервизора, перемещает 
постепенно блоки диска и оперативной памяти, пока процесс не сойдется и виртуальная 
машина не оживет на другом сервере. В случае с контейнерами, надо помнить что 
контейнер – просто группа процессов и ресурсов (сокеты, блокировки и т.п.) внутри 
одной ОС, их очень непросто даже собрать вместе с клубком их зависимостей (там все 
сложно, см., например, свежие теоретические модели и алгоритмы [5]), не говоря уже о 
перемещении на другой сервер, и выполнять большую часть работы надо не в 
гипервизоре, в пользовательском пространстве, наравне с перемещаемыми процессами.   
Grushin D.A., Lazarev D.O., Fomin S.A. Data caching in multi-container systems. Trudy ISP RAN/Proc. ISP RAS, vol. 31, issue 6, 
2019. pp. 125-144 
134 
CRIU безусловно является «швейцарским ножом» для целого ряда разнородных задач 
(есть даже использование для «паузы в играх»), но именно возможность «заморозить» и 
«восстановить» группу процессов, оказалась той самой «серебряной пулей» для задачи 
быстрого «холодного старта», которую и стали пытаться использовать практически 
только год назад, добиваясь отличных результатов.  
Так, в докладе [6] приведены приведенные в табл. 2 цифры обычного времени старта 
против восстановления через CRUI для тестовых приложений на разных современных 
фреймворках – видно, что стабильно получается выигрыш практически на порядок, 
сводя задержку старта к уровню сетевой между географически удаленными 
датацентрами. 
Табл. 1: Выигрыш в скорости запуска контейнера при использовании CRIU 
Table 2: Gain in container launch speed when using CRIU 
Время до старта (мс) 
python 
3 
nodejs 6 
java 8 
Обычный старт 
919 
743 
566 
Восстановление 
78 
91 
89 
Более того, аналогичный подход можно применять не только для того чтобы запускать 
миниатюрные микросервисы, но и чтобы реплицировать готовые базы или кэширующие 
сервисы (т.к. решать проблему «прогрева кэшей» для веб-сервисов). 
Также в [6] (кстати, это исследование от IBM и Red Hat) приводится эксперимент с 
наполнением кэширующих NoSQL баз данных 100K короткими записями в сравнении с 
миграцией уже подготовленного контейнера (табл. 3). 
Табл. 2: Выигрыш в скорости репликации хранилища при использовании CRIU 
Tab. 3: Gain in storage replication speed when using CRIU 
 
memcached 
redis 
Наполнение 
1.238 
6.254 
Миграция 
0.806 
1.671 
Прямо сейчас еще нет проверенных, стабильных решений, реализующих этот подход, 
есть попытки использовать его в стартапе SwiftyCloud1 и в проекте gVisor2, но эта 
технология уже в плане развития Kubernetes, можно ожидать, что появится в ближайшем 
году. 
Отдельный вопрос при запуске контейнеров системой оркестрации на распределенном 
ЦОД – обеспечение быстрой доступности самих образов на физических нодах. При 
быстром интерконнекте (10GB Ethernet, или Infiniband, с низкими задержками) эта 
проблема может быть решена использованием кластерной файловой системой, 
CEPH/GlusterFS/ZFS, или использованием специализированных сетевых хранилищ. Но 
если быстрый интерконнект отсутствует, или невозможен, например, в случае 
географически распределенных ЦОД, возникает задача предварительной планировки 
размещения образов контейнеров на всех физических узлах. Да, можно заранее 
реплицировать реестр всех контейнеров на все узлы, но это может привести и к 
существенному расходу дискового пространства и сетевой ширины канала, поэтому 
сейчас есть исследования, как размещать на узлах только часть докер-образов, 
основываясь на статистике использования или различных алгоритмах, таких как 
многомерная упаковка контейнеров [7]. 
                                                           
1 https://github.com/swiftycloud 
2 https://gvisor.dev 
Грушин Д.А., Лазарев Д.О., Фомин С.А. Кэширование данных в мультиконтейнерных системах. Труды ИСП РАН, том 31, вып. 
6, 2019 г., стр. 125–144 
135 
4. Кэширование, контейнеры и безопасность 
Наверное 
самой 
большой 
иллюстрацией 
инженерного 
конфликта 
между 
«эффективностью» 
и 
«безопасностью», 
а 
именно, 
между 
«кешированием» 
и 
«изоляцией», явилась открытая в 2018 году серия уязвимостей, известная теперь под 
«брендами» MELTDOWN и SPECTRE. Смешные картинки-логотипы (рис. 8), должны 
всем донести разницу – если «Meltdown» – «утечка» памяти ядра в юзерспейс, вещь 
неприятная, но которую уже придумали, как починить, то «Spectre» – страшное 
привидение, проникающее через любые закрытые двери («изолирующие песочницы», 
проверки в совершенно корректных программах). 
 
Рис. 8: Логотипы уязвимостей Meltdown и Spectre 
Fig. 8: Vulnerability logos for Meltdown and Specter 
Для тех, кто не в курсе, напомним основные идеи этих уязвимостей. Гонка последних 
десятилетий 
за 
производительностью 
процессоров, 
чтобы 
соответствовать 
маркетинговым ожиданиям так называемого «закона Мура», привела не только к росту 
параллелизма и ядер, но и хитрым техникам реализации микроархитектуры процессоров, 
основанных на аппаратных кешах и спекулятивных вычислениях. Вспомните таблицу 
иерархии задержек доступа к памяти из введения – доступ к оперативной памяти по 
сравнению с кешами процессора в сотни раз дольше, и при любой операции, где операнд 
не попадает в кеш, возникает «голодание по данным» – процессор бессмысленно 
простаивает в ожидании данных, и часто в операциях ветвления в зависимости от 
значения в памяти, выгодно, не дожидаясь выяснения результата в условии, выполнить 
«упреждающие вычисления» (instruction speculation, speculative execution) наиболее 
вероятной ветки, параллельно дождавшись «приехавшего условия», ну и если «не 
повезло», то отбросить вычисленные результаты, восстановив регистры (до оперативной 
памяти результаты все равно не успеют доехать, это еще дольше, чем читать RAM). 
Кроме ветвлений так ускоряют и некоторые циклы, и в целом, это, кроме 
многоядерности, и было основным методом роста производительности процессоров 
основных производителей популярных архитектур (Intel, AMD, ARM, Power), с тех пор, 
как из-за инженерных ограничений практически остановилась миниатюризация и рост 
частоты.  К сожалению, это открыло возможность к еще одной «атаке по побочным 
каналам» (side-channel attack), используя в качестве такого канала время чтения данных.  
Если кратко, то заставив процессор упреждающе выполнить операцию с косвенной 
адресацией по атакуемому участку памяти, разумеется, мы не получаем ни значение 
этой ячейки напрямую, ни то, куда она указывает – это даже не нужно атакующему, и 
процессор, выполнив упреждающе эту операцию, поймет, что сделал это зря, спокойно 
сбросит регистры, но значение атакуемой ячейки осядет в одном из кэшей процессора, в 
подсистеме TLB, Translation Lookaside Buffers, который не сбрасывается вообще 
никогда. Затем, можно устроить перебор выборок значений из собственной памяти, с 
замером времени выполнения, и статистически достоверно «поймать» тот момент, когда 
выборка будет быстрее – и тем самым понять, какое значение осталось в кеше TLB.  В 
целом, это комбинация достаточно известных подходов «исследование состояния 
Grushin D.A., Lazarev D.O., Fomin S.A. Data caching in multi-container systems. Trudy ISP RAN/Proc. ISP RAS, vol. 31, issue 6, 
2019. pp. 125-144 
136 
аппаратуры методом временного прозвона» и  «получение данных, осевших в кэше» 
примененный конкретно к архитектуре процессоров.  Атаки Spectre (их две, первого и 
второго рода) еще более хитрые, там происходит обход проверки границ (bounds check 
bypass) и манипуляция целевым кешем адресов ветвлений (branch target injection). За 
подробными деталями этих атак отошлем к [8, 9], а также десяткам популярных статей и 
презентаций, которых легко найти по «брендам этих уязвимостей», заметим, что многим 
специалистам по ИБ эти уязвимости были известна, и скорее всего, они активно 
эксплуатировалась и раньше. 
Эти уязвимости безусловно были опасны во многих сценариях и вовсе не связанных с 
виртуализацией и контейнерами, везде, куда каким-нибудь образом, можно было 
доставить атакующий код. Так, например, Javascript мог атаковать память браузера и 
вытащить логины-пароли. Но если с Javascript в браузере удалось быстро разобраться – 
загрубление таймеров доступных для JS-кода, модификация JIT компилятора с 
использованием хитрых программных конструкций retpoline, сохраняющих большую 
часть выгод упреждающих вычислений, но сбивающих выполнение этих атак, то с 
областью 
виртуализации, 
особенно 
со 
стороны 
облачных 
провайдеров, 
предоставляющих публичные услуги, или крупных частных корпоративных центров 
данных с виртуальными контурами доступа, и автоматической миграцией виртуальных 
контейнеров между физическими серверами, все хуже, причем именно для 
контейнерной виртуализации. C помощью Meltdown и Spectre пока не удалось 
преодолеть барьер между гостевой системой и гипервизором при физической 
виртуализации, а вот попасть из контейнера в адресное пространство ядра – да, удалось.  
Да, производители оборудования, операционных систем, и систем виртуализации уже 
выпустили множество патчей на всех уровнях: 
1. обновление микрокода для процессоров (особенно Intel и AMD); 
2. обновление ядра ОС Linux и Windows – патчи KPTI (Kernel Page Table Isolation, 
изоляция таблицы страниц ядра), KAISER (Kernel Address Isolation to have Sidechannels Efficiently Removed – изоляция пространства адресов ядра во избежание атак 
по стороннему каналу); 
3. обновление компиляторов, использующихся для сборки критических бинарников – 
ядра ОС, гипервизоров, для реализации в них конструкций retpoline (новорожденное 
слово из «return» и «trampoline») [10]  
Но, во-первых, каждое из таких исправлений может уронить производительность на 1020%, что сильно снижает эффективность контейнерной виртуализации по сравнению с 
аппаратной. Во-вторых, специалисты признают, что все это полумеры, рассчитанные на 
затруднение эксплуатации уже реализованных по этим уязвимостям эксплоитов, и 
надежно избавится от этих проблем можно будет только сменив оборудование, или хотя 
бы процессоры на самые современные. Основные выводы тут такие – сейчас, до смены 
поколения серверов, лучше избегать контейнерной виртуализации при предоставлении 
публичных услуг хостинга, когда недоверенный атакующий код злоумышленника может 
внезапно оказаться среди контейнеров других клиентов, если этого нельзя избежать – то 
обязательно поставить все необходимые патчи на всех уровнях, проверив нагрузочными 
тестами, что это не вызовет критического падения производительности в тех задачах, 
для которых применяются контейнеры. 
5. Кеширование при сборке образов контейнеров 
В отличие от контейнеров LXC и OpenVZ, контейнеры Docker обрели огромную 
популярность среди разработчиков, именно благодаря эффективной реализации 
«шаблонов», «образов» контейнеров, которые, благодаря «слоистой структуре» overlayfs 
Грушин Д.А., Лазарев Д.О., Фомин С.А. Кэширование данных в мультиконтейнерных системах. Труды ИСП РАН, том 31, вып. 
6, 2019 г., стр. 125–144 
137 
и  иерархическим ссылкам обеспечивают дедупликацию данных, и экономию диска и 
памяти. С другой стороны – инфраструктура так называемых докер-реестров, публичных 
и приватных, решает проблему эффективного совместного использования этих образов и 
доставку и выкатку их как на тестовые среды, так и на «боевые» системы.  В результате 
докер-образы стали практически самым универсальным и популярным методом 
упаковки приложения, вытесняя линукс-пакеты, которых надо было раньше мучительно 
собирать под каждый целевой дистрибутив, и разные другие инсталляционные форматы 
– ведь теперь, приложение в виде docker-контейнера можно запустить не только на 
Linux, но даже и на Windows-системах. В результате расцвели практики непрерывной 
интеграции, с сборкой и тестированием приложения после каждого минимального 
изменения.  
Но тут же возникли проблемы максимального ускорения этого процесса – если раньше 
разработчики 
выпадали 
из 
«потока 
разработки», 
оправдываясь, 
что 
оно 
«компилируется», то сейчас, простую «компиляцию» заменила сборка новых докеробразов (установка зависимостей, размещение артефактов, быстрое автоматическое 
тестирование). Даже если простая сборка образа в чистом окружении системы сборки 
будет занимать, скажем, минут 10, то при росте команды и числа коммитов в день 
ожидание готового результата будет расти нелинейно, даже в пуассоновской модели 
теории массового обслуживания [11] (рис. 9). 
 
Рис.9: Нелинейный рост времени ожидания сборки при росте числа коммитов 
Fig. 9: Non-linear increase in build waiting time with an increase in the number of commits 
Решения есть – нужно обеспечить максимальную атомарность всех изменений в докеробразах, чтобы каждое изменение изменяло только один, небольшой «слой» докеробраза, ссылаясь на уже неизменные существующие в докер-реестре образы. Это можно 
сделать на «человеческом уровне», обязав разработчиков и докер инженеров, 
максимально атомизировать собираемые докер-образы при сборке, путем специальных 
соглашений в формировании Dockerfile (см. [11]). Но как любое решение, зависящее от 
людей («стандарты кодирования», «архитектурные политики» и т.п.), это трудно 
соблюдать, и в индустрии есть желание это автоматизировать. 
Так, появляются проекты, такие как Kaniko Cache3, которые автоматически кэшируют в 
докер-реестре все промежуточные слои, вне зависимости от того, как разработчики 
оформляли докер-файлы. Да, это приводит к дополнительным затратам дискового 
пространства, но существенно ускоряет скорость сборки. 
                                                           
3 
https://cloud.google.com/cloud-build/docs/kaniko-cache 
Grushin D.A., Lazarev D.O., Fomin S.A. Data caching in multi-container systems. Trudy ISP RAN/Proc. ISP RAS, vol. 31, issue 6, 
2019. pp. 125-144 
138 
6. Исследования оптимального размещения мультиконтейнерных 
конфигураций на физических серверах 
Кроме рассмотренных ранее сценариев использования, где требовалось экономить время 
запуска или сборки, часто возникают и альтернативные требования – когда скорость 
запуска не важна, а требуется максимальная энергоэффективность оборудования. 
Опуская возможности инженерной оптимизации датацентров – эффективные технологии 
охлаждения 
и 
электропитания, 
на 
программном 
уровне, 
для 
максимальной 
энергоэффективности требуется обеспечить максимальную загрузку имеющихся 
физических серверов, чтобы исключить использование в холостую. Еще недавно [12] 
ситуация была совершенно неудовлетворительной; условно говоря, средний сервер был 
загружен всего лишь на 10%, при этом его совокупное удельное энергопотребление 
падало не больше чем в два раза, от такового при максимальной нагрузке.  
Сейчас, с ростом популярности виртуализации и глобализации вычислительных 
ресурсов в ЦОД крупных провайдеров, таких как Google и Amazon, ситуация 
значительно улучшилась. Впрочем, крупные провайдеры обычно не ставят задачу 100% 
утилизации каждого сервера, оставляя резерв для низколатентного маневрирования 
мощностью под внезапные заказы дополнительной мощности от клиентов, но эта задача 
актуальна для небольших и средних специализированных ЦОД, где задержки адаптации 
к меняющейся нагрузке не так важны, а важна именно максимальная загрузка. Как 
пример, можно привести различные системы обработки пакетных заданий – научных 
расчетов, сборок сложного ПО (линукс-дистрибутивов), рендер-фермы и т.п.  Для таких 
задач еще нет реализованного в ПО виртуализации оптимальных решений, но идут 
исследования ученых, как из университетов, так и крупных компаний (IBM, Redhat, 
VMWare), предлагающих различные алгоритмы эффективного планирования.  
6.1 
Консолидация 
виртуальных 
машин 
на 
основе 
решения 
декомпозированной многоцелевой задачи об упаковке 
В статье [13] специалистов IBM рассмотрена задача размещения виртуальных машин 
(ВМ) на физических серверах, известная как Virtual Machine Consolidation или Virtual 
Machine Packing Problem. Решение использует тот факт, что обычно провайдеры 
виртуальных машин предоставляют виртуальные машины с конечным множеством 
наборов параметров. Задача сведена к 2-шаговому варианту многомерной задачи 
упаковки в контейнеры multi-dimensional bin packing problem. На первом шаге 
виртуальные машины объединяются в кластеры методом 𝑘-средних. ВМ из одного 
кластера изменяются так, что все становятся одинаковыми. Для полученных ВМ 
проводится метод динамического программирования. На втором шаге из полученных 
наборов виртуальных машин строится их расположение на физических серверах. 
Существенно уменьшено время работы алгоритма по сравнению с ранее известными 
алгоритмами, такими как одномерный First Fit и многомерный Bin Packing. 
Метод First Fit для одномерной задачи максимально прост: «Очередной объект 
упаковывается в последний созданный контейнер, в который он помещается. Если 
объект не помещается ни в один контейнер, то для его упаковки создаётся новый, куда 
он и размещается». Зато он может работать в режиме онлайн, когда объекты на вход 
поступают последовательно в некотором неизвестном порядке и при упаковке 
очередного объекта не известны размеры следующих за ним по порядку объектов. В 
этом случае алгоритм гарантирует, что асимптотически отношение суммы размеров 
созданных контейнеров к сумме размеров объектов будет не более, чем 17/10, как 
показано в [14]. Однако оффлайновый алгоритм First Fit, на вход которому подаются 
объекты с размерами, упорядоченными по убыванию, как показано в [15], гарантирует 
Грушин Д.А., Лазарев Д.О., Фомин С.А. Кэширование данных в мультиконтейнерных системах. Труды ИСП РАН, том 31, вып. 
6, 2019 г., стр. 125–144 
139 
уменьшение асимптотики данной величины до 11/9. Данный алгоритм известен как First 
Fit Decreasing и в [13] применяется его обобщение на многомерный случай – 
«Иерархический многомерный алгоритм Bin Packing для виртуальных машин». 
Задача размещения виртуальных машин сводится к многомерной задаче упаковке в 
контейнеры, где измерения соответствуют ресурсам различных типов, которые 
необходимы для создания ВМ, а именно: CPU, RAM, IOPS.  
Задача ставится следующим образом: даны параметры серверов: 𝐶= {𝑐1, 𝑐2, 𝑐3, … , 𝑐𝑛} 
такие, что ∀𝑐𝑖∈𝐶, 𝑐𝑖≥0 для каждого сервера 𝐶. Каждый параметр соответствует 
количеству ресурса определённого типа на физическом сервере. Также известны 
параметры ВМ для каждой машины 𝑉: 𝑉= {𝑣1, 𝑣2, 𝑣3, … , 𝑣𝑛}. 
Требуется расположить ВМ на физических серверах так, чтобы каждая ВМ находилась 
на сервере и сумма каждого из параметров по ВМ, находящихся на сервере, не 
превосходила значение этого параметра по серверу. При размещении требуется 
минимизировать число используемых физических серверов. 
Задача в данной постановке сводится к многомерной задаче упаковки объектов в 
контейнеры Multi-dimensional Bin Packing, где 𝑉, или ВМ – упаковываемые объекты, а 
𝐶, или серверы – контейнеры. Поэтому задача далее рассматривается в терминах 
многомерной задачи упаковки в контейнеры. 
Рассмотрим построение шаблонов для оптимального решения. 
Фиксируется натуральное число 𝐾. Применяется метод 𝑘-средних кластеризации к 
набору объектов или параметров ВМ, в результате применения которого ВМ разобьются 
на 𝐾  кластеров, и в каждом кластере будут ВМ с близкими параметрами.  
Объекты из каждого кластера преобразуются следующим образом: пусть в кластере 𝐶𝑙 
находятся объекты 𝑉1, … , 𝑉𝑙𝑎𝑠𝑡 с параметрами 𝑣1
1, … , 𝑣𝑛
1, … , 𝑣1
𝑙𝑎𝑠𝑡, … , 𝑣𝑛
𝑙𝑎𝑠𝑡. Тогда после 
преобразования, параметр с номером 𝑖 для каждой новой ВМ будет равен 𝑣𝑖
𝑘 𝑛𝑒𝑤=
𝑣𝑖
𝑛𝑒𝑤= max1≤𝑗≤𝑙𝑎𝑠𝑡𝑣𝑖
𝑗, и все объекты из одного кластера будут одинаковыми. 
Далее для этих изменённых объектов методом динамического программирования 
строятся все возможные наборы объектов, такие, что число объектов из каждого 
кластера в наборе не превышает общего числа объектов, принадлежащего данному 
кластеру и такие, что для каждого параметра сумма данных параметров в наборе не 
превосходит количества параметра данного типа в самом вместительном по параметру 
данного типа контейнере.  
Считая любые 2 объекта из одного кластера идентичными, данные наборы можно 
построить за 𝑂(𝑛𝑘) операций, где 𝐾 – число кластеров, а 𝑛 – число ВМ, которые нужно 
упаковать. 
Аллокация ВМ производится алгоритмом, похожим на First Fit Decreasing. Сначала 
наборы ВМ упорядочиваются с помощью метрики из оценки МакКарти (short-circuit 
evaluation), а именно, если у наборов 𝑆𝑒𝑡1 и 𝑆𝑒𝑡2 сумма параметров по всем ВМ равна 
{с1
𝑠𝑒𝑡1, … , 𝑐𝑛
𝑠𝑒𝑡1} и {с1
𝑠𝑒𝑡2, … , 𝑐𝑛
𝑠𝑒𝑡2} соответственно, то 𝑆𝑒𝑡1 > 𝑆𝑒𝑡2 тогда и только тогда, 
когда ∃𝑖∈{1, … , 𝑛}:  если 𝑗< 𝑖, то 𝑐𝑗
𝑠𝑒𝑡1 = 𝑐𝑗
𝑠𝑒𝑡1, а 𝑐𝑖
𝑠𝑒𝑡1 > 𝑐𝑖
𝑠𝑒𝑡1. При оценке по данной 
метрике сначала сравниваются наборы по сумме первых параметров всех элементов. 
Если для одного из наборов данная величина больше, чем для другого набора, то данный 
набор больше другого. Если же суммы первых параметров равны, рассматриваются 
вторые параметры и т.д.  
Алгоритм аллокации ВМ работает следующим образом.  
Выбирается самый большой по метрике МакКарти набор ВМ из оставшихся наборов и 
сервер с самой большой оценкой оставшихся параметров. Если набор ВМ можно 
расположить на сервере так, что сумма каждого параметра не превышается, а ВМ из 
Grushin D.A., Lazarev D.O., Fomin S.A. Data caching in multi-container systems. Trudy ISP RAN/Proc. ISP RAS, vol. 31, issue 6, 
2019. pp. 125-144 
140 
каждого кластера осталось достаточно, чтобы набрать набор, то выбираются ВМ 
наибольшего размера из оставшихся из каждого кластера и на сервере размещаются 
данные ВМ. Если этого нельзя сделать, то набор удаляется из рассмотрения. 
6.2 
Консолидация 
виртуальных 
машин 
с 
учетом 
производительности 
В работе [16] был предложен алгоритм «PACMan» для задачи размещения виртуальных 
машин. В отличии от [13], алгоритм строился с учётом того, что в результате уплотнения 
ВМ происходит размещение нескольких ВМ на одном сервере. Как следствие, 
происходит конфликт за использование разделяемых ресурсов, таких как CPU или RAM, 
в результате чего происходит ухудшение качества и скорости работы ВМ. Алгоритм 
позволяет сохранить ухудшение качества в заранее указанных пределах, при этом 
добиваясь производительности, близкой к лучшей возможной. 
Предполагается возможной оценка числа раз, в которое ухудшается работа каждой ВМ 
относительно изолированного размещения на отдельном сервере, при размещении 
набора ВМ на одном сервере для каждой ВМ каждого набора, исполняемого на том 
сервере.  
Для размещаемого набора 𝑆 ВМ вводится мера удельной стоимости совместного 
размещения 
𝑉(𝑆) = 𝑤(𝑆)
|𝑆| , 
где за 𝑤(𝑆) обозначено использование ресурсов каждой ВМ из набора. Качество работы 
алгоритма будем оценивать суммой затрат 𝐸(𝐴𝐿𝐺), равной сумме стоимостей всех 
размещенных наборов ВМ:  
𝐸(𝐴𝐿𝐺) =
∑
|𝑆|
𝑆 ∈ 𝑝ℎ𝑖𝑠𝑖𝑐𝑎𝑙 𝑠𝑒𝑟𝑣𝑒𝑟
 𝑉(𝑆). 
Все наборы ВМ с допустимым ухудшением работы каждой размещаются в порядке 
неубывания 𝑉(𝑆). Обозначим за ℱ множество всех этих наборов. 
Алгоритм размещения ВМ работает следующим образом: 
1. выбираем набор 𝑆 из ℱ с наименьшей стоимостью 𝑉(𝑆);  
2. если все ВМ из него можно разместить на сервере, то делаем это; 
3. если это сделать нельзя, удаляем набор 𝑆 из множества наборов. 
Пусть на сервере нельзя разместить больше 𝑘 ВМ. Была доказана следующая: 
Теорема. Для всех входных данных, алгоритм имеет сумму затрат 𝐸(𝐴𝐿𝐺) =
𝑂(ln 𝑘 ∙𝐸(𝑂𝑃𝑇)),  
где 𝐸(𝑂𝑃𝑇)) – сумма затрат оптимального алгоритма размещения ВМ. Однако так как 
максимальное число 𝑘 ВМ, которые можно разместить на сервере может быть велико, то 
и расчёт ухудшения для каждого набора ВМ занимает 𝑂(𝑘 ∙ 𝑛𝑘  ∙ln 𝑛) операций, где 𝑛  
– число размещаемых ВМ и может быть вычислительно сложным. Однако, если, 
аналогично работе Dow, разбить множества ВМ на 𝑙 кластеров методом 𝑘-средних, то, 
возможно, за счет уменьшения 𝐸(𝐴𝐿𝐺), вычислительная сложность алгоритма может 
быть уменьшена до 𝑂(𝑙 ∙ 𝑛𝑙  ∙ln 𝑛) для любого 𝑙∈ℕ.  
6.3 Оптимальные онлайновые детерминированные алгоритмы и 
эвристики для динамической консолидации виртуальных машин  
В работе [17] был предложен динамический алгоритм уплотнения расположения 
виртуальных машин в дата-центрах. Задача размещения ВМ представлена в виде задачи 
Грушин Д.А., Лазарев Д.О., Фомин С.А. Кэширование данных в мультиконтейнерных системах. Труды ИСП РАН, том 31, вып. 
6, 2019 г., стр. 125–144 
141 
bin packing упаковки в контейнеры с разными значениями размеров и стоимости 
контейнеров. Размеры контейнеров здесь – число доступных ЦП узлов дата-центра, а 
стоимости соответствуют потреблению энергии узла.  
Был предложен алгоритм, являющийся обобщением эвристики Best Fit Decreasing Height 
для задачи bin packing. Данная эвристика упаковывает объекты, расположенные в 
порядке убывания их веса в тот контейнер, при упаковке в который остаётся 
наименьшее число незаполненного пространства. Как показано в [18], данный алгоритм 
использует не более, чем 11/9·OPT+1 контейнеров, где OPT – это число решений, 
используемых оптимальным решением задачи. 
Алгоритм Power Aware Best Fit Decreasing  для размещения ВМ работает следующим 
образом: Сначала все ВМ размещаются по убыванию числа используемых ЦП, затем 
очередная ВМ размещается в хост, на котором из-за добавления ВМ происходит 
наименьшее среди всех хостов увеличение энергопотребления. 
6.4 Консолидация контейнеров в облачных центрах данных с 
обеспечением эффективного энергопотребления  
В работе [19] предложен фреймворк, уплотняющий расположение контейнеров на 
виртуальных машинах и сокращающий потребление энергии на хосте.  
В последнее время наряду с такими облачным сервисами, как IaaS (инфрастуктура как 
сервис), PaaS (платформа как сервис) и SaaS (программное обеспечение как сервис), всё 
шире и шире входит в употребление новый вид сервиса – CaaS, или контейнеры как 
сервис. Благодаря ему получается уменьшить время запуска системы.  
Слой 
CaaS 
находится 
между 
слоем 
IaaS, 
предоставляющим 
виртуальные 
вычислительные ресурсы и слоем PaaS, предоставляет приложениям среду выполнения 
и соединяет эти два слоя вместе. 
Наиболее широко используемым инструментом контейнеризации на сегодняшний день 
является ПО Docker. Однако в [20] показано, что запуск контейнеров Docker-а на ВМ 
позволяет достичь близкой, а, в некоторых случаях, даже лучшей производительности, 
чем запуск в “родной” среде, благодаря изолированности ВМ. Подход разворачивания 
контейнеров внутри ВМ был рассмотрен в работе.  
Серверы, элементы сети и системы охлаждения являются основными потребителями 
энергии в современном дата-центре. В работе с помощью алгоритмов миграции 
контейнеров 
уменьшается 
количество 
работающих 
серверов, 
благодаря 
чему 
уменьшается суммарное по серверам потребление энергии.  
Однако, уменьшая потребление энергии, следует удовлетворять SLA-соглашениям 
(Service Level Agreement). В случае развёртывания контейнеров на ВМ, SLA-соглашения 
нарушаются тогда и только тогда, когда ВМ, на которой запущен контейнер не имеет 
ЦП, необходимых ему для работы. Таким образом, за SLA сумму по всем ВМ, не 
получивших требующегося числа ядер, отношений разности чисел требуемых и 
полученных чисел ЦП к требуемому числу ЦП, или 
𝑆𝐿𝐴= ∑∑∑𝐶𝑃𝑈𝑟(𝑣𝑚𝑗,𝑖, 𝑡𝑝) −𝐶𝑃𝑈𝛼(𝑣𝑚𝑗,𝑖, 𝑡𝑝)
𝐶𝑃𝑈𝑟(𝑣𝑚𝑗,𝑖, 𝑡𝑝)
.
𝑁𝑣
𝑝=1
𝑁𝑣𝑚
𝑗=1
𝑁𝑠
𝑖=1
 
Здесь 𝑁𝑠 – число серверов, 𝑁𝑣𝑚  – чиcло ВМ, 𝑁𝑣 – число нарушений SLA-соглашений, 
𝑣𝑚𝑗,𝑖 – ВМ 𝑗 на сервере 𝑖 , 𝑡𝑝 – время, в которое произошло нарушение SLA-соглашений 
с номером 𝑝, 𝐶𝑃𝑈𝑟(𝑣𝑚𝑗,𝑖, 𝑡𝑝) – число ЦП, требующихся ВМ 𝑗 для развертывания 
контейнеров, расположенной на сервере 𝑖 в момент времени 𝑡𝑝, и 𝐶𝑃𝑈𝛼(𝑣𝑚𝑗,𝑖, 𝑡𝑝) – 
число ЦП, выделенных ВМ j в момент времени  𝑡𝑝. 
Grushin D.A., Lazarev D.O., Fomin S.A. Data caching in multi-container systems. Trudy ISP RAN/Proc. ISP RAS, vol. 31, issue 6, 
2019. pp. 125-144 
142 
Алгоритмы, отвечающие за миграцию контейнеров, работают в три этапа:  
1. определение ситуаций, при которых контейнеры должны мигрировать; 
2. определение, какие контейнеры мигрируют; 
3. определение, куда контейнеры мигрируют. 
Данные этапы в фреймворке реализованы следующим образом. 
1. Для каждого хоста статически заданы пороги недогрузки UL (under-load) и 
перегрузки OL (over-load), определяемые специальными алгоритмами. Если хост 
перегружен или недогружен, происходит миграция контейнеров 
2. Рассмотрены 
алгоритмы 
Mcor, 
выбирающий 
контейнеры 
с 
наиболее 
коррелированной нагрузкой с сервером, на котором контейнеры развёрнуты, и 
алгоритм MU, выбирающий контейнеры, которые используют больше всего ЦП. 
3. Были рассмотрен три алгоритма для выбора хоста, работающие по принципам 
алгоритмов для классической задачи Bin Packing: First Fit Host Select, выбирающий 
последний созданный подходящий хост для размещения контейнера; Random HS, 
выбирающий случайный подходящий хост и Least Full HS, выбирающий хост с 
наименьшим числом использованных ЦП из имеющихся. 
При увеличении порога UL и уменьшении порога OL, число миграций контейнеров 
увеличивается, и, следовательно, создаётся большее число ВМ и увеличивается 
использование ресурсов. С другой стороны, при увеличении OL и уменьшении UL, риск 
нарушения SLA-соглашений увеличивается. Оптимальные значения OL и UL для 
алгоритма Mcor выбора мигрирующих контейнеров и алгоритма First Fit HS выбора 
хоста назначения миграции равны OL=0.8, UL=0.7 соответственно. По сравнению с 
другими методами выбора мигрирующего контейнера и назначения миграции и 
значениями OL, UL, достигается экономия ресурсов на 7.4% при средних нарушениях 
SLA меньше 5% 
7. Заключение 
В данной работе рассмотрено множество аспектов технологий кэширования в 
мультиконтейнерных системах облачных вычислений. 
Рассмотрены практические технологии, позволяющие улучшить производительность 
мультиконтейнерных систем за счет различного кэширования — ускорение скорости 
старта 
за 
счет 
кэширования 
образов 
и 
состояний 
контейнеров, 
улучшение 
экономичности за счет разделения файлов, кэширование слоев образов при сборке и т.п. 
При этом рассмотрены и проблемы безопасности, которые приносят в контейнерную 
виртуализацию технологии аппаратного кэширования данных в процессорах. 
Отдельно рассмотрены теоретические подходы 
улучшения 
производительности 
мультиконтейнерных 
систем 
виртуализации 
за 
счет 
различных 
алгоритмов 
оптимального планирования.  

